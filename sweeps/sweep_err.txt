Loading python/3.8.13/slu6jvw
  Loading requirement: bzip2/1.0.8/xipsq2f libmd/1.0.4/sioeueg
    libbsd/0.11.5/gdafde7 expat/2.4.8/5wnje43 ncurses/6.2/pqxvmoe
    readline/8.1/52qiwcn gdbm/1.23/lkx5uz6 libiconv/1.16/lr5guq5
    xz/5.2.5/khdza45 zlib/1.2.12/bsohwcg libxml2/2.9.13/f5kumg5 pigz/2.7/zay4a5o
    zstd/1.5.2/jvujieu tar/1.34/4cnckqw gettext/0.21/hgt6t5w
    libffi/3.4.2/svyohlf openssl/1.1.1o/bowp5gw sqlite/3.38.5/ulzkiln
    util-linux-uuid/2.37.4/nlecwm6
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: xezwgfkj with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1285067156010174
wandb: 	learning_rate: 0.007894718299920826
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_125156-xezwgfkj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/xezwgfkj
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▅▄▅▄▄▄▄▅▅▄▅▄▄▄▄▄▄▄▄▄▄▄▄
wandb:    label_loss ▄▂▂▂▃▂▁▃▂▂▇▆▄▅▅▂▅▇▃▃▁▅▃▅▄▄▃▂█▃▃▃▃▆▄▃▁▁▁▄
wandb: test_accuracy ▆▇██▁▆▆▆▇▆▇▇▇▆▇
wandb:     test_loss ▂▁▁▁█▆▆▆▅▆▆▆▅▅▅
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0341
wandb:    label_loss 0.0303
wandb: test_accuracy 93.89
wandb:     test_loss 0.03942
wandb:    train_loss 0.03535
wandb: 
wandb: 🚀 View run unique-sweep-1 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/xezwgfkj
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_125156-xezwgfkj/logs
wandb: Agent Starting Run: q5oxvb9r with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.26627544896264177
wandb: 	learning_rate: 0.004589020336294243
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_125348-q5oxvb9r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/q5oxvb9r
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇█████
wandb:    image_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁      
wandb:    label_loss ▂▂▂▁▁▁▂▁▂▁▁▁▁█▂▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▁▁        
wandb: test_accuracy ████████████▁▁▁
wandb:     test_loss ▂▁▁▁▁▁█▇▆▆▅▅   
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁      
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run deep-sweep-2 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/q5oxvb9r
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_125348-q5oxvb9r/logs
wandb: Agent Starting Run: z2ortf8y with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2662794426781611
wandb: 	learning_rate: 0.0022016048002837223
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_125539-z2ortf8y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/z2ortf8y
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▃▂▂▃▂▁▂▂▂▁▂▂▂▂▂▃▁▁▂▂▁▁▂▂▁▂▂▂▂▂▁▁▂▂▁▁▂▁
wandb:    label_loss █▃▄▂▇▃▃▄▂▂▂▄▂▁▁▃▁▃▂▁▁▁▁▂▁▁▃▁▂▁▁▁▁▁▁▁▁▁▂▂
wandb: test_accuracy ▁▄▆▆▇▇▇▇█▇███▆▇
wandb:     test_loss █▅▃▃▁▂▂▂▂▃▁▁▂▃▃
wandb:    train_loss ██▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▂▁▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00248
wandb:    label_loss 0.00019
wandb: test_accuracy 97.33
wandb:     test_loss 0.00623
wandb:    train_loss 0.00319
wandb: 
wandb: 🚀 View run sparkling-sweep-3 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/z2ortf8y
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_125539-z2ortf8y/logs
wandb: Agent Starting Run: tjj75vqx with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.19947058033152024
wandb: 	learning_rate: 0.003530631628121066
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_125751-tjj75vqx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/tjj75vqx
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▄▃▃▂▂▂▃▂▃▂▃▃▂▂▂▁▃▂▃▃▃▂▂▃▃▃▂▂▂▂▁▄▄▂▂▃▂▃
wandb:    label_loss ▅█▃▃▄▂▁▄▄▂▁▁▂▂▃▂▂▁▁▃▁▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▇▇▇▇██▇▇█▇██▇
wandb:     test_loss █▃▁▁▂▄▂▃▂▄▃▂▃▃▃
wandb:    train_loss █▆▄▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂▁▁▂▂▁▁▁▁▁▂▁▁▂▁▂▂▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00326
wandb:    label_loss 0.00184
wandb: test_accuracy 96.85
wandb:     test_loss 0.00555
wandb:    train_loss 0.0042
wandb: 
wandb: 🚀 View run good-sweep-4 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/tjj75vqx
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_125751-tjj75vqx/logs
wandb: Agent Starting Run: 2m1cqn17 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.002544200276965003
wandb: 	learning_rate: 0.008487089891512848
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_125942-2m1cqn17
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/2m1cqn17
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▃▂▂▂▂▁▁▁▁▁▁                           
wandb:    label_loss ▅▁▂▇▃█▄▄▃▁                              
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▅▂▁           
wandb:    train_loss █▃▂▂▂▂▂▂▂▂▁▁▁▁▁                         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run revived-sweep-5 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/2m1cqn17
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_125942-2m1cqn17/logs
wandb: Agent Starting Run: 27bz1ayi with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.33589933775246883
wandb: 	learning_rate: 0.006429110881119819
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_130134-27bz1ayi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/27bz1ayi
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁   
wandb:    label_loss █▆▆▅▅▄▅▃▃▂▁▂▂▁▂▃▂▄▄▂▁▂▁▁▂▃▂▂▅▂▆▃▃▆▂▄▂   
wandb: test_accuracy ████████▇████▁▁
wandb:     test_loss ▂▁▁▁▁▁▁▁█▅▅▄▄  
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ 
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run hardy-sweep-6 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/27bz1ayi
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_130134-27bz1ayi/logs
wandb: Agent Starting Run: j0c7kbzb with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4552871761843664
wandb: 	learning_rate: 0.002116644840300464
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_130315-j0c7kbzb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/j0c7kbzb
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▅▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb:    label_loss ▆▂▄▃▅▂▁▁▁▁█▂▁▁▂▂▂▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁
wandb: test_accuracy ▁▆▇▇▇███▇████▇█
wandb:     test_loss █▂▂▂▂▁▁▂▂▂▁▂▃▂▂
wandb:    train_loss █▅▅▃▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00302
wandb:    label_loss 3e-05
wandb: test_accuracy 97.42
wandb:     test_loss 0.00841
wandb:    train_loss 0.00512
wandb: 
wandb: 🚀 View run magic-sweep-7 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/j0c7kbzb
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_130315-j0c7kbzb/logs
wandb: Agent Starting Run: n99u9pgs with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.11515152622259615
wandb: 	learning_rate: 0.008280422265211036
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_130527-n99u9pgs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/n99u9pgs
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃
wandb:    label_loss ▂▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁█▆▃▃▃▂▂▂▂▂▂▂▂
wandb: test_accuracy ▇▇▇██████▁▅▇▇▇▇
wandb:     test_loss ▂▁▁▁▁▁▁▁▁█▆▅▅▄▄
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03206
wandb:    label_loss 0.0297
wandb: test_accuracy 89.44
wandb:     test_loss 0.03817
wandb:    train_loss 0.0378
wandb: 
wandb: 🚀 View run dry-sweep-8 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/n99u9pgs
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_130527-n99u9pgs/logs
wandb: Agent Starting Run: ra9qlnnp with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.432314710085456
wandb: 	learning_rate: 0.0015795290568511877
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_130709-ra9qlnnp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/ra9qlnnp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▇▆▄▆▂▂▄▄▃▃▂▂▂▃▃▁▂▃▂▁▂▁▁▂▂▃▁▂▁▁▁▁▁▁▁▁▁▂
wandb: test_accuracy ▁▅▆▇▇▇▇████████
wandb:     test_loss █▅▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss █▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00101
wandb:    label_loss 0.00397
wandb: test_accuracy 97.75
wandb:     test_loss 0.00432
wandb:    train_loss 0.00167
wandb: 
wandb: 🚀 View run zany-sweep-9 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/ra9qlnnp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_130709-ra9qlnnp/logs
wandb: Agent Starting Run: 585yjwho with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.47966482212225
wandb: 	learning_rate: 0.008018157802650663
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_130850-585yjwho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/585yjwho
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:    image_loss ▁▁▁▁▁█▁▁▁▁▁▁▁                           
wandb:    label_loss ▂▂▄▅▃█▇▁▁▂                              
wandb: test_accuracy █▇██▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁██            
wandb:    train_loss ▁▁▁▁▁█▂▂▂▂                              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run glowing-sweep-10 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/585yjwho
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_130850-585yjwho/logs
wandb: Agent Starting Run: kqwmf1gq with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.25851979929091357
wandb: 	learning_rate: 0.0009694429128205156
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_131102-kqwmf1gq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/kqwmf1gq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ██▆▄▄▆▅▆▄▆▃▄▄▅▄▂▄▆▂▄▂▃▄▂▃▁▂▂▁▂▂▁▂▂▃▁▄▂▂▁
wandb: test_accuracy ▁▁▃▅▅▆▇▇▇▇▇████
wandb:     test_loss █▇▅▄▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00053
wandb:    label_loss 0.00425
wandb: test_accuracy 97.43
wandb:     test_loss 0.00271
wandb:    train_loss 0.00162
wandb: 
wandb: 🚀 View run ethereal-sweep-11 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/kqwmf1gq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_131102-kqwmf1gq/logs
wandb: Agent Starting Run: lsn9xkm0 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.43752106081073455
wandb: 	learning_rate: 0.0009510143904060136
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_131243-lsn9xkm0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/lsn9xkm0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆█▅▇▃▄▇▇▃▃▃▂▁▂▃▂▁▂▂▂▁▃▂▁▂▃▁▃▁▁▁▂▁▁▂▁▁▁▁
wandb: test_accuracy ▁▃▄▆▆▇▇▇█▇█████
wandb:     test_loss █▆▅▄▃▂▂▂▁▂▁▁▁▁▁
wandb:    train_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00072
wandb:    label_loss 0.00274
wandb: test_accuracy 97.74
wandb:     test_loss 0.00389
wandb:    train_loss 0.00209
wandb: 
wandb: 🚀 View run deep-sweep-12 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/lsn9xkm0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_131243-lsn9xkm0/logs
wandb: Agent Starting Run: klzmx9dt with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17533400475709193
wandb: 	learning_rate: 0.0024825923219826615
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_131424-klzmx9dt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/klzmx9dt
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆▆▄█▃▅▂▃▁▅▂▃▃▁▂▁▂▃▁▂▂▁▁▁▃▁▁▂▆▁▁▄▁▂▁▁▁▁▁▃
wandb: test_accuracy ▁▆▇▆▇▇▇████████
wandb:     test_loss █▄▂▂▂▂▂▂▂▂▁▂▂▂▂
wandb:    train_loss █▆▅▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00169
wandb:    label_loss 0.00926
wandb: test_accuracy 97.51
wandb:     test_loss 0.00357
wandb:    train_loss 0.00217
wandb: 
wandb: 🚀 View run balmy-sweep-13 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/klzmx9dt
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_131424-klzmx9dt/logs
wandb: Agent Starting Run: r8jg1n4j with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1488543825714702
wandb: 	learning_rate: 0.008667417251857124
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_131616-r8jg1n4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/r8jg1n4j
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▇▇▇▇▇▇▇▆▇▆▆▆
wandb:    label_loss █▃█▄▃▁▅▃▂▂▃▂▃▂▂▂▃▁▂▂▃▃▂▁▂▂▂▁▂▅▄▅▃▅▆▆▃▂▃▃
wandb: test_accuracy ▇▇▇██████▁▇▇▇▇▇
wandb:     test_loss ▁▁▁▁▁▁▁▁▁█▂▂▂▂▂
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▃▂▂▂▂▂▂▂▂▂▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03248
wandb:    label_loss 0.02212
wandb: test_accuracy 92.98
wandb:     test_loss 0.03819
wandb:    train_loss 0.03818
wandb: 
wandb: 🚀 View run different-sweep-14 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/r8jg1n4j
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_131616-r8jg1n4j/logs
wandb: Agent Starting Run: f2vs9jgo with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.15791636644422374
wandb: 	learning_rate: 0.0046233708721304155
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_131757-f2vs9jgo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/f2vs9jgo
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇███
wandb:    image_loss ▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▆▅▅▄▄▄▄▄▄▄▄▄▃▄
wandb:    label_loss ▇█▇▅▅▂▆▃▄▄▁▁▂▂▁▃▁▂▂▄▁▂▁▃▃▄▂▁▁▂▂▃▁▂▃▂▂▂▁▂
wandb: test_accuracy ▆▇▇█████▁▇▇▇▇██
wandb:     test_loss ▁▁▁▁▁▁▁▁█▂▁▁▁▁▁
wandb:    train_loss ▇▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01166
wandb:    label_loss 0.00961
wandb: test_accuracy 95.97
wandb:     test_loss 0.01414
wandb:    train_loss 0.01295
wandb: 
wandb: 🚀 View run desert-sweep-15 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/f2vs9jgo
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_131757-f2vs9jgo/logs
wandb: Agent Starting Run: g0qkitgh with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2873641948618484
wandb: 	learning_rate: 0.005748736242483066
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_131938-g0qkitgh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/g0qkitgh
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▇▅▃▃▂▂▁▂▁▁▂▁▁▂▁▁▂▂▃▃▃▂▃                
wandb:    label_loss ▇█▄▃▄▃▃▃▂▂▃▃▂▁▂▂▂▁▂▁▁▂▁▃                
wandb: test_accuracy ████████▁▁▁▁▁▁▁
wandb:     test_loss █▄▁▁▁▂▂▂       
wandb:    train_loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁                
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run tough-sweep-16 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/g0qkitgh
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_131938-g0qkitgh/logs
wandb: Agent Starting Run: 6xqwlcaz with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.02534899703309418
wandb: 	learning_rate: 0.002851583440359326
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_132120-6xqwlcaz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/6xqwlcaz
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▃▂▂▂▂▂▁▁▂▁▂▂▁▂▂▁▁▂▁▂▁▂▂▁▁▂▁▁▁▂▁▂▁▂▂▁▂▁
wandb:    label_loss ▆▅▄▅▅█▆▅▆▂▂▃▅▇█▆▅▃▄▄▄▅▇▄▃▃▂▃▃▂▃▂▁▂▂▅▃▁▃▁
wandb: test_accuracy ▁▁▁▂▅▆▇▇▇█████▇
wandb:     test_loss █▆▅▆▃▂▃▂▂▃▃▁▂▁▂
wandb:    train_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00142
wandb:    label_loss 0.01193
wandb: test_accuracy 95.15
wandb:     test_loss 0.00198
wandb:    train_loss 0.00174
wandb: 
wandb: 🚀 View run dandy-sweep-17 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/6xqwlcaz
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_132120-6xqwlcaz/logs
wandb: Agent Starting Run: a4ktngre with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2849093477916598
wandb: 	learning_rate: 0.004097802473528018
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_132311-a4ktngre
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/a4ktngre
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▂▂▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▂▁▂▂▁▁▁▁▂▂▁▁▁▁▁█▄▃▃▃▂▂
wandb:    label_loss ▂█▅▃▃▄▂▂▂▄▁▂▁▁▁▁▂▃▁▂▂▁▃▂▂▁▂▁▂▁▁▁▂▂▁▁▁▁▃▂
wandb: test_accuracy ▁▆▆▆▇█▇▇█▇█▄▇▇▇
wandb:     test_loss ▂▁▁▁▁▁▁▁▁▁▁█▃▃▃
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00755
wandb:    label_loss 0.00746
wandb: test_accuracy 97
wandb:     test_loss 0.0134
wandb:    train_loss 0.00896
wandb: 
wandb: 🚀 View run whole-sweep-18 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/a4ktngre
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_132311-a4ktngre/logs
wandb: Agent Starting Run: gp00fndd with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.05260033147856402
wandb: 	learning_rate: 0.004113746900901782
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_132503-gp00fndd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/gp00fndd
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆▆█▇▄▄▅▂▅▄▂▄▂▃▃▄▂▁▅▃▂▄▁▃▆▃▁▅█▆▁▃▃▂▃▇▅▁▅▂
wandb: test_accuracy ▅▆▇███▁▇▇▇█▇█▇█
wandb:     test_loss ▁▁▁▁▁▁█▃▃▂▂▂▂▂▂
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00731
wandb:    label_loss 0.01081
wandb: test_accuracy 95.43
wandb:     test_loss 0.00898
wandb:    train_loss 0.00877
wandb: 
wandb: 🚀 View run upbeat-sweep-19 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/gp00fndd
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_132503-gp00fndd/logs
wandb: Agent Starting Run: tt4wvj47 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.18725230160626016
wandb: 	learning_rate: 0.007193540132750543
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_132659-tt4wvj47
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/tt4wvj47
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▂▁▁▁▁▁▁▁▂█▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▅▄▅▄▄▄▄▄▄▄▄▄▄
wandb:    label_loss ▂▁▂▂▂▂▁▁▁▁█▃▃▂▂▃▂▂▂▂▂▃▄▂▂▁▁▂▁▂▁▂▁▂▁▂▂▃▂▂
wandb: test_accuracy ▆▇██▃▄▁▆▅▂▆▄▆▆▅
wandb:     test_loss ▁▁▁▂█▇█▇▇▇▇▇▆▆▆
wandb:    train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▇▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03079
wandb:    label_loss 0.00625
wandb: test_accuracy 91.84
wandb:     test_loss 0.03918
wandb:    train_loss 0.03774
wandb: 
wandb: 🚀 View run summer-sweep-20 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/tt4wvj47
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_132659-tt4wvj47/logs
wandb: Agent Starting Run: 7j56y3m8 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.35781080930982406
wandb: 	learning_rate: 0.00262463685483156
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_132851-7j56y3m8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/7j56y3m8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▅▄▃▃▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁
wandb:    label_loss ▆▆█▃▂▄▅▂▄▃▃▄▂▂▃▁▃▂▁▂▂▂▁▁▁▂▁▂▂▁▁▁▁▁▁▁▃▁▂▁
wandb: test_accuracy ▁▄▆▇▇▇▇█▇████▇█
wandb:     test_loss █▄▃▁▂▁▁▁▁▁▁▁▁▂▁
wandb:    train_loss █▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00133
wandb:    label_loss 0.00147
wandb: test_accuracy 97.73
wandb:     test_loss 0.00467
wandb:    train_loss 0.00188
wandb: 
wandb: 🚀 View run rich-sweep-21 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/7j56y3m8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_132851-7j56y3m8/logs
wandb: Agent Starting Run: vioo1v5i with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.22101672899746697
wandb: 	learning_rate: 0.004025823651248553
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_133032-vioo1v5i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/vioo1v5i
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▆▅
wandb:    label_loss ██▃▄▃▄▄▄▂▄▂▁▂▃▂▂▂▁▃▂▁▂▂▂▁▂▂▁▁▂▂▁▁▁▂▁▂▁▁▂
wandb: test_accuracy ▁▅▆▇▇█▇██████▁▇
wandb:     test_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁█▁
wandb:    train_loss ▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01769
wandb:    label_loss 0.00587
wandb: test_accuracy 96.46
wandb:     test_loss 0.01961
wandb:    train_loss 0.02019
wandb: 
wandb: 🚀 View run solar-sweep-22 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/vioo1v5i
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_133032-vioo1v5i/logs
wandb: Agent Starting Run: nxob58u4 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.15197726896226127
wandb: 	learning_rate: 0.005681824123901947
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_133213-nxob58u4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/nxob58u4
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇████
wandb:    image_loss ▁▁▁▁▂██▇▆▆▆▇▆▅▅▅▅▄▄▄▄                   
wandb:    label_loss ▄▁▁▃▂▁▁▄▃▂▂▇▃▃▂▅▂█▃▇▆▃▇▁▄               
wandb: test_accuracy █████████▁▁▁▁▁▁
wandb:     test_loss ▁▇▆▅▅█▄▄▄      
wandb:    train_loss ▁▁▁▁█▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃                    
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run sleek-sweep-23 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/nxob58u4
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_133213-nxob58u4/logs
wandb: Agent Starting Run: lxux6g1s with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3956932188170931
wandb: 	learning_rate: 0.004786658661760261
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_133430-lxux6g1s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/lxux6g1s
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇▇████
wandb:    image_loss ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁                     
wandb:    label_loss ▇▄▄▆▃▂█▁▂▁▁▁▂▁▁▂▂▃▄▁▁▁                  
wandb: test_accuracy ███████▁▁▁▁▁▁▁▁
wandb:     test_loss ▃▅▄▁▇█         
wandb:    train_loss █▇▆▄▄▃▃▃▃▃▂▃▂▁▁▄▃▄▂▃▄▄                  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run daily-sweep-24 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/lxux6g1s
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_133430-lxux6g1s/logs
wandb: Agent Starting Run: ftq7csmu with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.49166326694937845
wandb: 	learning_rate: 0.003914650281233349
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_133647-ftq7csmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/ftq7csmu
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▄▄▃▂▂▂▂▂▁▂▁▁▂▁▁▁▁▂▁▂▁▁▂▁▁▂▁▂▁▂▂▁▁▁▂   
wandb:    label_loss █▅▅▄▅▇▃▃▄▃▃▁▂▂▃▃▁▂▂▂▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▂   
wandb: test_accuracy █████████████▁▁
wandb:     test_loss █▄▂▁▂▂▂▁▁▁▂▃▄  
wandb:    train_loss █▄▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run laced-sweep-25 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/ftq7csmu
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_133647-ftq7csmu/logs
wandb: Agent Starting Run: x3tpnpmw with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4510564731254992
wandb: 	learning_rate: 0.008152899021085276
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_133828-x3tpnpmw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/x3tpnpmw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▃▂▁▁▁▁▁▁▂▂▆▆▆▆▆▆▆▆▆▆▇█▇▇▇█              
wandb:    label_loss ▂▂▂▂▁▂▁▁▁▁▁█▂▁▂▁▁▂▁▂▁▁▁▂▁▂▁▂▁           
wandb: test_accuracy ██████████▁▁▁▁▁
wandb:     test_loss ▁▁▁▂▆▇▇██▇     
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁           
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run fanciful-sweep-26 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/x3tpnpmw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_133828-x3tpnpmw/logs
wandb: Agent Starting Run: vfsz0z1h with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.05502581736144735
wandb: 	learning_rate: 0.007761555992480203
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_134020-vfsz0z1h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/vfsz0z1h
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▆▇▇█▁▄▆▆▆▆▆▇▇▇▇
wandb:     test_loss ▁▁▁▁█▇▆▇▆▆▇▆▅▅▅
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02765
wandb:    label_loss 0.02046
wandb: test_accuracy 90.69
wandb:     test_loss 0.02876
wandb:    train_loss 0.03179
wandb: 
wandb: 🚀 View run avid-sweep-27 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/vfsz0z1h
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_134020-vfsz0z1h/logs
wandb: Agent Starting Run: a4xfnvea with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4131520930377741
wandb: 	learning_rate: 0.006122216481046015
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_134216-a4xfnvea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/a4xfnvea
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss ▁▁▁▁▂████▇▇▇▇▆▇▆▆▆▆▆                    
wandb:    label_loss ▃▁▂▁▂▁█▃▂▃▂▁▁▂▁▁▂▃▂▁▂▁▂▂▁               
wandb: test_accuracy ████████▁▁▁▁▁▁▁
wandb:     test_loss ▁▁█▅▅▅▅▅       
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run rosy-sweep-28 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/a4xfnvea
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_134216-a4xfnvea/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: hw99ik55 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.10890122870866951
wandb: 	learning_rate: 0.007391473210838394
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_134418-hw99ik55
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/hw99ik55
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████
wandb:    image_loss ▄▂▂▁▁▁▁▁▁▁▁▁▁▁█▇▇▇▆▆▆▆▆▆▆▅▆▆▆▆▆▆▅▅▅▅▅▅▅▄
wandb:    label_loss ▂▂▂▂▁▁▁▁▂▁▁▁█▅▃▂▁▂▂▂▁▂▂▁▁▁▂▂▂▂▁▁▁▂▂▁▂▂▂▂
wandb: test_accuracy ▇▇██▁▅▅▆▅▅▆▆▆▅▆
wandb:     test_loss ▁▁▁▁█▆▆▅▆▇▅▅▅▅▄
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03047
wandb:    label_loss 0.03977
wandb: test_accuracy 90.31
wandb:     test_loss 0.02991
wandb:    train_loss 0.03388
wandb: 
wandb: 🚀 View run valiant-sweep-29 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/hw99ik55
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_134418-hw99ik55/logs
wandb: Agent Starting Run: 0jxq4nsi with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4283457786191721
wandb: 	learning_rate: 0.00338908025953088
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_134609-0jxq4nsi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/0jxq4nsi
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███
wandb:    image_loss █▅▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁
wandb:    label_loss ██▅▆▅▂▃▃▄▂▂▂▄▃▂▂▃▂▁▂▃▁▁▂▁▁▂▁▁▁▁▂▁▂▂▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆█▇▇█▇▇█▇███
wandb:     test_loss █▄▂▂▁▂▂▁▂▂▂▂▂▂▂
wandb:    train_loss █▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.002
wandb:    label_loss 0.00086
wandb: test_accuracy 97.85
wandb:     test_loss 0.006
wandb:    train_loss 0.0028
wandb: 
wandb: 🚀 View run brisk-sweep-30 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/0jxq4nsi
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_134609-0jxq4nsi/logs
wandb: Agent Starting Run: q7pn0gzt with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.27005785282621947
wandb: 	learning_rate: 0.006704944507167383
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_134751-q7pn0gzt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/q7pn0gzt
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▅▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂               
wandb:    label_loss ▅▆█▃▄▃▃▂▁▃▂▂▂▃▂▂▂▂▂▂▃▁▁▂▂               
wandb: test_accuracy █████████▁▁▁▁▁▁
wandb:     test_loss █▄▂▁▁▂▃▂▃      
wandb:    train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run restful-sweep-31 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/q7pn0gzt
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_134751-q7pn0gzt/logs
wandb: Agent Starting Run: 5j0oxhjk with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.10327108010851854
wandb: 	learning_rate: 0.008756448215120227
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_134932-5j0oxhjk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/5j0oxhjk
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█
wandb:    image_loss ▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁███▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅
wandb:    label_loss ▂▂▂▂▂▁▂▁▂▁▁▂▁▁█▇▅▃▃▂▂▂▃▃▂▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂
wandb: test_accuracy ████▁▅▆▇▇▇▇▇▇██
wandb:     test_loss ▁▁▁▁█▃▂▂▂▂▂▂▂▂▂
wandb:    train_loss ▂▁▁▁▁▁▁▁▁▁█▆▆▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.04083
wandb:    label_loss 0.03057
wandb: test_accuracy 89.22
wandb:     test_loss 0.04787
wandb:    train_loss 0.0485
wandb: 
wandb: 🚀 View run elated-sweep-32 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/5j0oxhjk
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_134932-5j0oxhjk/logs
wandb: Agent Starting Run: fd2bboff with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07907566969399876
wandb: 	learning_rate: 0.000192709610820541
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_135114-fd2bboff
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/fd2bboff
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▄▂▂▂▃▃▂▂▁▁▃▃▂▂▃▂▂▂▁▂▁▁▂▂▂▁▃▂▂▂▂▁▂▂▂▂▂
wandb: test_accuracy ▁▅▅▆▆▆▆▇▇▇▆█▇▇▇
wandb:     test_loss █▅▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss █▆▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00036
wandb:    label_loss 0.02396
wandb: test_accuracy 92.64
wandb:     test_loss 0.00237
wandb:    train_loss 0.00233
wandb: 
wandb: 🚀 View run fearless-sweep-33 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/fd2bboff
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_135114-fd2bboff/logs
wandb: Agent Starting Run: nnhf8bfv with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1846486147172219
wandb: 	learning_rate: 0.008711424604033098
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_135305-nnhf8bfv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/nnhf8bfv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▂▁▁█▇▆▇▆█▆▆                             
wandb:    label_loss ▁▁█▁▁▁▁▁▁▁                              
wandb: test_accuracy █▃██▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁█▅▆           
wandb:    train_loss ▂▁▁▁▁▁█▅▅▅▅▅▅                           
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run olive-sweep-34 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/nnhf8bfv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_135305-nnhf8bfv/logs
wandb: Agent Starting Run: niez1jry with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.011477948531707582
wandb: 	learning_rate: 0.0052801645997715
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_135521-niez1jry
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/niez1jry
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss ▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅
wandb:    label_loss ▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂█▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▁▁▁
wandb: test_accuracy ██████▁▆▆▇▇▇▇▇▇
wandb:     test_loss ▁▁▁▁▁▁█▆▅▄▄▄▄▄▄
wandb:    train_loss ▇▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▇▇▇▆▆▅▅▅▅▅▅▄▄▅▅▄▄▄▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02251
wandb:    label_loss 0.03157
wandb: test_accuracy 87.26
wandb:     test_loss 0.02328
wandb:    train_loss 0.02439
wandb: 
wandb: 🚀 View run lemon-sweep-35 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/niez1jry
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_135521-niez1jry/logs
wandb: Agent Starting Run: vmj1dh0q with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.0878139016867931
wandb: 	learning_rate: 0.008343436726996712
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_135703-vmj1dh0q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/vmj1dh0q
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▇▇▇▆▅▅▅▅▅
wandb:    label_loss ▂▂▂▂▂▂▂▁▃▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁█▄▄▃▃▄▃▃▂▂▂
wandb: test_accuracy ▇█████████▁▅▆▇▇
wandb:     test_loss ▂▁▁▁▁▁▁▁▁▁█▆▆▅▅
wandb:    train_loss ▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▇▇▇▇▆▆▆▅▅▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.04126
wandb:    label_loss 0.04375
wandb: test_accuracy 87.38
wandb:     test_loss 0.04621
wandb:    train_loss 0.04551
wandb: 
wandb: 🚀 View run fresh-sweep-36 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/vmj1dh0q
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_135703-vmj1dh0q/logs
wandb: Agent Starting Run: 6tpu770r with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4890169291267704
wandb: 	learning_rate: 0.00037133231714628
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_135844-6tpu770r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/6tpu770r
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█████
wandb:    image_loss █▇▇▆▆▅▅▅▄▅▄▄▄▄▄▄▃▄▃▃▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▇▇▄▅▄▄█▄▃▆▂▂█▄▄▄▆▃▃▅▅▃▂▄▃▁▂▂▂▁▁▂▂▁▂▂▂▁
wandb: test_accuracy ▁▂▃▄▅▆▆▇▇▇▇████
wandb:     test_loss █▇▅▄▄▃▃▂▂▂▁▁▁▁▁
wandb:    train_loss █▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00074
wandb:    label_loss 0.004
wandb: test_accuracy 97.42
wandb:     test_loss 0.00502
wandb:    train_loss 0.00343
wandb: 
wandb: 🚀 View run sage-sweep-37 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/6tpu770r
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_135844-6tpu770r/logs
wandb: Agent Starting Run: kmcj2z4i with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.49158809120013114
wandb: 	learning_rate: 0.008549556137672103
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_140036-kmcj2z4i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/kmcj2z4i
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▃▂▁▁▁▁▁▂▁▃▂                           
wandb:    label_loss █▆▇▄▆▁▃▂▄▂▁▃▃▂▄▂                        
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▁▁█           
wandb:    train_loss █▇▅▄▃▃▂▂▁▂▁▂▃▂▂                         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run hopeful-sweep-38 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/kmcj2z4i
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_140036-kmcj2z4i/logs
wandb: Agent Starting Run: riy8nw0m with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.23727107892690225
wandb: 	learning_rate: 0.005865096196996477
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_140227-riy8nw0m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/riy8nw0m
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ▂▂▁▁▁▁▁▁▁▂▁▁▂█▇▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▃▄     
wandb:    label_loss █▇▂▃▅▄▃▃▅▃▂▂▄▃█▃▅▆▆▂▂▄▂▄▄▂▁▂▂▅▄▃▂▂      
wandb: test_accuracy ████████████▁▁▁
wandb:     test_loss ▁▁▁▂█▅▅▅▅▅▄▅   
wandb:    train_loss ▃▂▁▁▁▁▁▁▁▁▁▁▁█▇▅▅▅▅▅▅▄▅▅▄▅▄▄▄▄▄▄▄▄▄     
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run avid-sweep-39 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/riy8nw0m
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_140227-riy8nw0m/logs
wandb: Agent Starting Run: dhgeyv49 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2581979977259307
wandb: 	learning_rate: 0.00537965463162844
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_140418-dhgeyv49
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_elu/sweeps/ze8vz884
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_elu/runs/dhgeyv49
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██
wandb:    image_loss ▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▆▅▅▅▄▄▄▄▄▄▄▄▄
wandb:    label_loss █▆▅▅▅▃▄▄▂▂▂▃▃▂▁▁▁▃▂▁▁▁▂▂▂▄▇▃▂▃▆▃▂▃▄▂▂▃▅▄
wandb: test_accuracy ▇▇██████▁▇█▇███
wandb:     test_loss ▁▁▁▁▁▁▁▁█▂▂▂▂▂▂
wandb:    train_loss ▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01741
wandb:    label_loss 0.01737
wandb: test_accuracy 95.36
wandb:     test_loss 0.02076
wandb:    train_loss 0.0198
wandb: 
wandb: 🚀 View run vocal-sweep-40 at: https://wandb.ai/cavaokcava/auto_512_elu/runs/dhgeyv49
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_140418-dhgeyv49/logs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: 7p2p8rdy with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.007221139306346047
wandb: 	learning_rate: 0.005755316882446299
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_140607-7p2p8rdy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/7p2p8rdy
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▁▂▂▁▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁
wandb: test_accuracy ▁▄▅▅▅▆▆▆▇▇▇▇███
wandb:     test_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▆▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00168
wandb:    label_loss 0.02046
wandb: test_accuracy 94.52
wandb:     test_loss 0.00182
wandb:    train_loss 0.00183
wandb: 
wandb: 🚀 View run swept-sweep-1 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/7p2p8rdy
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_140607-7p2p8rdy/logs
wandb: Agent Starting Run: mjikru3h with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.13095220488813408
wandb: 	learning_rate: 0.007162965303362349
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_140749-mjikru3h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/mjikru3h
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇█████
wandb:    image_loss █▃▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▂▅▂▇▅▂▂▁▁▂▄▂▃▁▁▂▃▂▂▁▁▁▁▃▁▁▁▁▂▃▄▁▁▁▃▃▁▃▂
wandb: test_accuracy ▁▂▅▇▇█▇▇▇▇█▇███
wandb:     test_loss █▅▃▂▂▁▁▂▁▂▁▂▁▁▁
wandb:    train_loss █▆▆▆▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▂▂▁▂▁▁▂▁▁▁▂▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00814
wandb:    label_loss 0.00899
wandb: test_accuracy 96.47
wandb:     test_loss 0.00966
wandb:    train_loss 0.00858
wandb: 
wandb: 🚀 View run earnest-sweep-2 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/mjikru3h
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_140749-mjikru3h/logs
wandb: Agent Starting Run: 80o0dhhu with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4315639581473977
wandb: 	learning_rate: 0.007118811658946513
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_141006-80o0dhhu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/80o0dhhu
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▅▆▆▇▇▇▇██▇██
wandb:     test_loss █▅▄▃▃▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0039
wandb:    label_loss 0.004
wandb: test_accuracy 97.18
wandb:     test_loss 0.00882
wandb:    train_loss 0.00544
wandb: 
wandb: 🚀 View run jumping-sweep-3 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/80o0dhhu
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_141006-80o0dhhu/logs
wandb: Agent Starting Run: nu7uifnu with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.21713276933944453
wandb: 	learning_rate: 0.006193869039324178
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_141147-nu7uifnu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/nu7uifnu
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▇▄▅▄▃▃▂▂▂▂▂▁▁▂▂▁▂▁▁▂▁▂▁▁▁▂▁▁▂▂▁▁▂▁▁▁▂▁▁
wandb:    label_loss █▃▂▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆▇▇█▇▇█▇█▇██
wandb:     test_loss █▄▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00542
wandb:    label_loss 0.00106
wandb: test_accuracy 97.02
wandb:     test_loss 0.0089
wandb:    train_loss 0.00718
wandb: 
wandb: 🚀 View run fresh-sweep-4 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/nu7uifnu
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_141147-nu7uifnu/logs
wandb: Agent Starting Run: ibi9mjn3 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4216105883751132
wandb: 	learning_rate: 0.0053182770393001046
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_141404-ibi9mjn3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/ibi9mjn3
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▃▂▁▁▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▅▆▇█▆▇█▆█▇▇▇
wandb:     test_loss █▅▃▃▂▂▁▂▁▁▂▁▁▁▁
wandb:    train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0029
wandb:    label_loss 8e-05
wandb: test_accuracy 97.53
wandb:     test_loss 0.00736
wandb:    train_loss 0.00374
wandb: 
wandb: 🚀 View run autumn-sweep-5 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/ibi9mjn3
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_141404-ibi9mjn3/logs
wandb: Agent Starting Run: lopjz9s4 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2892617220449047
wandb: 	learning_rate: 0.00688732842029818
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_141555-lopjz9s4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/lopjz9s4
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████
wandb:    image_loss █▇▄▄▅▄▄▂▃▁▂▃▄▂▄▂▂▂▃▃▂▂▂▁▂▂▂▂▂▂▂▂▂▃▂▂▂▁▁▂
wandb:    label_loss ▅▃▄█▇▄▂▂▂▂▁▄▁▂▁▁▅▁▁▂▁▁▁▁▂▁▁▂▁▁▂▁▁▁▂▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆█▇▇█▇▇▇███▇
wandb:     test_loss █▄▃▂▁▂▂▁▁▂▂▁▂▁▂
wandb:    train_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00834
wandb:    label_loss 0.00244
wandb: test_accuracy 96.56
wandb:     test_loss 0.01289
wandb:    train_loss 0.00899
wandb: 
wandb: 🚀 View run curious-sweep-6 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/lopjz9s4
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_141555-lopjz9s4/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: pamquepj with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2736134298456992
wandb: 	learning_rate: 0.0021690139653691935
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_141817-pamquepj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/pamquepj
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█████
wandb:    image_loss ██▅▅▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▂▁▂▂▁▁▁▂▁
wandb:    label_loss █▇▇▆▄▄▂▃▁▃▂▁▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇█▇██▇▇▇▇██
wandb:     test_loss █▄▃▂▂▁▁▁▁▁▁▂▁▁▁
wandb:    train_loss █▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00205
wandb:    label_loss 4e-05
wandb: test_accuracy 98.05
wandb:     test_loss 0.0039
wandb:    train_loss 0.00189
wandb: 
wandb: 🚀 View run radiant-sweep-7 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/pamquepj
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_141817-pamquepj/logs
wandb: Agent Starting Run: ub403tdi with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2980185831133399
wandb: 	learning_rate: 0.006403912672330435
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_142029-ub403tdi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/ub403tdi
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇█
wandb:    image_loss █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▆▇▇▇▇████▇██
wandb:     test_loss █▄▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00286
wandb:    label_loss 0.00131
wandb: test_accuracy 97.41
wandb:     test_loss 0.0061
wandb:    train_loss 0.00391
wandb: 
wandb: 🚀 View run vital-sweep-8 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/ub403tdi
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_142029-ub403tdi/logs
wandb: Agent Starting Run: u6palc0r with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.409578102663473
wandb: 	learning_rate: 0.0032582539959704975
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_142210-u6palc0r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/u6palc0r
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▅▅▄▅▂▃▄▃▁▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆▇▇▇████████
wandb:     test_loss █▅▃▃▃▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00184
wandb:    label_loss 0.00017
wandb: test_accuracy 98.19
wandb:     test_loss 0.00432
wandb:    train_loss 0.00199
wandb: 
wandb: 🚀 View run absurd-sweep-9 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/u6palc0r
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_142210-u6palc0r/logs
wandb: Agent Starting Run: vf6e34rh with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4626899560745493
wandb: 	learning_rate: 0.004986720667917905
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_142352-vf6e34rh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/vf6e34rh
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▆▃▄▄▄▃▃▂▂▂▃▂▃▂▂▁▁▂▁▁▁▁▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁
wandb:    label_loss █▄▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁
wandb: test_accuracy ▁▆▆▇▇▅▇█▇█▇▇▅▇█
wandb:     test_loss █▃▃▂▂▃▂▂▁▁▁▁▃▁▁
wandb:    train_loss █▅▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00609
wandb:    label_loss 0.0011
wandb: test_accuracy 97.6
wandb:     test_loss 0.00959
wandb:    train_loss 0.00746
wandb: 
wandb: 🚀 View run revived-sweep-10 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/vf6e34rh
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_142352-vf6e34rh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 328rwd0w with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4637613797065332
wandb: 	learning_rate: 0.004353747594644002
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_142617-328rwd0w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/328rwd0w
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▃▆▁▂▁▁▂▃▁▁▁▁▂▁▁▁▁▃▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇▇▇▇█▇▇▇▇█▇
wandb:     test_loss █▄▃▂▂▂▂▁▁▂▂▂▁▂▂
wandb:    train_loss █▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00371
wandb:    label_loss 8e-05
wandb: test_accuracy 97.61
wandb:     test_loss 0.00941
wandb:    train_loss 0.00565
wandb: 
wandb: 🚀 View run drawn-sweep-11 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/328rwd0w
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_142617-328rwd0w/logs
wandb: Agent Starting Run: 7y23e1qd with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2968895398898078
wandb: 	learning_rate: 0.0005946508630486507
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_142828-7y23e1qd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/7y23e1qd
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▃▃▃▃▂▃▃▃▂▂▃▂▂▂▂▂▁▁▂▂▁▂▂▁▂▂▂▂▂▂▁▁▂▁▁▁▁▁
wandb: test_accuracy ▁▃▄▄▅▅▅▆▆▇▇▇███
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00255
wandb:    label_loss 0.00611
wandb: test_accuracy 96.45
wandb:     test_loss 0.00592
wandb:    train_loss 0.0053
wandb: 
wandb: 🚀 View run robust-sweep-12 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/7y23e1qd
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_142828-7y23e1qd/logs
wandb: Agent Starting Run: 222jykfz with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.14884035911835758
wandb: 	learning_rate: 0.006584358634729422
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_143010-222jykfz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/222jykfz
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▄▂▂▂▂▂▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▃▄▄▆▇▇▇▇███▇██
wandb:     test_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00304
wandb:    label_loss 0.00291
wandb: test_accuracy 96.9
wandb:     test_loss 0.00467
wandb:    train_loss 0.00372
wandb: 
wandb: 🚀 View run volcanic-sweep-13 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/222jykfz
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_143010-222jykfz/logs
wandb: Agent Starting Run: ze7s5dnm with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2822972803604129
wandb: 	learning_rate: 0.0030105108943952316
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_143152-ze7s5dnm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/ze7s5dnm
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇█▅▃▂▂▄▃▅▂▂▁▂▁▁▁▁▁▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁
wandb: test_accuracy ▁▅▅▇▇████▇▇▇███
wandb:     test_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00191
wandb:    label_loss 0.00119
wandb: test_accuracy 98.04
wandb:     test_loss 0.00397
wandb:    train_loss 0.00206
wandb: 
wandb: 🚀 View run denim-sweep-14 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/ze7s5dnm
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_143152-ze7s5dnm/logs
wandb: Agent Starting Run: aa9dq6kx with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4586752999057771
wandb: 	learning_rate: 0.007151059625618025
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_143342-aa9dq6kx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/aa9dq6kx
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████
wandb:    image_loss █▆▅▃▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▆█▇▇███▇█▇█▇█
wandb:     test_loss █▅▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▆▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00534
wandb:    label_loss 0.00364
wandb: test_accuracy 97.13
wandb:     test_loss 0.01045
wandb:    train_loss 0.00702
wandb: 
wandb: 🚀 View run generous-sweep-15 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/aa9dq6kx
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_143342-aa9dq6kx/logs
wandb: Agent Starting Run: b0maqjfj with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3262972897715222
wandb: 	learning_rate: 0.0028378998497786633
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_143534-b0maqjfj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/b0maqjfj
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█████
wandb:    image_loss █▆▄▄▃▃▂▃▂▂▂▂▁▂▂▂▁▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▂▂▂▁▂▄▁▁▁▂▁▂▃▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb: test_accuracy ▁▆▆▇█▇▇█▇█▇▇▇▇█
wandb:     test_loss █▄▃▂▁▂▁▁▁▁▂▁▁▁▁
wandb:    train_loss █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00229
wandb:    label_loss 0.00242
wandb: test_accuracy 98.08
wandb:     test_loss 0.00542
wandb:    train_loss 0.00326
wandb: 
wandb: 🚀 View run solar-sweep-16 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/b0maqjfj
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_143534-b0maqjfj/logs
wandb: Agent Starting Run: hubdsjpr with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.025363732257947413
wandb: 	learning_rate: 0.00438219335834599
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_143751-hubdsjpr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/hubdsjpr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb: test_accuracy ▁▂▃▄▅▅▆▆▇▇▇▇███
wandb:     test_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00132
wandb:    label_loss 0.00533
wandb: test_accuracy 96.48
wandb:     test_loss 0.00154
wandb:    train_loss 0.00147
wandb: 
wandb: 🚀 View run worldly-sweep-17 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/hubdsjpr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_143751-hubdsjpr/logs
wandb: Agent Starting Run: 5ry4nojq with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.04395042422409151
wandb: 	learning_rate: 0.00396820896779502
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_143932-5ry4nojq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/5ry4nojq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▇▅▅█▅▆▂▃▃▃▂▅▂▂▁▁▄▃▃▁▂▃▁▂▂▁▄▁▂▂▁▁▂▂▁▁▁▂▁▁
wandb: test_accuracy ▁▄▄▇▇█▇█▇█▇█▇█▇
wandb:     test_loss █▅▄▂▂▂▂▂▂▂▂▂▂▁▂
wandb:    train_loss █▅▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00254
wandb:    label_loss 0.00308
wandb: test_accuracy 96.33
wandb:     test_loss 0.00291
wandb:    train_loss 0.00247
wandb: 
wandb: 🚀 View run swept-sweep-18 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/5ry4nojq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_143932-5ry4nojq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qk52rocr with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.47256603719847456
wandb: 	learning_rate: 0.0049008889701383125
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_144153-qk52rocr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/qk52rocr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▅▇▇▇▇██████▇█
wandb:     test_loss █▅▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00226
wandb:    label_loss 0.00038
wandb: test_accuracy 97.7
wandb:     test_loss 0.00625
wandb:    train_loss 0.00301
wandb: 
wandb: 🚀 View run volcanic-sweep-19 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/qk52rocr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_144153-qk52rocr/logs
wandb: Agent Starting Run: o4978dxe with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.09066472698550332
wandb: 	learning_rate: 0.007497972855713786
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_144335-o4978dxe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/o4978dxe
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▇▆█▅▃▂▁▂▂▂▂▂▃▂▂▂▂▃▂▂▁▂▂▂▃▂▂▃▂▂▁▁▁▂▂▂▁▁▁▂
wandb:    label_loss ▆▄▅▃▄▂▃▃▂▅▄▃▂▁▃▂▁▁▁▁▁▂▂▁▃▂▁▁▃█▂▂▁▁▂▁▁▂▂▃
wandb: test_accuracy ▁▄▅▇▇▇▇▆▇█▇▇███
wandb:     test_loss █▄▃▂▂▂▁▂▂▁▁▂▁▁▁
wandb:    train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00793
wandb:    label_loss 0.00337
wandb: test_accuracy 96.02
wandb:     test_loss 0.00931
wandb:    train_loss 0.00881
wandb: 
wandb: 🚀 View run misty-sweep-20 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/o4978dxe
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_144335-o4978dxe/logs
wandb: Agent Starting Run: 8am1op8o with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.18322447642758283
wandb: 	learning_rate: 0.001164863504024096
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_144547-8am1op8o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/8am1op8o
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 129-149, summary, console lines 130-156
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆▅▇█▇▆▇▃▇▇▃▄▃▂▃▆▇▄▃▂▂▄▂▂▃▁▂▃▁▂▂▂▂▂▃▄▁▁▁▂
wandb: test_accuracy ▁▃▃▅▆▆▇▇▇▇▇████
wandb:     test_loss █▆▅▄▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▆▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00084
wandb:    label_loss 0.00375
wandb: test_accuracy 97.43
wandb:     test_loss 0.00249
wandb:    train_loss 0.00158
wandb: 
wandb: 🚀 View run exalted-sweep-21 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/8am1op8o
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_144547-8am1op8o/logs
wandb: Agent Starting Run: mgfxhr4s with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.493646463007994
wandb: 	learning_rate: 0.003024747262677725
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_144738-mgfxhr4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/mgfxhr4s
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▇▅▅▃▄▂▂▂▁▃▂▃▁▂▁▁▂▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇█▇██████▇█
wandb:     test_loss █▅▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0019
wandb:    label_loss 0.00058
wandb: test_accuracy 97.94
wandb:     test_loss 0.00543
wandb:    train_loss 0.0023
wandb: 
wandb: 🚀 View run ethereal-sweep-22 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/mgfxhr4s
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_144738-mgfxhr4s/logs
wandb: Agent Starting Run: mclc8ysv with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3772269659165434
wandb: 	learning_rate: 0.008649989537147119
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_144919-mclc8ysv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/mclc8ysv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▃▂▂▁▁▁▁▁▂▂▁▁▁▂▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▂▁▁▂▁▁
wandb:    label_loss ▅▁▇█▃▂▃▅▁▁▂▁▂▃▁▂▂▁▁▁▅▁▁▁▂▂▁▁▂▂▄▁▃▂▁▂▂▁▁▁
wandb: test_accuracy ▁▅▆▆▆▇▆████▇▇█▇
wandb:     test_loss █▄▂▂▂▁▂▂▁▁▁▂▂▂▂
wandb:    train_loss █▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01424
wandb:    label_loss 0.00016
wandb: test_accuracy 96.31
wandb:     test_loss 0.0194
wandb:    train_loss 0.01584
wandb: 
wandb: 🚀 View run dazzling-sweep-23 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/mclc8ysv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_144919-mclc8ysv/logs
wandb: Agent Starting Run: dj1bec3d with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.11337760718455364
wandb: 	learning_rate: 0.008052301433969691
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_145131-dj1bec3d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/dj1bec3d
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▂█▂▅▂▂▁▁▁▂▃▃▁▃▂▂▂▁▁▁▁▁▂▁▂▁▁▂▁▄▂▁▁▁▂▁▂▂
wandb: test_accuracy ▁▄▆▆▇▆▆▇▇▇▇▇███
wandb:     test_loss █▄▂▂▂▂▂▁▁▁▂▁▂▁▁
wandb:    train_loss █▅▄▃▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01033
wandb:    label_loss 0.01254
wandb: test_accuracy 96.2
wandb:     test_loss 0.01061
wandb:    train_loss 0.01064
wandb: 
wandb: 🚀 View run daily-sweep-24 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/dj1bec3d
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_145131-dj1bec3d/logs
wandb: Agent Starting Run: tdke42y9 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.06201536758758358
wandb: 	learning_rate: 0.003845893624254697
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_145343-tdke42y9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/tdke42y9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▅█▆▄█▃▄▃▄▄▁▂▄▁▄▂▁▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▂▃▁▂▂▁▁▁
wandb: test_accuracy ▁▅▆▇▇▆▇██▇█▇█▇█
wandb:     test_loss █▅▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00209
wandb:    label_loss 0.00196
wandb: test_accuracy 96.97
wandb:     test_loss 0.00298
wandb:    train_loss 0.0025
wandb: 
wandb: 🚀 View run vital-sweep-25 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/tdke42y9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_145343-tdke42y9/logs
wandb: Agent Starting Run: 74fuhyc3 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.33575573136747466
wandb: 	learning_rate: 0.004464062185017144
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_145600-74fuhyc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/74fuhyc3
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▄▅▅▃▁▂▂▅▃▄▁▃▁▁▄▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆▇▇▇▇▇▇▇█▇▆█
wandb:     test_loss █▅▃▃▂▂▂▂▂▁▂▁▂▂▁
wandb:    train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00202
wandb:    label_loss 0.00357
wandb: test_accuracy 97.95
wandb:     test_loss 0.00526
wandb:    train_loss 0.00267
wandb: 
wandb: 🚀 View run lemon-sweep-26 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/74fuhyc3
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_145600-74fuhyc3/logs
wandb: Agent Starting Run: nc0knbn2 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4294499144242136
wandb: 	learning_rate: 0.006264096016855003
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_145751-nc0knbn2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/nc0knbn2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▅▆▇▇████████
wandb:     test_loss █▅▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00302
wandb:    label_loss 0.00341
wandb: test_accuracy 97.39
wandb:     test_loss 0.00695
wandb:    train_loss 0.00385
wandb: 
wandb: 🚀 View run brisk-sweep-27 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/nc0knbn2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_145751-nc0knbn2/logs
wandb: Agent Starting Run: 6kxxsa53 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.060133561730448704
wandb: 	learning_rate: 0.0016622005436317784
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_145932-6kxxsa53
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/6kxxsa53
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆▇▆▅█▅▄▂▆▆▆▃▅█▃▃▅▄▃▄▄▃▄▂▂▄▄▃▂▂▅▁▃▂▃▂▁▁▂▁
wandb: test_accuracy ▂▂▃▁▃▂▅▅▆▇▇▇███
wandb:     test_loss █▅▄▄▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00068
wandb:    label_loss 0.02584
wandb: test_accuracy 95.87
wandb:     test_loss 0.00153
wandb:    train_loss 0.00131
wandb: 
wandb: 🚀 View run proud-sweep-28 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/6kxxsa53
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_145932-6kxxsa53/logs
wandb: Agent Starting Run: cwjdn43e with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.32846166508748575
wandb: 	learning_rate: 0.007415337733026589
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_150124-cwjdn43e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/cwjdn43e
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▆▇▇▇█▇█▇▇▇▇█
wandb:     test_loss █▅▄▂▂▂▂▂▁▁▁▁▂▁▁
wandb:    train_loss █▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0059
wandb:    label_loss 0.00011
wandb: test_accuracy 97.13
wandb:     test_loss 0.00949
wandb:    train_loss 0.00711
wandb: 
wandb: 🚀 View run spring-sweep-29 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/cwjdn43e
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_150124-cwjdn43e/logs
wandb: Agent Starting Run: 5zqpgg30 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.41323446513396217
wandb: 	learning_rate: 0.0025797447587987913
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_150315-5zqpgg30
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/5zqpgg30
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 129-149, summary, console lines 130-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇████
wandb:    image_loss ██▇▆▆▄▄▃▄▃▂▃▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▅▇▂▂▃▂▂▃▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▅▆▇▆▇▇▇███▇▇▇
wandb:     test_loss █▅▄▃▂▂▂▁▂▁▁▁▁▁▂
wandb:    train_loss █▆▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00206
wandb:    label_loss 8e-05
wandb: test_accuracy 97.64
wandb:     test_loss 0.00565
wandb:    train_loss 0.00238
wandb: 
wandb: 🚀 View run rich-sweep-30 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/5zqpgg30
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_150315-5zqpgg30/logs
wandb: Agent Starting Run: 250ydafg with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2800773255887608
wandb: 	learning_rate: 0.005594691100031358
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_150507-250ydafg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/250ydafg
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▄▂▄▂▂▂▂▂▂▃▂▃▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb: test_accuracy ▁▄▅▆▇▇▇▇▇█▇▇██▇
wandb:     test_loss █▅▄▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▇▆▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00299
wandb:    label_loss 0.0038
wandb: test_accuracy 96.97
wandb:     test_loss 0.0065
wandb:    train_loss 0.00395
wandb: 
wandb: 🚀 View run zesty-sweep-31 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/250ydafg
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_150507-250ydafg/logs
wandb: Agent Starting Run: pcyir527 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1511377935042641
wandb: 	learning_rate: 0.0014920050799714943
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_150658-pcyir527
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/pcyir527
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▄▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▄▂▂▂▂▃▂▂▁▂▂▂▁▃▁▂▁▂▂▃▁▁▁▃▁▂▁▁▁▂▁▁▁▁▂▁▁
wandb: test_accuracy ▁▂▃▄▅▆▇▇▇▇█████
wandb:     test_loss █▆▅▄▃▃▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▇▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00082
wandb:    label_loss 0.01171
wandb: test_accuracy 97.54
wandb:     test_loss 0.00204
wandb:    train_loss 0.00123
wandb: 
wandb: 🚀 View run eager-sweep-32 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/pcyir527
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_150658-pcyir527/logs
wandb: Agent Starting Run: k1cjqxez with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.009820305543246475
wandb: 	learning_rate: 0.007356665822626053
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_150850-k1cjqxez
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/k1cjqxez
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▄▃▄▄▂▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▁▁▂▂▁▁▁▁▁▁▂▂▁▁
wandb: test_accuracy ▁▄▄▅▅▆▆▇▇▇▇▇███
wandb:     test_loss █▅▄▃▂▂▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▆▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00297
wandb:    label_loss 0.0115
wandb: test_accuracy 94.87
wandb:     test_loss 0.00323
wandb:    train_loss 0.00298
wandb: 
wandb: 🚀 View run floral-sweep-33 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/k1cjqxez
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_150850-k1cjqxez/logs
wandb: Agent Starting Run: le08ibud with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.015880539976579955
wandb: 	learning_rate: 0.009213695217631645
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_151041-le08ibud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/le08ibud
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▃▂▂▂▃▂▁▁▁▂▄▂▃▂▁▁▂▁▁▂▁▂▁▂▂▂▁▁▁▁▁▁▂▃▂▁▂▁▁
wandb: test_accuracy ▁▅▆▆▇███▅▇█████
wandb:     test_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▂
wandb:    train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0079
wandb:    label_loss 0.02518
wandb: test_accuracy 93.19
wandb:     test_loss 0.00898
wandb:    train_loss 0.00852
wandb: 
wandb: 🚀 View run stilted-sweep-34 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/le08ibud
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_151041-le08ibud/logs
wandb: Agent Starting Run: e70haf7x with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3194294678040831
wandb: 	learning_rate: 0.009143362454092203
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_151253-e70haf7x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/e70haf7x
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:    image_loss █▅▃▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁
wandb:    label_loss █▆▂▃▅▂▂▂▃▃▂▂▂▁▆▄▁▂▁▄▁▁▂▂▅▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁
wandb: test_accuracy ▁▆▆▇▇████▇▇████
wandb:     test_loss █▃▂▂▂▁▂▁▂▂▂▁▂▂▂
wandb:    train_loss █▇▆▅▄▃▃▂▃▃▂▃▂▂▂▁▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▂▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01309
wandb:    label_loss 0.03863
wandb: test_accuracy 96.86
wandb:     test_loss 0.01769
wandb:    train_loss 0.01574
wandb: 
wandb: 🚀 View run atomic-sweep-35 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/e70haf7x
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_151253-e70haf7x/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: irr02k58 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.05822403097087786
wandb: 	learning_rate: 0.00510424870310536
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_151514-irr02k58
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/irr02k58
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇█████
wandb:    image_loss █▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▅▄▅▃▇▂▃▂▂▂▂▁▄▂▃▂▂▁▁▂▂▂▁▂▂▁▃▁▁▂▁▁▁▂▁▂▁▁
wandb: test_accuracy ▁▂▄▆▇▇█████████
wandb:     test_loss █▅▄▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▇▅▅▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00204
wandb:    label_loss 0.00223
wandb: test_accuracy 96.56
wandb:     test_loss 0.00275
wandb:    train_loss 0.00235
wandb: 
wandb: 🚀 View run avid-sweep-36 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/irr02k58
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_151514-irr02k58/logs
wandb: Agent Starting Run: 2rhaedbq with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3959448250992706
wandb: 	learning_rate: 0.001867374071541872
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_151706-2rhaedbq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/2rhaedbq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▄▂▃▂▂▃▂▂▃▂▃▁▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▅▇▇▇█▇▇█████
wandb:     test_loss █▅▄▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00184
wandb:    label_loss 0.00227
wandb: test_accuracy 98.01
wandb:     test_loss 0.0043
wandb:    train_loss 0.00184
wandb: 
wandb: 🚀 View run effortless-sweep-37 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/2rhaedbq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_151706-2rhaedbq/logs
wandb: Agent Starting Run: 86mrtkmp with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2892943320800384
wandb: 	learning_rate: 0.0016799743232923012
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_151857-86mrtkmp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/86mrtkmp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███
wandb:    image_loss █▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▅▃▂▂▂▂▁▂▂▂▂▁▂▂▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▆▇▇█████████
wandb:     test_loss █▅▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00117
wandb:    label_loss 0.0006
wandb: test_accuracy 97.92
wandb:     test_loss 0.00321
wandb:    train_loss 0.00157
wandb: 
wandb: 🚀 View run glad-sweep-38 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/86mrtkmp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_151857-86mrtkmp/logs
wandb: Agent Starting Run: zj02cz2q with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.18710480932945367
wandb: 	learning_rate: 0.007634583297705755
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_152048-zj02cz2q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/zj02cz2q
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▂▂▂▂▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb: test_accuracy ▁▄▅▆▇▇█▇█████▇▇
wandb:     test_loss █▅▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00567
wandb:    label_loss 0.00191
wandb: test_accuracy 96.11
wandb:     test_loss 0.00789
wandb:    train_loss 0.00641
wandb: 
wandb: 🚀 View run bumbling-sweep-39 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/zj02cz2q
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_152048-zj02cz2q/logs
wandb: Agent Starting Run: g0ctspgg with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1060755951225762
wandb: 	learning_rate: 0.008081340366932801
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_152240-g0ctspgg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_512_sigmoid/sweeps/9djlqyjb
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/g0ctspgg
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▄▃▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▅▃▅▃▅▂▄▁▂▂▂▁▅▁▁▁▃▂▁▁▂▁▄▁▂▁▂▂▃▁▁▂▁▁▂▁▂▂
wandb: test_accuracy ▁▄▅▆▆▇▇█████▇█▇
wandb:     test_loss █▄▃▂▂▂▁▂▁▁▁▁▁▁▁
wandb:    train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00925
wandb:    label_loss 0.00971
wandb: test_accuracy 96.13
wandb:     test_loss 0.01042
wandb:    train_loss 0.01006
wandb: 
wandb: 🚀 View run effortless-sweep-40 at: https://wandb.ai/cavaokcava/auto_512_sigmoid/runs/g0ctspgg
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_512_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_152240-g0ctspgg/logs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: bzd1o6lj with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2444403859716036
wandb: 	learning_rate: 0.004150055777910724
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_152458-bzd1o6lj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/bzd1o6lj
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▂▂▁▁▁▁▁▂▂
wandb:    label_loss █▇▆▅▄▂▄▃▃▂▄▄▃▄▂▄▂▄▃▃▁▃▂▃▂▂▂▂▂▁▃▄▂▄▂▁▁▁▂▁
wandb: test_accuracy ▁▄▅▇▅▆▇▆▇█▇▇▆█▇
wandb:     test_loss █▄▂▂▃▁▁▂▂▁▂▂▂▂▂
wandb:    train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00674
wandb:    label_loss 0.00378
wandb: test_accuracy 96.52
wandb:     test_loss 0.01048
wandb:    train_loss 0.00841
wandb: 
wandb: 🚀 View run silver-sweep-1 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/bzd1o6lj
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_152458-bzd1o6lj/logs
wandb: Agent Starting Run: hy4kw8ec with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17194284596275966
wandb: 	learning_rate: 0.007791138626876793
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_152639-hy4kw8ec
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/hy4kw8ec
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇████
wandb:    image_loss █▆▇▆▆▅▅▄▄▄▄▃▃▃▂▂▂▁▁                     
wandb:    label_loss █▆▆▂▄▁▁▃▂▂▂▂▁▁▁                         
wandb: test_accuracy ▅▇█▇██▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▄▃▃▁▁         
wandb:    train_loss █▆▆▅▅▄▄▃▃▃▃▃▃▂▂▁▁▂▁▁                    
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run fine-sweep-2 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/hy4kw8ec
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_152639-hy4kw8ec/logs
wandb: Agent Starting Run: gusvh0ud with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4531558541564748
wandb: 	learning_rate: 0.0012414775953426306
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_152835-gusvh0ud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/gusvh0ud
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆▇█▅▂▂▂▃▄▄▁▃▁▂▄▃▁▂▂▂▂▁▃▂▂▁▂▁▂▂▁▁▁▁▂▁▂▁▁▁
wandb: test_accuracy ▁▅▅▇▇▇▇▇▇███▇██
wandb:     test_loss █▅▄▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00373
wandb:    label_loss 0.00324
wandb: test_accuracy 97.49
wandb:     test_loss 0.00812
wandb:    train_loss 0.00467
wandb: 
wandb: 🚀 View run earnest-sweep-3 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/gusvh0ud
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_152835-gusvh0ud/logs
wandb: Agent Starting Run: q11o1fi5 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.08361753401261635
wandb: 	learning_rate: 0.0010292901655912698
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_153017-q11o1fi5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/q11o1fi5
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 255-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▅▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁
wandb:    label_loss ▅█▄▃▅▅▁▂▁▃▃▂▂▂▃▁▃▁▂▁▁▂▂▃▂▂▁▁▁▁▁▂▁▁▂▁▃▁▂▂
wandb: test_accuracy ▁▄▄▆▇▇▇▆▇███▇▇▇
wandb:     test_loss █▅▄▂▂▂▁▂▂▁▁▁▂▁▁
wandb:    train_loss █▇▆▅▅▃▄▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▂▁▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00329
wandb:    label_loss 0.00086
wandb: test_accuracy 96.9
wandb:     test_loss 0.00407
wandb:    train_loss 0.00371
wandb: 
wandb: 🚀 View run smooth-sweep-4 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/q11o1fi5
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_153017-q11o1fi5/logs
wandb: Agent Starting Run: 9dvl8bux with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.08908656343528687
wandb: 	learning_rate: 0.00554898956993428
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_153241-9dvl8bux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/9dvl8bux
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 254-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▅▅▁▁▁▂▃▃▄▅▄▆▄▇                         
wandb:    label_loss ▅▁▆▂▁▅█▂▄▂▂▂▂                           
wandb: test_accuracy ███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▂▁█            
wandb:    train_loss █▄▃▂▂▁▁▁▂▂▂                             
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run magic-sweep-5 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/9dvl8bux
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_153241-9dvl8bux/logs
wandb: Agent Starting Run: 8ufhm2la with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2396239299675984
wandb: 	learning_rate: 0.006210126452115046
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_153503-8ufhm2la
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/8ufhm2la
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▅▄▂▂▂▁▁▁▁▁▁▁▁▁▂▂▃▂▂                  
wandb:    label_loss █▃▂▃▃▂▁▃▃▃▃▂▂▂▂▂▂▁▂▂▂▁                  
wandb: test_accuracy █████████▁▁▁▁▁▁
wandb:     test_loss █▃▂▁▁▁▂▂▅      
wandb:    train_loss █▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃               
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run atomic-sweep-6 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/8ufhm2la
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_153503-8ufhm2la/logs
wandb: Agent Starting Run: eitikq24 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.14701247816690272
wandb: 	learning_rate: 0.0036971368176814783
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_153700-eitikq24
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/eitikq24
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▂▁▁▂▁▁▁▁▁▁
wandb:    label_loss ▆▆▄█▃▃▅▄▂▃▂▄▂▂▂▂▂▃▁▂▂▃▁▂▂▁▃▂▂▂▂▂▂▂▂▂▁▁▁▁
wandb: test_accuracy ▁▄▄▆▆▆▄▇▇▇▇▇███
wandb:     test_loss █▄▃▁▁▁▂▁▂▁▂▂▂▁▁
wandb:    train_loss █▆▄▃▃▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00548
wandb:    label_loss 0.00371
wandb: test_accuracy 97.31
wandb:     test_loss 0.00691
wandb:    train_loss 0.00624
wandb: 
wandb: 🚀 View run genial-sweep-7 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/eitikq24
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_153700-eitikq24/logs
wandb: Agent Starting Run: bg1vatm3 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.36026055152519854
wandb: 	learning_rate: 0.0075549019083787266
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_153841-bg1vatm3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/bg1vatm3
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████
wandb:    image_loss █▇▆▆▃▂▂▁▁▁▁▁▁▂▃▄                        
wandb:    label_loss █▂▃▂▃▁▂▁▂▁▁▁▁▃▁▂▂▁▂▂▂▂▂                 
wandb: test_accuracy ██████▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▃▂▁▂▇         
wandb:    train_loss █▆▄▃▂▂▁▂▁▁▁▂▂▂▂▂▂▄▅▃▅▅                  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run vivid-sweep-8 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/bg1vatm3
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_153841-bg1vatm3/logs
wandb: Agent Starting Run: 47n7l1qf with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.007830006195378714
wandb: 	learning_rate: 0.007563625026623587
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_154038-47n7l1qf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-aardvark-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/47n7l1qf
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇█
wandb:    image_loss █▇▄▄▃▁▁                                 
wandb:    label_loss █▇▄▁▁▂                                  
wandb: test_accuracy ▆█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▁             
wandb:    train_loss █▅▄▄▃▁                                  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run ruby-aardvark-9 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/47n7l1qf
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_154038-47n7l1qf/logs
wandb: Agent Starting Run: f5v2r441 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2218813211515433
wandb: 	learning_rate: 0.00207306088612047
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_154259-f5v2r441
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/f5v2r441
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 255-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▆█▄▃▃▅▃▄▂▃▃▄▅▅▂▂▃▄▃▂▂▂▃▂▃▃▂▃▃▄▂▃▃▃▃▃▁▂
wandb:    label_loss ▆▂▇▇█▂▇▄▇▃▁▂▂▃▂▁▃▁▂▁▂▃▁▁▁▁▃▁▁▂▃▃▃▄▅▁▄▃▁▁
wandb: test_accuracy ▁▃▇▆▇▆▇▇█▇▇▇▇█▆
wandb:     test_loss █▅▄▃▄▃▂▄▂▂▂▄▂▁▅
wandb:    train_loss █▇▅▅▄▆▅▃▄▃▅▃▃▂▄▅▂▃▃▃▃▃▄▂▂▄▃▁▃▃▂▃▂▃▃▂▂▂▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00921
wandb:    label_loss 0.00695
wandb: test_accuracy 96.12
wandb:     test_loss 0.01199
wandb:    train_loss 0.01062
wandb: 
wandb: 🚀 View run earnest-sweep-10 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/f5v2r441
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_154259-f5v2r441/logs
wandb: Agent Starting Run: hq1sxzk9 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17919275897794829
wandb: 	learning_rate: 0.008763927365593387
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_154521-hq1sxzk9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/hq1sxzk9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▇▆▅▅▄▄▄▃▃▂▂▂▂▂▂▂▁▁                     
wandb:    label_loss █▇▄▅▃▂▂▃▂▂▂▂▂▂▂▁▁▂▁                     
wandb: test_accuracy ▆▇███▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▅▂▁▁          
wandb:    train_loss █▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁                       
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run desert-sweep-11 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/hq1sxzk9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_154521-hq1sxzk9/logs
wandb: Agent Starting Run: lkifztyu with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4485727243847514
wandb: 	learning_rate: 0.0010797930171921172
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_154718-lkifztyu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/lkifztyu
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▆▅▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▅▃▄▃▂▄▄▅▃▂▃▂▁▃▁▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂
wandb: test_accuracy ▁▄▇▆▇▇▇▇▇▇████▇
wandb:     test_loss █▅▃▃▂▂▂▂▁▂▁▁▁▁▂
wandb:    train_loss █▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00307
wandb:    label_loss 0.00081
wandb: test_accuracy 97.26
wandb:     test_loss 0.00817
wandb:    train_loss 0.00432
wandb: 
wandb: 🚀 View run electric-sweep-12 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/lkifztyu
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_154718-lkifztyu/logs
wandb: Agent Starting Run: ns4zbozk with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.03705197535409349
wandb: 	learning_rate: 0.0016869607399627416
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_154904-ns4zbozk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/ns4zbozk
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▄▅▆▄▃▃▄▅▂▄▃▄▂▂▂▂▂▂▂▁▃▁▂▂▃▂▂▂▂▂▄▂▂▄▂▁▂▂
wandb:    label_loss ▄▄▄▃▄▂▆▄▅▁▃▄█▃▂▂▄▃▄▃▂▅▁▂▂▁▅▃▂▂▄▃▁▃▅▃▃▃▁▃
wandb: test_accuracy ▁▂▄▅▆▆▆▇▇▆▇▇▇▇█
wandb:     test_loss █▇▄▄▃▃▃▃▂▃▂▂▂▂▁
wandb:    train_loss ██▅▅▄▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▂▂▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00399
wandb:    label_loss 0.00378
wandb: test_accuracy 96.13
wandb:     test_loss 0.0043
wandb:    train_loss 0.00478
wandb: 
wandb: 🚀 View run confused-sweep-13 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/ns4zbozk
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_154904-ns4zbozk/logs
wandb: Agent Starting Run: 25vbn9pg with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.34844968599754467
wandb: 	learning_rate: 0.006008846050278289
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_155125-25vbn9pg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/25vbn9pg
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 255-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇████
wandb:    image_loss █▅▃▁▁                                   
wandb:    label_loss ▄▂▄▂▁▂▂▂▂█                              
wandb: test_accuracy ███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▁█            
wandb:    train_loss ▄▄▃▁▃▂▁█                                
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run rosy-sweep-14 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/25vbn9pg
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_155125-25vbn9pg/logs
wandb: Agent Starting Run: vl5seyib with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4214712647217303
wandb: 	learning_rate: 0.005799446714898512
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_155347-vl5seyib
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/vl5seyib
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████
wandb:    image_loss █▁▁▁▁▂▅▄                                
wandb:    label_loss ▄▇█▂▁▄                                  
wandb: test_accuracy ███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▄█            
wandb:    train_loss ▄▁▁▁▂▁▂▄▄█                              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run woven-sweep-15 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/vl5seyib
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_155347-vl5seyib/logs
wandb: Agent Starting Run: l0kagwl0 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.41351320894190086
wandb: 	learning_rate: 0.000968551500302602
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_155609-l0kagwl0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/l0kagwl0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇███
wandb:    image_loss █▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▆▅▅▂▃▃▄▃▄▂▂▂▁▃▃▃▂▂▃▂▂▁▂▁▁▂▂▁▂▂▄▂▁▁▃▁▂▂
wandb: test_accuracy ▁▃▆▆▇▇▆▅▇▇▇████
wandb:     test_loss █▅▃▃▂▂▂▂▁▂▁▁▁▁▁
wandb:    train_loss █▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00304
wandb:    label_loss 0.00325
wandb: test_accuracy 97.62
wandb:     test_loss 0.0065
wandb:    train_loss 0.00419
wandb: 
wandb: 🚀 View run lyric-sweep-16 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/l0kagwl0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_155609-l0kagwl0/logs
wandb: Agent Starting Run: jd1wf1k2 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07641500793404604
wandb: 	learning_rate: 0.007685107924724577
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_155750-jd1wf1k2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/jd1wf1k2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█
wandb:    image_loss █▇▃▁▂                                   
wandb:    label_loss █▃▃▂▃▃▄▃▁▂                              
wandb: test_accuracy ▇██▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▄▁            
wandb:    train_loss █▆▅▄▃▂▂▂▁▁                              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run autumn-sweep-17 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/jd1wf1k2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_155750-jd1wf1k2/logs
wandb: Agent Starting Run: dv7spi27 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2388684553324543
wandb: 	learning_rate: 0.007846523037690954
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_155947-dv7spi27
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/dv7spi27
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇████
wandb:    image_loss ▃▁▁▁▁▄▄▅▅▄▅▅▅▅▅▆█▅▆▅▅▆▆▅▅▅▇▅▅▆▅▅▆▅▅▅▅▇▄▆
wandb:    label_loss ▇▄▃▂▃▁▂▁▆▆▆▇▆▇▆▆▆▆▆▆▆▇▆▆▆▇█▆▆▆▆▇▆▆▆▇▅▆▅▆
wandb: test_accuracy ▆█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▁▇█▇▇▇▇▇▇▇▇▇▇▇
wandb:    train_loss ▂▂▁▁▁▁▁█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.10441
wandb:    label_loss 0.24532
wandb: test_accuracy 10.32
wandb:     test_loss 0.15942
wandb:    train_loss 0.16872
wandb: 
wandb: 🚀 View run usual-sweep-18 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/dv7spi27
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_155947-dv7spi27/logs
wandb: Agent Starting Run: fez64qkb with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2397130120387303
wandb: 	learning_rate: 0.0049191610103040415
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_160208-fez64qkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/fez64qkb
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇████
wandb:    image_loss █▆▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▂▂▂▂▂▁▂
wandb:    label_loss ▇▄▅█▄▂▆▄▃▃▁▄▃▂▅▂▁▃▂▂▅▄▂▃▂▃▃▂▂▂▄▃▂▃▃▁▂▃▁▂
wandb: test_accuracy ▁▄▆▆▆▇▆▇▇▆█▇█▇█
wandb:     test_loss █▄▂▁▁▁▁▁▂▃▂▂▂▃▃
wandb:    train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01015
wandb:    label_loss 0.00695
wandb: test_accuracy 96.84
wandb:     test_loss 0.01178
wandb:    train_loss 0.01097
wandb: 
wandb: 🚀 View run dandy-sweep-19 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/fez64qkb
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_160208-fez64qkb/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9dc4i4ww with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.44665737536740974
wandb: 	learning_rate: 0.009836977259630873
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_160400-9dc4i4ww
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/9dc4i4ww
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███
wandb:    image_loss ███▆▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▃▃▃▃▃▄▃▃▅▃▂▂▂▁       
wandb:    label_loss █▇▆▅▃▃▃▂▂▂▂▂▁▂▃▁▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁▂▁       
wandb: test_accuracy ▅▇▇█████████▁▁▁
wandb:     test_loss █▄▃▂▁▁▁▁▂▁▁▁   
wandb:    train_loss █▇▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁     
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run generous-sweep-20 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/9dc4i4ww
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_160400-9dc4i4ww/logs
wandb: Agent Starting Run: j9d7oxcd with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.41637174687852874
wandb: 	learning_rate: 0.0026303196347788513
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_160541-j9d7oxcd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/j9d7oxcd
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███
wandb:    image_loss ▆▆▅▄▄▃▄▄▆▃▆▃▅▂▂▃▅█▅▃▄▂▁▄▂▆▃▇▁▃▅█▃▄▆▁▅▂▅▃
wandb:    label_loss █▆▇▄▃█▃▅▃▂▁█▁▁▅▂▄▁▂▆▆▃▃▂▁▃▁▁▁▂▂▁▂▂▁▁▂▅▂▁
wandb: test_accuracy ▁▃▅▁▄▇▄█▆▆▆▄▇█▇
wandb:     test_loss ▅▄▂▄▆▂█▂▁▄▅▅▂▇▅
wandb:    train_loss █▄▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▂▁▂▁▁▂▁▁▂▁▁▁▂▁▁▂▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01766
wandb:    label_loss 0.00024
wandb: test_accuracy 96.72
wandb:     test_loss 0.0223
wandb:    train_loss 0.02178
wandb: 
wandb: 🚀 View run deep-sweep-21 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/j9d7oxcd
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_160541-j9d7oxcd/logs
wandb: Agent Starting Run: vo13fiaj with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2317726899326923
wandb: 	learning_rate: 0.007698701432677877
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_160804-vo13fiaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/vo13fiaj
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█
wandb:    image_loss █▅▁▂▁▂                                  
wandb:    label_loss ▄▅▃▆▃██▁▁                               
wandb: test_accuracy ██▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁█             
wandb:    train_loss ▅▅▂▁█▁                                  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run brisk-sweep-22 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/vo13fiaj
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_160804-vo13fiaj/logs
wandb: Agent Starting Run: ujholuq3 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4383596772077215
wandb: 	learning_rate: 0.005522079393207509
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_161025-ujholuq3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/ujholuq3
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▅▃▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▂▂▂▂▂
wandb:    label_loss █▆█▅▄▄▄▄▂▅▅▂▃▂▄▂▂▂▃▄▂▃▄▂▃▃▃▃▂▃▃▂▃▃▂▂▃▂▁▁
wandb: test_accuracy ▁▄▇▆▆▇▇▇▅██████
wandb:     test_loss █▄▂▁▁▁▁▂▂▁▃▂▄▅▂
wandb:    train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01197
wandb:    label_loss 0.00354
wandb: test_accuracy 96.92
wandb:     test_loss 0.0165
wandb:    train_loss 0.01446
wandb: 
wandb: 🚀 View run stellar-sweep-23 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/ujholuq3
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_161025-ujholuq3/logs
wandb: Agent Starting Run: if8bll8a with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.25275261477576244
wandb: 	learning_rate: 0.008111257790813347
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_161207-if8bll8a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/if8bll8a
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇███
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run glamorous-sweep-24 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/if8bll8a
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_161207-if8bll8a/logs
wandb: Agent Starting Run: dmblmn3f with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1903524410439903
wandb: 	learning_rate: 0.0005248591666069196
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_161404-dmblmn3f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/dmblmn3f
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▅▄▆▅▅▅▂▃▂▃▄▂▃▁▂▂▂▃▂▃▂▁▂▁▂▂▃▂▂▃▁▃▄▂▂▂▁▁
wandb: test_accuracy ▁▄▅▆▇▇▇█▇▇█▇███
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0025
wandb:    label_loss 0.0029
wandb: test_accuracy 97.59
wandb:     test_loss 0.00387
wandb:    train_loss 0.00311
wandb: 
wandb: 🚀 View run usual-sweep-25 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/dmblmn3f
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_161404-dmblmn3f/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: lwxadbyg with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2702966328982661
wandb: 	learning_rate: 0.0013816029051541604
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_161555-lwxadbyg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/lwxadbyg
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇████
wandb:    image_loss █▅▄▄▄▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆██▆▃▃▄▃▅▂▂▃▃▂▃▂▁▂▄▁▂▃▂▂▁▂▂▂▁▂▂▂▁▂▂▁▁▂▁▁
wandb: test_accuracy ▁▅▆▆▇▇▇▇▇████▇█
wandb:     test_loss █▅▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00308
wandb:    label_loss 0.0029
wandb: test_accuracy 97.35
wandb:     test_loss 0.00602
wandb:    train_loss 0.00368
wandb: 
wandb: 🚀 View run grateful-sweep-26 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/lwxadbyg
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_161555-lwxadbyg/logs
wandb: Agent Starting Run: 7f43g218 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4558191492734244
wandb: 	learning_rate: 0.000874406746902563
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_161740-7f43g218
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/7f43g218
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▅▃▃▄▄▄▄▃▁▂▂▂▃▅▁▁▄▁▂▃▂▁▁▂▁▂▂▂▁▂▁▁▂▁▁▁▂▂
wandb: test_accuracy ▁▄▅▆▇▇▇▇▇█▇▇█▇█
wandb:     test_loss █▅▄▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00341
wandb:    label_loss 0.00599
wandb: test_accuracy 97.59
wandb:     test_loss 0.00767
wandb:    train_loss 0.00451
wandb: 
wandb: 🚀 View run proud-sweep-27 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/7f43g218
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_161740-7f43g218/logs
wandb: Agent Starting Run: xhixtnm7 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.24062870119699337
wandb: 	learning_rate: 0.0028076403314141528
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_161921-xhixtnm7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/xhixtnm7
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▅▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂
wandb:    label_loss █▇▇▆▆▃▂▃▃▃▃▄▂▃▃▄▃▂▃▂▂▄▁▂▂▃▂▂▂▂▂▅▂▁▂▁▁▂▁▃
wandb: test_accuracy ▁▃▅▆▇▇▆█▇█▇████
wandb:     test_loss █▆▄▄▃▃▂▁▂▁▂▁▁▁▃
wandb:    train_loss █▇▅▄▄▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00565
wandb:    label_loss 0.00429
wandb: test_accuracy 97.21
wandb:     test_loss 0.00841
wandb:    train_loss 0.00622
wandb: 
wandb: 🚀 View run laced-sweep-28 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/xhixtnm7
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_161921-xhixtnm7/logs
wandb: Agent Starting Run: 4ldyv98s with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4583779326094808
wandb: 	learning_rate: 0.0006828937225670417
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_162103-4ldyv98s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/4ldyv98s
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▆▇▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▂
wandb:    label_loss ▄▂▃▂▂▃▄▂▁▂▁▂▂▁▃▃▁▁▂▂▂▁▁█▁▂▁▁▁▁▁▂▁▁▁▂▁▁▂▂
wandb: test_accuracy ▁▅▄▆▆▇▇█▇▇█▇██▇
wandb:     test_loss █▄▄▃▃▂▂▁▂▂▁▂▁▁▂
wandb:    train_loss █▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00379
wandb:    label_loss 0.00058
wandb: test_accuracy 97.19
wandb:     test_loss 0.00824
wandb:    train_loss 0.00522
wandb: 
wandb: 🚀 View run warm-sweep-29 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/4ldyv98s
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_162103-4ldyv98s/logs
wandb: Agent Starting Run: vxvaergk with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.24912840492072055
wandb: 	learning_rate: 0.003456785118399985
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_162325-vxvaergk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/vxvaergk
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇█████
wandb:    image_loss █▄▄▃▄▂▂▂▂▂▂▂▂▂▃▂▃▂▂▂▂▂▂▃▂▂▂▂▃▂▂▂▂▃▂▂▃▂▁▃
wandb:    label_loss █▄▄▄▃▂▆▂▃▃▃▃▂▅▂▁▃▂▂▄▁▁▂▃▂▁▁▁▂▁▂▁▁▁▄▃▃▂▃▂
wandb: test_accuracy ▁▃▆▅▇▇▆▇▇▇█▇█▇▇
wandb:     test_loss █▄▃▃▃▂▃▁▃▄▂▃▁▅▂
wandb:    train_loss █▅▄▄▃▂▃▃▃▃▂▂▂▂▂▃▂▁▂▂▃▂▁▂▃▃▂▂▃▃▂▂▂▂▂▃▂▂▂▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01037
wandb:    label_loss 0.00976
wandb: test_accuracy 96.61
wandb:     test_loss 0.01272
wandb:    train_loss 0.01146
wandb: 
wandb: 🚀 View run rosy-sweep-30 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/vxvaergk
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_162325-vxvaergk/logs
wandb: Agent Starting Run: 0tlwls42 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.19527029968986775
wandb: 	learning_rate: 0.003252657758693437
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_162521-0tlwls42
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/0tlwls42
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 254-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█████
wandb:    image_loss █▄▃▄▂▃▃▂▄▄▂▃▄▂▂▂▃▃▃▂▂▂▃▂▄▃▂▂▃▃▂▄▃▂▂▃▂▁▃▃
wandb:    label_loss ▄█▆█▂▃▅▄▄▅▅▅▁▃▂▁▅▅▁▂▂▃▁▆▂▄▂▂▂▂▂▂▁▃▁▂▁▅▂▂
wandb: test_accuracy ▁▄▆▇▇▇▇▇▆▇▅█▇▇▇
wandb:     test_loss ▄▂▂▂▃▁▅▄▇▃▅▃█▃▂
wandb:    train_loss ▄▆▆▇▆█▆▆▆▄▄▆▅▆█▄▆▇▆▄▇▆▅▇▄▅▃▄▇▄▇▄▆▄▃▅▄▅▁█
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01403
wandb:    label_loss 0.02584
wandb: test_accuracy 95.98
wandb:     test_loss 0.01621
wandb:    train_loss 0.0158
wandb: 
wandb: 🚀 View run dark-sweep-31 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/0tlwls42
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_162521-0tlwls42/logs
wandb: Agent Starting Run: oc8vd0yt with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.04798154966806467
wandb: 	learning_rate: 0.003137714542939861
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_162743-oc8vd0yt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/oc8vd0yt
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▃▂▂▃▂▃▄▃▅▄▅▅▃▃▅▃▄▆▂▁▅▃▃▃▅▅▄▄▅▄▃▃▄▁▃▄▃▃
wandb:    label_loss ▆█▅▅▇▄▃▃▄▄▂▅▁▃▆▇▃▂▂▅▂▂▂▂▄▇▁▂▄▁▃▄▂▃▁▃▁▃▂▂
wandb: test_accuracy ▁▄▅▇▇▇▇▇██▇▇▇▇█
wandb:     test_loss ▇▄▄▄▄▆█▅▆▄▁▅▇▄▆
wandb:    train_loss █▄▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁▂▂▂▁▂▂▂▂▂▁▁▂▁▁▁▁▂▂▂▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00677
wandb:    label_loss 0.00961
wandb: test_accuracy 95.73
wandb:     test_loss 0.00686
wandb:    train_loss 0.00687
wandb: 
wandb: 🚀 View run winter-sweep-32 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/oc8vd0yt
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_162743-oc8vd0yt/logs
wandb: Agent Starting Run: 7m6ffl5o with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.33158911872098473
wandb: 	learning_rate: 0.00017081402136502464
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_162939-7m6ffl5o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/7m6ffl5o
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:    image_loss █▅▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▅▄▆▄▃▂▂▂▃▂▃▃▂▁▂▂▂▅▂▂▁▂▁▂▁▁▁▂▃▁▁▁▁▁▁▂▂▁
wandb: test_accuracy ▁▂▄▅▅▇▆▇███████
wandb:     test_loss █▆▄▃▃▂▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00417
wandb:    label_loss 0.00687
wandb: test_accuracy 97.21
wandb:     test_loss 0.00678
wandb:    train_loss 0.00565
wandb: 
wandb: 🚀 View run eager-sweep-33 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/7m6ffl5o
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_162939-7m6ffl5o/logs
wandb: Agent Starting Run: pnr515oe with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4109035656736182
wandb: 	learning_rate: 0.0067360218400993105
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_163136-pnr515oe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/pnr515oe
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▆▅▄▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▃                 
wandb:    label_loss █▄▅▃▃▂▂▃▂▃▂▂▃▂▁▁▂▂▁▂▃                   
wandb: test_accuracy ████████▁▁▁▁▁▁▁
wandb:     test_loss █▄▂▁▁▁▃▇       
wandb:    train_loss █▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂                    
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run fallen-sweep-34 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/pnr515oe
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_163136-pnr515oe/logs
wandb: Agent Starting Run: xdrfesyy with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3500786072855185
wandb: 	learning_rate: 0.007153160929018903
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_163332-xdrfesyy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/xdrfesyy
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▅▅▃▂▁                                 
wandb:    label_loss █▄▃▃▂▅▁▁                                
wandb: test_accuracy ▇█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▁             
wandb:    train_loss █▅▄▄▃▂▂▂▂▁                              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run driven-sweep-35 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/xdrfesyy
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_163332-xdrfesyy/logs
wandb: Agent Starting Run: pnd58qjp with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17043248805597838
wandb: 	learning_rate: 0.0009082353518935926
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_163529-pnd58qjp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/pnd58qjp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▅█▄▅▃▄▃▃▃▃▃▃▂▃▃▂▂▁▁▂▁▂▃▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆▆▇▆█▇▇█████
wandb:     test_loss █▅▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00252
wandb:    label_loss 0.0014
wandb: test_accuracy 97.54
wandb:     test_loss 0.00388
wandb:    train_loss 0.00306
wandb: 
wandb: 🚀 View run vibrant-sweep-36 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/pnd58qjp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_163529-pnd58qjp/logs
wandb: Agent Starting Run: u6ar78w3 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.05185816108294017
wandb: 	learning_rate: 0.0038106610375686152
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_163710-u6ar78w3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/u6ar78w3
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▂▂▂▂▂▁▂▂▂▂▂▂
wandb:    label_loss ▇█▆▇▇▄▃▄▄▄▄▃▄▂▅▂▃▅▄▃▂▅▄▄▂▄▅▂▃▅▂▄▄▂▂▂▁▂▂▂
wandb: test_accuracy ▁▃▄▆▆▆▆▇▇▇▇▇███
wandb:     test_loss █▄▂▁▁▂▁▂▂▁▃▂▂▃▂
wandb:    train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00469
wandb:    label_loss 0.00812
wandb: test_accuracy 95.8
wandb:     test_loss 0.00529
wandb:    train_loss 0.00497
wandb: 
wandb: 🚀 View run warm-sweep-37 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/u6ar78w3
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_163710-u6ar78w3/logs
wandb: Agent Starting Run: d9vvy68s with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.431538621617416
wandb: 	learning_rate: 0.009582180391268038
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_163851-d9vvy68s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/d9vvy68s
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▇▆▆▆▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▁▂▃▃▁▂▂▆
wandb:    label_loss ██▆▅▄▂▃▂▂▂▂▂▃▂▁▂▁▂▂▂▂▂▂▁▁▁▁▁▃▂▁▁▁▂▂▁▁▁▂▂
wandb: test_accuracy ▁▆▆▇▇▇█████▇██▇
wandb:     test_loss ▃▂▂▂▁▁▁▁▁▁▁▁▁▁█
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06097
wandb:    label_loss 0.02119
wandb: test_accuracy 93.34
wandb:     test_loss 0.25482
wandb:    train_loss 0.04903
wandb: 
wandb: 🚀 View run vital-sweep-38 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/d9vvy68s
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_163851-d9vvy68s/logs
wandb: Agent Starting Run: ut7yphu6 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.33844279078411893
wandb: 	learning_rate: 0.0018276190640511873
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_164032-ut7yphu6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/ut7yphu6
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▆▅▃▃▂▃▂▂▂▂▂▂▁▁▂▂▁▂▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▃▃▂▃▅▂▃▄▂▂▄▂▃▂▁▃▂▂▁▂▁▄▂▁▂▂▂▃▃▂▁▁▁▂▁▁▁▁
wandb: test_accuracy ▁▄▅▆▅▆▆▇▆█▇▇█▇▇
wandb:     test_loss █▆▃▃▃▃▃▂▂▁▂▂▁▁▁
wandb:    train_loss █▇▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00561
wandb:    label_loss 0.00277
wandb: test_accuracy 97.44
wandb:     test_loss 0.00874
wandb:    train_loss 0.00695
wandb: 
wandb: 🚀 View run astral-sweep-39 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/ut7yphu6
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_164032-ut7yphu6/logs
wandb: Agent Starting Run: r20vpggy with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.24648343071582623
wandb: 	learning_rate: 0.007249948243246099
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_164229-r20vpggy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_elu/sweeps/18985tbe
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_elu/runs/r20vpggy
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 254-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▁                                      
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run quiet-sweep-40 at: https://wandb.ai/cavaokcava/auto_256_elu/runs/r20vpggy
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_164229-r20vpggy/logs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: quenmpnh with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.15092071588651745
wandb: 	learning_rate: 0.005841741935988734
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_164458-quenmpnh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/quenmpnh
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss ▄▃▃▇▅▇▄▁▁▃▆▄▄▄▅▂▃▃▃▅█▄▅▃▁▅▃▄▄▃▂▄▄▃▂▅▅▆▆▅
wandb:    label_loss ▅▃▇▃▂▂▄▃▆▃▁▃▂▃▂▆▂▃▂▃▃▃▃▂▄▃▁▆▃▇▄▃▃▆▆▄█▄▆█
wandb: test_accuracy ▄██▃▃█▃█▁██▂▄▂█
wandb:     test_loss ▁▄▇▅▁▆█▆█▅▄▆▅▇▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06714
wandb:    label_loss 0.23243
wandb: test_accuracy 11.35
wandb:     test_loss 0.10264
wandb:    train_loss 0.10249
wandb: 
wandb: 🚀 View run valiant-sweep-1 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/quenmpnh
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_164458-quenmpnh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qqbolj18 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.033678133479274286
wandb: 	learning_rate: 0.00891710781183159
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_164649-qqbolj18
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/qqbolj18
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇███
wandb:    image_loss ▃▄▆▅▇▆▄▃▃▆▄▄▄▅▅▆▄▄▆▃▄▆▅▅▃▅▄▄▅▂▆▅█▃▅▆▄▁▆▇
wandb:    label_loss ▃▆▅▅▆▆▆▃▆▃▃▄▅▂▆▄▃▄▆▃▅▁▅▅▄▄█▂▅▄▄▅▄█▄▄▅▄▅▆
wandb: test_accuracy ▃██▅▃█▅███▁█▅█▅
wandb:     test_loss █▄▄▅▇▁▂▆▂▆▃▃▆▇▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06983
wandb:    label_loss 0.23191
wandb: test_accuracy 10.28
wandb:     test_loss 0.07562
wandb:    train_loss 0.07531
wandb: 
wandb: 🚀 View run elated-sweep-2 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/qqbolj18
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_164649-qqbolj18/logs
wandb: Agent Starting Run: swfejnwo with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.28712284369767294
wandb: 	learning_rate: 0.004210256939169416
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_164831-swfejnwo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/swfejnwo
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 254-283, summary, console lines 205-231
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▃▅▅▅▅▃▂▆█▃▆▇▅▅▃▆▆▃▇▆▆█▆▁▇▆▆▃▇▆▆▅▁▆▄▅▄▂▇▂
wandb:    label_loss ▆▅▄▄▅▄▆▃▄▆▆▄▆▅▄▄▅▆▅▅▄▄▅▄▅▇▄▄▃▃▄▄▅▅█▄▁▅▆▄
wandb: test_accuracy ▅▅▄▅▄██▃▅▄█▄█▄▁
wandb:     test_loss ▁▃▁▁▂▂▂▇▃▃▃▂▂█▃
wandb:    train_loss █▂▁▁▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07021
wandb:    label_loss 0.23003
wandb: test_accuracy 8.92
wandb:     test_loss 0.13466
wandb:    train_loss 0.1345
wandb: 
wandb: 🚀 View run distinctive-sweep-3 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/swfejnwo
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_164831-swfejnwo/logs
wandb: Agent Starting Run: 3j8weopz with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.30111457060186786
wandb: 	learning_rate: 0.003963362249900009
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_165053-3j8weopz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/3j8weopz
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ▆▃▂▅▃▄▃▂▄▃▃█▅▄▁▃▃▅▅▂▁▆▃▆▆▅▄▃▁▄▄▅▄▃▇▂▁▄▂▃
wandb:    label_loss ▆▇▄▄▄▃▄▃▃▆▃▄▃▂▇▄▆▂▆▂▆▂▅▃▅▃▄▄▅▁▄▅▇█▄▃▇▄▅▂
wandb: test_accuracy ███▁█▁▁▂███▂█▂█
wandb:     test_loss ▄▃▅▅█▂▆▅▇▅▅▄█▇▁
wandb:    train_loss █▅▅▆▄▆▄▆▆▆▂▄▅▅▆▄▆▄▅▂▄▅▄▃▅▃▅▆▃▄▅▆▃▁▃▆▅▆▄▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06425
wandb:    label_loss 0.2292
wandb: test_accuracy 11.35
wandb:     test_loss 0.13716
wandb:    train_loss 0.1373
wandb: 
wandb: 🚀 View run sunny-sweep-4 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/3j8weopz
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_165053-3j8weopz/logs
wandb: Agent Starting Run: p104ulvf with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3018959067800238
wandb: 	learning_rate: 0.001968275238928123
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_165249-p104ulvf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/p104ulvf
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇█████
wandb:    image_loss █▅▃▂▃▂▂▂▁▂▁▁▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▃▅█▂▂▂▁▇▂▁▁▁▁▁▄▁▂▁▄▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▆█▇██▆██▅█▇▇█▇
wandb:     test_loss █▄▃▃▂▂▂▁▁▂▁▁▁▁▁
wandb:    train_loss █▇▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0056
wandb:    label_loss 0.00011
wandb: test_accuracy 97.9
wandb:     test_loss 0.00905
wandb:    train_loss 0.00726
wandb: 
wandb: 🚀 View run copper-sweep-5 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/p104ulvf
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_165249-p104ulvf/logs
wandb: Agent Starting Run: diagieur with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.20976905821639796
wandb: 	learning_rate: 0.00329140694429098
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_165511-diagieur
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/diagieur
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▇▄▇▆▃▁█▃▃▃▂▃▃▃▁▃▂▂▄▄▃▂▃▁█▁▂▂▁▁▁▂▁▃▂▁▁▂▁▁
wandb: test_accuracy ▁▅▆▆▆▇▇▆▇▆▇▇█▇▇
wandb:     test_loss █▄▃▂▂▂▂▂▂▂▁▂▁▁▂
wandb:    train_loss █▆▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00642
wandb:    label_loss 0.00468
wandb: test_accuracy 97.93
wandb:     test_loss 0.00921
wandb:    train_loss 0.0067
wandb: 
wandb: 🚀 View run prime-sweep-6 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/diagieur
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_165511-diagieur/logs
wandb: Agent Starting Run: b79ipsol with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.393871263714259
wandb: 	learning_rate: 0.008231116955241985
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_165708-b79ipsol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/b79ipsol
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇██
wandb:    image_loss ▄▂▅▆▅▃▆▆█▅▆▃▃▄▄▄▄▆▁▄▄▆▃▅▅▇▃▄▃▂▇▅▅▅▃▅▅▂█▇
wandb:    label_loss ▇▆▄▇▆▆▇▇▁▄▅▇▅▆▅▅▄▃▄▄▅▃▇▅▇▆▅▇▃▄▇█▇▄▅▅▃▇▄▅
wandb: test_accuracy ▁▃▃▃███▃█▂▂█▂▃▃
wandb:     test_loss ▃█▁▁▂▆▂▃▁█▂▆▅██
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07048
wandb:    label_loss 0.23027
wandb: test_accuracy 10.28
wandb:     test_loss 0.15904
wandb:    train_loss 0.15856
wandb: 
wandb: 🚀 View run dark-sweep-7 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/b79ipsol
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_165708-b79ipsol/logs
wandb: Agent Starting Run: nbg9koot with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17460158138401305
wandb: 	learning_rate: 0.004064843036234103
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_165854-nbg9koot
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/nbg9koot
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▅▅▁▇█▅▃▂▆▃▂▃▄▆▃▄▃▄▄▇▂▃▅▇▅▇▄▂▁▄▃▆▃▃▃▄▃▆▄▃
wandb:    label_loss ▆▄▅▅▄▆▃▇▃▁▅▅█▃▅▇▄▃▆▄█▂▇▂▇▅▅▄▆▅▄▄▅▂▄▆▄▅▆▄
wandb: test_accuracy ▁▃▂███▃█▁▂█▂█▁▂
wandb:     test_loss ▃▃█▁▁▁▂▂▃▂▅▄▂▂▄
wandb:    train_loss ▄▄▃▂▅▅▅▆▅▄▄▄▅▄▅▃▆▂▂▃▆▅▂▄▅▁█▃▄▄▅▄▂▄▆▅▄▅▂▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06915
wandb:    label_loss 0.23085
wandb: test_accuracy 10.1
wandb:     test_loss 0.10834
wandb:    train_loss 0.10837
wandb: 
wandb: 🚀 View run grateful-sweep-8 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/nbg9koot
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_165854-nbg9koot/logs
wandb: Agent Starting Run: aef8lvea with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3502693429723154
wandb: 	learning_rate: 0.008460545399161242
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_170050-aef8lvea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/aef8lvea
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▅▅▁▄▆▅▅▄▅▅▄█▇▄▄▆▃█▃▆▃▆▂▅▃▅▄▂▆▆▃▄▅▃▃▄▃▅▂▃
wandb:    label_loss ▄▄▃▂▅█▅▄▄▂▁▂▃▆▄▄▃▅▅▄▄▆▆▃▂▆▅▅▁▃▄▄▅▅▃▂▅▂▂▁
wandb: test_accuracy ▄█████▁▃█▁███▃▁
wandb:     test_loss ▇▅▁▃▆▃▄▇▄▆▁█▅▃▂
wandb:    train_loss ▄▄▄▅▄█▅█▄▁▃▁▃▅▄▅█▃▃▄▃▄▅▅▅▄▇▃▂▃▇▄▂▃▇▁▃▄▅▇
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06931
wandb:    label_loss 0.23125
wandb: test_accuracy 9.74
wandb:     test_loss 0.14897
wandb:    train_loss 0.14927
wandb: 
wandb: 🚀 View run daily-sweep-9 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/aef8lvea
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_170050-aef8lvea/logs
wandb: Agent Starting Run: 7yhbcb65 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.0694368913195832
wandb: 	learning_rate: 0.0010224193159743924
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_170312-7yhbcb65
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/7yhbcb65
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▅▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▄█▃▄▃▅▃▂▃▃▆▂▂▁▁▃▂▅▂▁▂▂▃▁▁▂▂▁▁▁▃▁▁▁▁▂▁▁▂▂
wandb: test_accuracy ▁▅▇▆▇█▆█▇▇▆█▅▇█
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00337
wandb:    label_loss 0.00345
wandb: test_accuracy 98.09
wandb:     test_loss 0.0033
wandb:    train_loss 0.00306
wandb: 
wandb: 🚀 View run ancient-sweep-10 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/7yhbcb65
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_170312-7yhbcb65/logs
wandb: Agent Starting Run: ksjbz2sv with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3697894161592887
wandb: 	learning_rate: 0.004229262717386776
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_170534-ksjbz2sv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ksjbz2sv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▇▇▄▂▂▃▂▃▃▇▅▅▁▄▆▃▅▂▄▆▂▂▂▂▃▄▃▇▄▅▃▅▅▃▅▆▅▁▅█
wandb:    label_loss ▆▅▇█▇▇▇▃▆▂▅▄▇▆▆▅▇▆▇▅▇▁▇▅▇▅▅▆▇▇▅▃▆▇▇█▅▇▇▄
wandb: test_accuracy ▄█▄▃█▄▃██▄█▃▂▄▁
wandb:     test_loss ▇▆▁▁▄▆▆▅▆█▄▂█▃▄
wandb:    train_loss ▄▄▁█▄▄▃▂▄▃▆▁▂▅▂▄▃▃▁▃▄▁▂▇▆▃▂▆▅▄▃▄▄▄▄▄▄▂▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07778
wandb:    label_loss 0.22774
wandb: test_accuracy 9.58
wandb:     test_loss 0.15351
wandb:    train_loss 0.15322
wandb: 
wandb: 🚀 View run feasible-sweep-11 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ksjbz2sv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_170534-ksjbz2sv/logs
wandb: Agent Starting Run: ts48a8hr with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.27344760364669773
wandb: 	learning_rate: 0.008022256406193858
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_170755-ts48a8hr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ts48a8hr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▁▆▅▆▅▇▅▇▅▄▆▇▃▂▅▄▃▂█▅▅▄▆▅▆▃▇▃▆▆▄▇▆▃▄▂▅▃▂
wandb:    label_loss ▅▅▆▄▄▅▅▄▄▁▇▄▆▅▄▅▄▄▇▅▆▅▅▃▄▅█▇▆▅▅▁▆▅▄▆▅▄▆▅
wandb: test_accuracy █▁██████▂█▁███▄
wandb:     test_loss ▇█▅▆▆▅▅▅▅▇▅▁▂▂▂
wandb:    train_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06154
wandb:    label_loss 0.22965
wandb: test_accuracy 10.28
wandb:     test_loss 0.13074
wandb:    train_loss 0.13061
wandb: 
wandb: 🚀 View run balmy-sweep-12 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ts48a8hr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_170755-ts48a8hr/logs
wandb: Agent Starting Run: k84i7io8 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2045692301419706
wandb: 	learning_rate: 0.004053979242047987
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_171017-k84i7io8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/k84i7io8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▇▆▅▆▅▅▂▃▇▂▆▃▆▆▇▆▄▆▄▄▃▃▅▁▆▅▅▃▆▄▄▆▇█▁▅▅▃▇▂
wandb:    label_loss ▂▂▅▂▄▃▃▄▅▄▃▃▃█▅▃▂▃▂▂▃▅▃▆▃▃▃▁▁▃▂▂▂▃▂▄▃▄▃▂
wandb: test_accuracy ▁██▃▁██▄▃▄██▃██
wandb:     test_loss ▇█▄▅▇█▇█▇▅▂▄▃▄▁
wandb:    train_loss █▂▁▂▂▂▁▂▂▂▁▁▂▂▂▂▂▂▁▁▁▁▂▁▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06647
wandb:    label_loss 0.22993
wandb: test_accuracy 11.35
wandb:     test_loss 0.11465
wandb:    train_loss 0.11417
wandb: 
wandb: 🚀 View run easy-sweep-13 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/k84i7io8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_171017-k84i7io8/logs
wandb: Agent Starting Run: qh40yyje with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3286597532703755
wandb: 	learning_rate: 0.00029741094526106627
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_171213-qh40yyje
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/qh40yyje
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▆▇▇▇▇▇▇▇████
wandb:     test_loss █▆▅▄▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01043
wandb:    label_loss 0.002
wandb: test_accuracy 97.82
wandb:     test_loss 0.01297
wandb:    train_loss 0.01233
wandb: 
wandb: 🚀 View run mild-sweep-14 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/qh40yyje
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_171213-qh40yyje/logs
wandb: Agent Starting Run: xtr3x59s with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.19276274658017
wandb: 	learning_rate: 0.007389989514810262
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_171355-xtr3x59s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/xtr3x59s
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 255-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ▂█▃▅▄▄▁▂▄▄▂▅▁▅▃▃▃▃▄▂▃▄▂▃▂▂▅▄▂▃▂▃▁▃▂▅▃▄▄▅
wandb:    label_loss ▄▇▇▄▄▆▄▆▄▅▅▅▆▄▁▄▃▄▅█▄▅▅▅▂▆▅▅▄▃▄▂▆▅▄▃▃▆▄▅
wandb: test_accuracy ▃▃▃▁▃▃▃█▁▄▃▃▃█▁
wandb:     test_loss ▅▃▁▃▄▄▂█▄▄▄▅▃▂▂
wandb:    train_loss ▄▄▅▅▂▂▁▅▃▄▁▆▂▂▂▅▆▅▅▁▅▁▄█▃▃▆▄▃▅▅▇▄▆▃▆▅▄▇▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07251
wandb:    label_loss 0.22902
wandb: test_accuracy 9.74
wandb:     test_loss 0.11314
wandb:    train_loss 0.11304
wandb: 
wandb: 🚀 View run light-sweep-15 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/xtr3x59s
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_171355-xtr3x59s/logs
wandb: Agent Starting Run: g0xr7q9x with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07481034728558489
wandb: 	learning_rate: 0.00477649809956426
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_171616-g0xr7q9x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/g0xr7q9x
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▆▄▄▃▄▁▇▂▅▆▃▄▆▃▅▄▄▃▇▇▆█▇▅▄▆▇▆▇▇▅▃▇▆▇▆▄▂▄▆
wandb:    label_loss ▁▄▄▆▄▅▅▃▄▂▃▁▄▃▂▃▃▃▂▂▂▃▂▂▂▂█▂▁▂▂▃▃▁▂▆▃▄▂▅
wandb: test_accuracy ▃██▁███████████
wandb:     test_loss █▂▂▂▂▃▂▂▁▂▃▁▂▂▂
wandb:    train_loss ▆█▆▃▁▄▃▃▁▅▁▄▁▅▄▄▆▄▂▃▃▅▃▅▅▃▄▂▃▄▃▅▃▄▃▃▃▄▂▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06917
wandb:    label_loss 0.23147
wandb: test_accuracy 11.35
wandb:     test_loss 0.08492
wandb:    train_loss 0.08441
wandb: 
wandb: 🚀 View run curious-sweep-16 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/g0xr7q9x
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_171616-g0xr7q9x/logs
wandb: Agent Starting Run: xh4zdq2w with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4389937715218958
wandb: 	learning_rate: 0.0023184750779215004
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_171838-xh4zdq2w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/xh4zdq2w
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▅▄▃▃▂▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▇▇█▇██▇▇██▇█▇
wandb:     test_loss █▅▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00609
wandb:    label_loss 0.00018
wandb: test_accuracy 97.89
wandb:     test_loss 0.00908
wandb:    train_loss 0.00658
wandb: 
wandb: 🚀 View run ruby-sweep-17 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/xh4zdq2w
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_171838-xh4zdq2w/logs
wandb: Agent Starting Run: szktuw8o with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2982796005448523
wandb: 	learning_rate: 0.0026910225655969715
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_172019-szktuw8o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/szktuw8o
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ██▆▄▄▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂▁▁▂
wandb:    label_loss ▃▄▅▂█▁▁▁▁▁▁▁▃▂▁▂▄▁▁▁▁▁▁▁▁▁▁▁▁▂▆▁▁▂▁▁▁▁▁▂
wandb: test_accuracy ▁▇▇▇█▇▆▄█▇█▆▇▆▇
wandb:     test_loss █▄▃▃▂▂▂▂▁▂▁▁▁▂▁
wandb:    train_loss █▇▆▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▂▂▁▁▁▂▁▂▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00779
wandb:    label_loss 0.00415
wandb: test_accuracy 97.73
wandb:     test_loss 0.01088
wandb:    train_loss 0.0093
wandb: 
wandb: 🚀 View run spring-sweep-18 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/szktuw8o
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_172019-szktuw8o/logs
wandb: Agent Starting Run: yjnyzfak with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.32972301908375695
wandb: 	learning_rate: 0.008239887370553403
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_172241-yjnyzfak
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/yjnyzfak
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▄▇▄▂▁▅▅▃▄▇▃▅▃▆▂▂▄▄▆▅▂▇▁▃▄▃▁▆▅▃█▃▃▇▆▂▁▆▄▄
wandb:    label_loss ▅▅▆▃▄▄▂▃▅▂▅█▄▆▆▂▅▆▆▄▁▅▃▄▄▁▅▇▄▅▃▄▄▄▃▆▃▅▃▆
wandb: test_accuracy █▄▁▄█▃▁█▄█▃▂█▃█
wandb:     test_loss ▁▄▅▂▃▂▄▁▇▃▄█▃▃▄
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06706
wandb:    label_loss 0.23164
wandb: test_accuracy 11.35
wandb:     test_loss 0.1442
wandb:    train_loss 0.14389
wandb: 
wandb: 🚀 View run earthy-sweep-19 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/yjnyzfak
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_172241-yjnyzfak/logs
wandb: Agent Starting Run: 2d7ur445 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.11482480452111232
wandb: 	learning_rate: 0.005177024817892951
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_172422-2d7ur445
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/2d7ur445
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▄▁▅▄▄▃▄▃▄▅▂▄▂▅▄▃▁▆▂▃▄▄█▆▄▃▆▄▄▃▄▄▄▇▄▃▄▄▆▅
wandb:    label_loss ▆▆▄▃▅▄▇▅▅▄█▅█▄▃▃▆▆▄▄▂▁▇▆█▄▇▄▅▆▅▇▇▄▄▇█▇▆▄
wandb: test_accuracy █▁█▄▃█▄█▁▄▄▄█▄▄
wandb:     test_loss ▂▂▁▁▂▁█▅█▇█▅█▅▄
wandb:    train_loss ▂▂▃▁▄▂▁▄▃▂▂▂▂▂█▅▆▇▆▅▆▆█▆▅▇▆▇▇▅▇▅▅▆▄▅▅▆▅▆
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06958
wandb:    label_loss 0.22946
wandb: test_accuracy 10.28
wandb:     test_loss 0.095
wandb:    train_loss 0.09633
wandb: 
wandb: 🚀 View run feasible-sweep-20 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/2d7ur445
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_172422-2d7ur445/logs
wandb: Agent Starting Run: jmi3bkrj with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3200938511415303
wandb: 	learning_rate: 0.00205114443270934
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_172644-jmi3bkrj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/jmi3bkrj
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▄▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▅▂▂▂▃▃▁▂▃▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁
wandb: test_accuracy ▁▃▅▆▅▇▇▇▆▄▅█▅▆▇
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▂▁▁▂▁
wandb:    train_loss █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00776
wandb:    label_loss 0.00014
wandb: test_accuracy 98.08
wandb:     test_loss 0.00893
wandb:    train_loss 0.00786
wandb: 
wandb: 🚀 View run gallant-sweep-21 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/jmi3bkrj
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_172644-jmi3bkrj/logs
wandb: Agent Starting Run: mn3ntczl with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2202656874734092
wandb: 	learning_rate: 0.0052471379575567195
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_172906-mn3ntczl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/mn3ntczl
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 254-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇███
wandb:    image_loss ▄▇▅▆▇▇█▆▄▆▄▄▅▄▂█▅▄▄▄▇▂▂▂▃▆▂▂▃▂▃▁▁▇▂▄▄▁▂▁
wandb:    label_loss ▄█▁▅▃▂▄▅▅▁▄▂▄▄▁▆▅▅▅▆▅▃▁▂▄▅▄▅▇▄▆▅▆▅▆▂▃▄▄▅
wandb: test_accuracy ▂██▁▃▃▂▃▃▃▃▃▁██
wandb:     test_loss ▄▄▃▃▃▃▅█▂▄▁▃▄▄▄
wandb:    train_loss ▆▅▇▄▆▅▇▅█▇▇▄▅▅▄▃▄▇█▄▅▃▇▇▆▇▂▄▅▅▃▄▅▁▆▇▆▄▆▆
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06303
wandb:    label_loss 0.23129
wandb: test_accuracy 11.35
wandb:     test_loss 0.11884
wandb:    train_loss 0.11812
wandb: 
wandb: 🚀 View run curious-sweep-22 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/mn3ntczl
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_172906-mn3ntczl/logs
wandb: Agent Starting Run: ao8jq2l1 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.20175585629421777
wandb: 	learning_rate: 0.005125897200221064
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_173128-ao8jq2l1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ao8jq2l1
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▄▂▆▆▅▅▃▅▄▁▅▆▆▇▃▄▄█▄▂▄▄▄▄▆▆▃▂▃▂▅▄▄▃▄▆▅▁▅▂
wandb:    label_loss █▇▂▂▇▄▅▅▅▂▃▁▁▂▃▂▇▂▁▅▆█▂▆▂▇▆▂▅▄▁▄▄▃▅▇▁▇▄▁
wandb: test_accuracy █▃▄█▁▄▁▄▄█▂██▂█
wandb:     test_loss ▄▅▇▇█▄▁▂▂▂▄▂▃▁▅
wandb:    train_loss █▂▁▁▁▂▂▁▂▂▁▂▂▁▁▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06384
wandb:    label_loss 0.22838
wandb: test_accuracy 11.35
wandb:     test_loss 0.11456
wandb:    train_loss 0.11366
wandb: 
wandb: 🚀 View run firm-sweep-23 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ao8jq2l1
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_173128-ao8jq2l1/logs
wandb: Agent Starting Run: fx4ixg62 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.35942840480913596
wandb: 	learning_rate: 0.004983989154697996
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_173349-fx4ixg62
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/fx4ixg62
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss ▃▄▅▄▅▂▂▂▁▅▂▂▁▅▂▅▆▆▄▂▄▄▄▅▄▃▅█▅▅▄▆▆▁▁▃█▄▄▄
wandb:    label_loss ▅▂▅▄▄▅▇▁▄▁▄▅▃▃▅▅▄▆▃▂▃▄▃▄▅▇▅▄▃▅▆▄█▆▃▄▃▃▅▆
wandb: test_accuracy █▁██▂█▂█▂▃████▃
wandb:     test_loss ▃▂▂▁▃▁▄▅▅▄▇▇▃█▄
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06775
wandb:    label_loss 0.23138
wandb: test_accuracy 10.32
wandb:     test_loss 0.15083
wandb:    train_loss 0.15039
wandb: 
wandb: 🚀 View run eternal-sweep-24 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/fx4ixg62
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_173349-fx4ixg62/logs
wandb: Agent Starting Run: qxqyrcdr with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1655628007393195
wandb: 	learning_rate: 0.004021302679390047
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_173531-qxqyrcdr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/qxqyrcdr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss █▇▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▆▆▇█▇▇▇██▇▇▇▇▇
wandb:     test_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss █▆▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00594
wandb:    label_loss 0.00194
wandb: test_accuracy 97.45
wandb:     test_loss 0.00742
wandb:    train_loss 0.00646
wandb: 
wandb: 🚀 View run soft-sweep-25 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/qxqyrcdr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_173531-qxqyrcdr/logs
wandb: Agent Starting Run: wqhkgo7z with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.12456383058110591
wandb: 	learning_rate: 0.0007971690446470313
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_173727-wqhkgo7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/wqhkgo7z
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇████
wandb:    image_loss █▇▅▄▄▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▄▄▃▂▃▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▆▇▇▇████▇▇▇▇▇
wandb:     test_loss █▅▄▃▃▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▆▅▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00365
wandb:    label_loss 0.00096
wandb: test_accuracy 97.89
wandb:     test_loss 0.00463
wandb:    train_loss 0.00393
wandb: 
wandb: 🚀 View run royal-sweep-26 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/wqhkgo7z
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_173727-wqhkgo7z/logs
wandb: Agent Starting Run: u9orzg4p with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.15120099329140518
wandb: 	learning_rate: 0.0009417066854113116
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_173923-u9orzg4p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/u9orzg4p
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▅▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▇██▃▃▁▃▁▁▁▃▂▁▂▁▁▁▁▁▁▁▂▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆██▇▇██▇▇█▆█
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▂▂▁▁▁
wandb:    train_loss █▆▆▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00361
wandb:    label_loss 0.00012
wandb: test_accuracy 98.17
wandb:     test_loss 0.00449
wandb:    train_loss 0.0039
wandb: 
wandb: 🚀 View run true-sweep-27 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/u9orzg4p
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_173923-u9orzg4p/logs
wandb: Agent Starting Run: ou2ot6je with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.46583693364745626
wandb: 	learning_rate: 0.003967541908408141
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_174145-ou2ot6je
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ou2ot6je
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█
wandb:    image_loss ▁▄▃▁▂▄▄▅▃▂▆▄▄▂▃▃▅▂▃▄█▄▄▅▁▂▇▂▃▅▆▂▂▁▅▅▃▃▃▂
wandb:    label_loss ▆▆▆▅▇▆▅▅▆▅▆▅▄▄▆▅▆▄▅▆▂▄▃▅▅▇█▄▄▅▆▁▅▆▅▄▅▄▄▆
wandb: test_accuracy ▂▂▃▃██▄▄▂▄█▂█▂▁
wandb:     test_loss ▂▇▁▃▃▆▃▃▃▁▅▅▃▇█
wandb:    train_loss ▆▅▇▃▆▅▃▆▄▃▆▆▅▄▄▃▂▁█▃▃▆▄▄▃▃▅▄▄▇▄▅▅▅▅▄▇▅▅▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06823
wandb:    label_loss 0.23245
wandb: test_accuracy 9.58
wandb:     test_loss 0.17641
wandb:    train_loss 0.17591
wandb: 
wandb: 🚀 View run resilient-sweep-28 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ou2ot6je
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_174145-ou2ot6je/logs
wandb: Agent Starting Run: q59utshw with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.33260239641952555
wandb: 	learning_rate: 0.007310849640034515
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_174407-q59utshw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/q59utshw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss ▂▆▃▇▆▅▄▅▃▃▅▅▄▃▅▇▇▆▃▁▆▆▂▅▆▄▄▃▄▃▄▃▃▃▃▄▅▆█▃
wandb:    label_loss ▆▃▆▇▃▃▅▄▂▄▁▇▄▇▆▄▂▂▁▄▂▂▃█▅▃▆▅▄▄▃▅▄▆▅▅▆▅▂▅
wandb: test_accuracy ▂▂███▃█▂▂██▃██▁
wandb:     test_loss ▃▂▃▄▃▃▁▄▅▂▄▆▂▁█
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06654
wandb:    label_loss 0.23066
wandb: test_accuracy 9.8
wandb:     test_loss 0.14457
wandb:    train_loss 0.14408
wandb: 
wandb: 🚀 View run dulcet-sweep-29 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/q59utshw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_174407-q59utshw/logs
wandb: Agent Starting Run: 51lhpetl with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.21623952610114333
wandb: 	learning_rate: 0.0043196299363553155
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_174548-51lhpetl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/51lhpetl
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ▂▄▅▆▅▇▁█▂█▅▅▃▃▆▂▃▃▂▇▄▄▅▅▄▃▅▃▃▅▅▃▄▇▅▅▅▂▆▅
wandb:    label_loss ▃▄▅▅▅▄▇▂▄▂▄▅▁▅▅▆▄▅█▅▆▃▄▆▅▃▁█▂▅▇▅▅▄▅▆▅▅▅▆
wandb: test_accuracy ▄▁█▂▂█▂▄▄█▃███▂
wandb:     test_loss ██▅▆▅▅▆▂▃▃▄▅▄▆▁
wandb:    train_loss █▂▁▂▁▂▂▁▁▂▁▁▂▂▁▁▁▂▁▂▁▂▂▂▁▁▂▁▂▂▂▂▁▂▁▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06776
wandb:    label_loss 0.22987
wandb: test_accuracy 9.74
wandb:     test_loss 0.11753
wandb:    train_loss 0.11761
wandb: 
wandb: 🚀 View run elated-sweep-30 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/51lhpetl
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_174548-51lhpetl/logs
wandb: Agent Starting Run: iuwua5c5 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.30178388613826196
wandb: 	learning_rate: 0.0055641625845814485
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_174745-iuwua5c5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/iuwua5c5
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:    image_loss ███▇▇▇▇▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▆▄▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▇█████████████
wandb:     test_loss █▅▃▂▂▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▇▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00837
wandb:    label_loss 0.00268
wandb: test_accuracy 97.54
wandb:     test_loss 0.01216
wandb:    train_loss 0.00885
wandb: 
wandb: 🚀 View run fresh-sweep-31 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/iuwua5c5
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_174745-iuwua5c5/logs
wandb: Agent Starting Run: mipau9dn with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.215486563671273
wandb: 	learning_rate: 0.009270523177792356
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_174926-mipau9dn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/mipau9dn
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▇▅▅▄▃█▁▄▄▄▅▅▄▅▆▃▅▃▄▆█▄▇▆▅▆▅▅▃▃▅▅▃▄▅▄▅▅▆▆
wandb:    label_loss ▃▅▇▃▆▃▄▄▂▅▄▃▁▄▄▃▄▄▄▆▅▄▄█▃▄▄▄▄▇▄▂▅▅▃▅▄▃▃▃
wandb: test_accuracy ▂▂▄▄██▄▃▁▂▂███▃
wandb:     test_loss ▁█▂▁▂▂▄▂▂▄▃▂▃▁▂
wandb:    train_loss ▄▄▄▂▅▂▅▅▆▅▁▆█▇▄▃▂▅▃▆▄▅▄▄▄▆▄▇▅▇▆▄▅▅▅▆▅▄▂▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07023
wandb:    label_loss 0.23144
wandb: test_accuracy 10.09
wandb:     test_loss 0.11789
wandb:    train_loss 0.11804
wandb: 
wandb: 🚀 View run atomic-sweep-32 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/mipau9dn
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_174926-mipau9dn/logs
wandb: Agent Starting Run: m92lpzpp with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.16119782541493272
wandb: 	learning_rate: 0.005901071536800897
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_175123-m92lpzpp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/m92lpzpp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▅▆▄▂▄█▃▂▄▄▆▆▃▃▆▃▂▅▄▂▄▄▇▅▄▅▁▅▃▅▃▆▂▄▃▁▄▃▄▆
wandb:    label_loss ▄▄▅▄▄▅▄▆▅▄▃▁▇█▃▆▄▅▇▃▅▅▇▆▄▅▂▇▂▄▄▄▄▅▄▄▆▄▆▅
wandb: test_accuracy ▂▃▄█▂▃▂▄▂█▂▂▁██
wandb:     test_loss ▆▃▃▆▅█▇▄▁▁▁▂▂▁▁
wandb:    train_loss ▁▂█▃▃▇▄▆▇▃▆▆▄▅▅▄▇▅▄▇▆▇█▇▄▄▄▄▄▄▃▅▅▅▃▇▅█▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07156
wandb:    label_loss 0.23115
wandb: test_accuracy 11.35
wandb:     test_loss 0.10487
wandb:    train_loss 0.10457
wandb: 
wandb: 🚀 View run zesty-sweep-33 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/m92lpzpp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_175123-m92lpzpp/logs
wandb: Agent Starting Run: zsome317 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.307827987809049
wandb: 	learning_rate: 0.005146655750428565
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_175319-zsome317
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/zsome317
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 254-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇█
wandb:    image_loss ▃▃▅▂▇▇▃█▄▆▃▄▅▄▄▃▁▄▂▅▄▅▂▅▃▅▅▆▄▆▅▃▃▃▆▆▄▄▂▃
wandb:    label_loss ▆▄▁▆▅▄▅▂▄▇▃▄▄▂▄▁▆▇▆▃▄██▆▅▅▃▄▆▃▂▄▃▅▅█▆▆▃▇
wandb: test_accuracy ▄▄▄▄█▃▃▁▄▄██▄█▃
wandb:     test_loss ▅█▅▅▅▅▇▅▄▁▄▁▅█▇
wandb:    train_loss █▂▂▁▁▁▂▁▁▁▁▂▁▂▂▁▁▁▂▂▂▂▁▁▂▂▁▁▁▁▁▂▁▁▁▁▂▂▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07
wandb:    label_loss 0.22765
wandb: test_accuracy 10.1
wandb:     test_loss 0.13935
wandb:    train_loss 0.1388
wandb: 
wandb: 🚀 View run electric-sweep-34 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/zsome317
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_175319-zsome317/logs
wandb: Agent Starting Run: dtg7s9pq with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.412818351356733
wandb: 	learning_rate: 0.0012355896134161174
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_175541-dtg7s9pq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/dtg7s9pq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▇▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▂▂▂▁▂▂▂▁▃▂▁▂▂▂▁▂▁▁▁
wandb:    label_loss ▆▅█▃▂▂▂▁▁▁▁▁▂▃▂▁▁▁▃▁▁▁▁▁▁▁▁▁▁▂▁▃▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇▇▇█▇█▇▇▅▇▇
wandb:     test_loss █▅▄▃▂▂▂▂▂▁▁▁▂▁▁
wandb:    train_loss ███▇▇▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00567
wandb:    label_loss 0.00192
wandb: test_accuracy 97.99
wandb:     test_loss 0.00876
wandb:    train_loss 0.00704
wandb: 
wandb: 🚀 View run stellar-sweep-35 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/dtg7s9pq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_175541-dtg7s9pq/logs
wandb: Agent Starting Run: gzglno2u with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.119959759985862
wandb: 	learning_rate: 0.006962478888020985
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_175803-gzglno2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/gzglno2u
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 254-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▅█▇▅▆▄█▇▆▅▅▅▄▆▅▅▄▇▇▄▆▄▅▇▄▆▅▄▁▇▅▅█▇▆▅▅▅▅▇
wandb:    label_loss ▁▃█▂▅▅▅▇▄▅▅▃▅▅▅▅▅▃▄▇▅▆▄▂▆▇▃▂▄▄▄▇▇▅▂▇▇▅█▆
wandb: test_accuracy ▃▂▂▃▂█▂▃█▁▂██▃▁
wandb:     test_loss ▃█▂▄▃▄▄▄▁▄▁▂▂▇▂
wandb:    train_loss ▆▆█▅▆▄▇▆▅▃█▆▄▄▁▅▅▄▁▄▃▅▃▃▅▄▂▇▆▃▅▄▄▄▆▄▄▃▄▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06889
wandb:    label_loss 0.23164
wandb: test_accuracy 9.82
wandb:     test_loss 0.09576
wandb:    train_loss 0.09563
wandb: 
wandb: 🚀 View run wise-sweep-36 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/gzglno2u
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_175803-gzglno2u/logs
wandb: Agent Starting Run: ckl4sb2v with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.282706872797549
wandb: 	learning_rate: 0.0034625746858470818
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_180024-ckl4sb2v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ckl4sb2v
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████
wandb:    image_loss █▃▃▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁
wandb: test_accuracy ▁▄▇▇▇▆▇▇█▇█▇▇█▆
wandb:     test_loss █▄▃▂▂▂▂▂▁▁▁▁▂▁▂
wandb:    train_loss █▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01043
wandb:    label_loss 6e-05
wandb: test_accuracy 97.12
wandb:     test_loss 0.01451
wandb:    train_loss 0.01182
wandb: 
wandb: 🚀 View run devoted-sweep-37 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/ckl4sb2v
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_180024-ckl4sb2v/logs
wandb: Agent Starting Run: b10titmx with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2090025923244944
wandb: 	learning_rate: 0.0014880633062168374
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_180246-b10titmx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/b10titmx
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▅▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▃▂▂▁▁▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▇▇▇█▇▇▇▇▇█▇▇▇
wandb:     test_loss █▅▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00404
wandb:    label_loss 0.00018
wandb: test_accuracy 97.54
wandb:     test_loss 0.00636
wandb:    train_loss 0.00454
wandb: 
wandb: 🚀 View run solar-sweep-38 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/b10titmx
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_180246-b10titmx/logs
wandb: Agent Starting Run: z4bj6l64 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4287357384904804
wandb: 	learning_rate: 0.008021850752560969
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_180427-z4bj6l64
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/z4bj6l64
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss ▂▅▃▂▃▄▅▇▄▄▃▃▄▃▄▄▄▃▄▂▄▁▆▇▃█▄▃▄▄▃▅▄▄▄▃▃▄▄▆
wandb:    label_loss ▃▄▃▃▃▅▄▄▄▆▅▃▄▅▂▅▄▃▄▄▅▄▅▅▄▅▁▇▄▄▅▆▆▇█▄▆▆▂▄
wandb: test_accuracy ▁▃▃█▂▁█▂▄▄▃▁▃██
wandb:     test_loss ▃▂▃▂▁▅▁█▂▂▂▃▅▇▆
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06538
wandb:    label_loss 0.23037
wandb: test_accuracy 11.35
wandb:     test_loss 0.16824
wandb:    train_loss 0.16781
wandb: 
wandb: 🚀 View run faithful-sweep-39 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/z4bj6l64
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_180427-z4bj6l64/logs
wandb: Agent Starting Run: rg00lvut with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.10510456743272206
wandb: 	learning_rate: 0.0003649947562205449
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_180624-rg00lvut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_256_sigmoid/sweeps/9up7qf8x
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/rg00lvut
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▂▂▁▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▆▆▇▇▇███████
wandb:     test_loss █▅▄▃▃▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00641
wandb:    label_loss 0.00342
wandb: test_accuracy 97.28
wandb:     test_loss 0.00743
wandb:    train_loss 0.00723
wandb: 
wandb: 🚀 View run polar-sweep-40 at: https://wandb.ai/cavaokcava/auto_256_sigmoid/runs/rg00lvut
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_256_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_180624-rg00lvut/logs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: v3yuoadi with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.31501424083941715
wandb: 	learning_rate: 0.00587507706475622
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_180809-v3yuoadi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/v3yuoadi
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 273-284, summary, console lines 220-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▆▄▄▃▂▂▁▁▁▃▂▂▃▅                         
wandb:    label_loss ▆▃▂▂▄▁▃▂▁▂▃█▃▂▂                         
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▂▁▆█           
wandb:    train_loss █▂▁▁▁▂▂▂▂▂                              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run fast-sweep-1 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/v3yuoadi
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_180809-v3yuoadi/logs
wandb: Agent Starting Run: lwbia2wb with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07655308946692274
wandb: 	learning_rate: 0.008828725046235135
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_181036-lwbia2wb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/lwbia2wb
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▇▄▄▃▂▂▂▂▁                              
wandb:    label_loss █▆▂▂▁                                   
wandb: test_accuracy ▇█▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▁             
wandb:    train_loss █▇▆▃▁                                   
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run glorious-sweep-2 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/lwbia2wb
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_181036-lwbia2wb/logs
wandb: Agent Starting Run: 8b2tykuc with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.060272307633709776
wandb: 	learning_rate: 0.004139172859615199
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_181233-8b2tykuc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/8b2tykuc
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▃▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂                       
wandb:    label_loss ▂▅▁▃▅▇▁▂▇█▂▃▂                           
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▇█▇           
wandb:    train_loss █▃▂▁▁▃▄▃▃▄▄▄▄▄▃                         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run swift-sweep-3 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/8b2tykuc
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_181233-8b2tykuc/logs
wandb: Agent Starting Run: ly848cns with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.02646272603511507
wandb: 	learning_rate: 0.0036916538903739218
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_181500-ly848cns
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/ly848cns
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▅▅▃▄▃▃▂▂▃▁▃▃▂▂▁▂▃▂▂▂▂▂▃▁▂▂▂▃▁▂▂▃▂▁▂▁▂▂
wandb: test_accuracy ▁▃▅▅▆▆▇▇▇▇▇▇███
wandb:     test_loss █▄▂▁▁▁▁▁▁▂▂▂▂▂▂
wandb:    train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00717
wandb:    label_loss 0.0146
wandb: test_accuracy 96
wandb:     test_loss 0.00741
wandb:    train_loss 0.00736
wandb: 
wandb: 🚀 View run sage-sweep-4 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/ly848cns
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_181500-ly848cns/logs
wandb: Agent Starting Run: 35ggdrvj with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4731501087937109
wandb: 	learning_rate: 0.007046123627617988
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_181646-35ggdrvj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/35ggdrvj
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████
wandb:    image_loss ▄▄▂▂▂▁▂▂▁▁▁▂▂▃▄▄█                       
wandb:    label_loss █▁▂▂▂▂▁▂▁▁▁▂▂▂▁▂▂▁                      
wandb: test_accuracy ██████▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▇▁▁▄▅█         
wandb:    train_loss █▃▂▁▂▂▁▁▁▁▂▁▁▂▂▃                        
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run glad-sweep-5 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/35ggdrvj
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_181646-35ggdrvj/logs
wandb: Agent Starting Run: vnztyrf7 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.16298355446911833
wandb: 	learning_rate: 0.007873681907755255
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_181848-vnztyrf7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/vnztyrf7
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ██▇▇▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁   
wandb:    label_loss ██▇▅▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁   
wandb: test_accuracy ▄▇▇▇█████████▁▁
wandb:     test_loss █▆▄▄▃▂▂▂▁▁▁▁▁  
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run efficient-sweep-6 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/vnztyrf7
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_181848-vnztyrf7/logs
wandb: Agent Starting Run: a5k9l8ys with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2941663439284403
wandb: 	learning_rate: 0.005162883497581059
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_182034-a5k9l8ys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/a5k9l8ys
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:    image_loss ▁▄▇▆█                                   
wandb:    label_loss ▄▁▇█▇▁█▄▃▂                              
wandb: test_accuracy ███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁█▂            
wandb:    train_loss ▂▄▁▅▇▃▃█                                
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run curious-sweep-7 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/a5k9l8ys
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_182034-a5k9l8ys/logs
wandb: Agent Starting Run: erpl01n8 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4149800628425171
wandb: 	learning_rate: 0.004334375335512118
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_182301-erpl01n8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/erpl01n8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▆▄▂▁▁▁▂▂▁▁▂▂▂▂▂▄▂▁▂▃▄▅▃▃▃▂             
wandb:    label_loss █▆▄▆▂▂▃▃▂▃▃▂▃▃▂▁▃▂▃▃▄▁▁▁▂▃▂▁▁▁          
wandb: test_accuracy ██████████▁▁▁▁▁
wandb:     test_loss ▂▃▃▁▆▆▅███     
wandb:    train_loss █▆▂▂▁▂▁▁▁▁▂▂▁▂▂▂▁▂▂▁▂▂▂▂▂               
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run pretty-sweep-8 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/erpl01n8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_182301-erpl01n8/logs
wandb: Agent Starting Run: mnvvp0ha with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.23375238149055055
wandb: 	learning_rate: 0.003656031293119091
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_182457-mnvvp0ha
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/mnvvp0ha
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▃▁▄▂▂▅▃▂▄▄▃▃▃▄▃▄▃▄▂                    
wandb:    label_loss ▄▄▁▂▄█▅▁▇▅▂▂▂▁▃▁▃▃▃▂▁▁▃                 
wandb: test_accuracy ███████▁▁▁▁▁▁▁▁
wandb:     test_loss █▁▃▂▅▅▃        
wandb:    train_loss █▄▃▃▃▄▃▃▂▂▃▁▂▃▃▁▃▂▃▁                    
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run wobbly-sweep-9 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/mnvvp0ha
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_182457-mnvvp0ha/logs
wandb: Agent Starting Run: virdv1rh with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2113069840031367
wandb: 	learning_rate: 0.0016048732924113557
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_182724-virdv1rh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/virdv1rh
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▃▃▃▃▃▁▄▃▃▄▂▃▃▂▂▁▁▂▂▂▃▂▃▃▂▁▂▂▂▂▁▁▂▁▁▁▁▁
wandb: test_accuracy ▁▃▆▆▆▆▇▇▇█▇█▇█▇
wandb:     test_loss █▅▄▃▃▃▂▂▂▁▂▁▂▁▂
wandb:    train_loss █▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00634
wandb:    label_loss 0.00135
wandb: test_accuracy 96.96
wandb:     test_loss 0.00924
wandb:    train_loss 0.00688
wandb: 
wandb: 🚀 View run restful-sweep-10 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/virdv1rh
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_182724-virdv1rh/logs
wandb: Agent Starting Run: bced7sz5 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.33721755761019956
wandb: 	learning_rate: 0.0006675998230426581
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_182911-bced7sz5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/bced7sz5
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▆▃▃▃▄▄▄▃▃▃▁▂▃▁▂▂▂▁▂▂▂▂▁▂▂▁▂▂▂▁▁▂▂▂▁▄▁▁
wandb: test_accuracy ▁▄▅▆▇▇█▇▇▇█▇▆██
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▁▁▂▁▁
wandb:    train_loss █▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0061
wandb:    label_loss 0.00276
wandb: test_accuracy 97.69
wandb:     test_loss 0.00875
wandb:    train_loss 0.00732
wandb: 
wandb: 🚀 View run pretty-sweep-11 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/bced7sz5
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_182911-bced7sz5/logs
wandb: Agent Starting Run: xfmtmdwr with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2355074758067071
wandb: 	learning_rate: 0.0008571318999159541
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_183052-xfmtmdwr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/xfmtmdwr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁
wandb:    label_loss ▄█▇▄▆▆▆▄▅▃▂▂▃▃▃▂▃▃▆▁▃▄▁▁▄▁▄▅▁▂▃▂▃▂▂▁▁▁▃▁
wandb: test_accuracy ▁▄▅▅▅▇▆▇▇▇▅▇▆██
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▂▂▂▁▁
wandb:    train_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00585
wandb:    label_loss 0.0022
wandb: test_accuracy 97.75
wandb:     test_loss 0.00749
wandb:    train_loss 0.00649
wandb: 
wandb: 🚀 View run snowy-sweep-12 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/xfmtmdwr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_183052-xfmtmdwr/logs
wandb: Agent Starting Run: 64mid6yi with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07941855817013233
wandb: 	learning_rate: 0.005170606956676434
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_183254-64mid6yi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/64mid6yi
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█
wandb:    image_loss █▆▃▂▃▁▃▃▄▄▄                             
wandb:    label_loss █▃▅▄▅▁▂▃▁▄                              
wandb: test_accuracy ███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▆█            
wandb:    train_loss █▂▂▂▁▂▄▄▅█▆                             
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run grateful-sweep-13 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/64mid6yi
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_183254-64mid6yi/logs
wandb: Agent Starting Run: t1fcn87b with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.479053833027584
wandb: 	learning_rate: 0.006673515095570292
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_183521-t1fcn87b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/t1fcn87b
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████
wandb:    image_loss ▁▁▁▁▁▁█                                 
wandb:    label_loss ▅▂▄▃▂▁▁█                                
wandb: test_accuracy ██▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▁             
wandb:    train_loss █▂▁▂▃▄▄▅▆                               
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run lucky-sweep-14 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/t1fcn87b
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_183521-t1fcn87b/logs
wandb: Agent Starting Run: 2hlp1e4w with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.16922228948013707
wandb: 	learning_rate: 0.0006197324293304804
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_183747-2hlp1e4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/2hlp1e4w
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆▄▄▂▄▂▃▆▅▃▂▄▂▂▃▂▆▁▁▅▄▂▂▃▁▄▂▁█▁▃▁▃▁▂▁▂▁▁▂
wandb: test_accuracy ▁▄▅▆▅▅▆▇▇▇█▇█▇▆
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0055
wandb:    label_loss 0.00159
wandb: test_accuracy 97.19
wandb:     test_loss 0.00675
wandb:    train_loss 0.00592
wandb: 
wandb: 🚀 View run atomic-sweep-15 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/2hlp1e4w
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_183747-2hlp1e4w/logs
wandb: Agent Starting Run: obfye0id with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.040487863754660625
wandb: 	learning_rate: 0.004462167298368566
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_183949-obfye0id
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/obfye0id
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇██
wandb:    image_loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▂▁▁▁▁▂
wandb:    label_loss █▆▃▃▄▂▃▃▂▂▁▂▁▂▂▁▁▂▃▃▂▁▁▁▁▂▁▁▂▁▃▃▂▂▂▃▂▃▁▂
wandb: test_accuracy ▁▃▅▆▇█▇▇▇▆▇▇▇▇█
wandb:     test_loss █▄▂▁▁▁▁▁▂▂▂▂▂▃▂
wandb:    train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00854
wandb:    label_loss 0.01122
wandb: test_accuracy 96.23
wandb:     test_loss 0.00859
wandb:    train_loss 0.0089
wandb: 
wandb: 🚀 View run ruby-sweep-16 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/obfye0id
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_183949-obfye0id/logs
wandb: Agent Starting Run: i3ctrpre with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.11218102121390822
wandb: 	learning_rate: 0.0018980061711028585
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_184136-i3ctrpre
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/i3ctrpre
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▆▄▄▄▃▅▁▃▃▂▃▂▂▅▂▂▄▂▂▂▃▃▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂
wandb: test_accuracy ▁▄▅▅▆▇▇▇▇▇▇▇▇▇█
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▂▂▁▂▁
wandb:    train_loss █▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0057
wandb:    label_loss 0.00757
wandb: test_accuracy 97.26
wandb:     test_loss 0.007
wandb:    train_loss 0.00682
wandb: 
wandb: 🚀 View run curious-sweep-17 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/i3ctrpre
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_184136-i3ctrpre/logs
wandb: Agent Starting Run: zzb254d0 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.35873340410535814
wandb: 	learning_rate: 0.0054356311191943955
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_184322-zzb254d0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/zzb254d0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▂▂▂▂▂▂▃▂▂▂
wandb:    label_loss █▅▃▅▃▃▂▃▂▃▂▂▄▂▃▂▁▂▂▂▃▂▁▃▁▂▂▂▁▁▂▂▂▂▄▁▃▂▃▂
wandb: test_accuracy ▁▆▇▇▇▇▇▇▆▆▇█▇██
wandb:     test_loss █▄▂▁▁▁▁▂▂▂▂▃▃▃▃
wandb:    train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0186
wandb:    label_loss 0.00641
wandb: test_accuracy 96.91
wandb:     test_loss 0.02238
wandb:    train_loss 0.02022
wandb: 
wandb: 🚀 View run leafy-sweep-18 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/zzb254d0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_184322-zzb254d0/logs
wandb: Agent Starting Run: 0wlrl7on with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.25890047355490803
wandb: 	learning_rate: 0.002548195288950572
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_184508-0wlrl7on
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/0wlrl7on
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█
wandb:    image_loss █▆▄▅▄▃▄▄▃▃▃▂▄▂▁▂▂▄▃▂▂▂▂▃▄▃▂▂▂▃▄▃▃▂▂▂▂▁▁▂
wandb:    label_loss ▇▅█▂▅▇▃▂▂▁▂▃▁▁▁▁▁▃▃▁▃▁▁▁▁▁▁▁▁▁▃▂▁▁▄▃▃▂▁▄
wandb: test_accuracy ▁▃▅▄▆█▆▇▅▇█▇█▆▆
wandb:     test_loss █▆▅▄▆▁▅▃▃▃▂▅▁▃▂
wandb:    train_loss ██▅▅▇▅▄▄▄▄▃▃▃▃▂▄▃▃▂▃▂▃▄▃▂▂▂▁▂▄▁▂▃▄▃▄▂▂▃▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01685
wandb:    label_loss 0.00106
wandb: test_accuracy 96.86
wandb:     test_loss 0.01988
wandb:    train_loss 0.01746
wandb: 
wandb: 🚀 View run volcanic-sweep-19 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/0wlrl7on
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_184508-0wlrl7on/logs
wandb: Agent Starting Run: hge3z0dw with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3288586313321824
wandb: 	learning_rate: 0.00930875415614526
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_184735-hge3z0dw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/hge3z0dw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▇▆▆▅▄▃▃▃▃▃▂▃▂▃▁▁                       
wandb:    label_loss █▅▃▂▁▂▂▂▃▂▂▂▁▂▂▁▁▁                      
wandb: test_accuracy ▆▇████▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▅▃▂▄▁         
wandb:    train_loss █▇▄▃▃▂▂▂▂▂▁▂▁▂▁                         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run cosmic-sweep-20 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/hge3z0dw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_184735-hge3z0dw/logs
wandb: Agent Starting Run: slntb3iw with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.18106178158818623
wandb: 	learning_rate: 0.005519962143818203
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_184937-slntb3iw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/slntb3iw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▇▆▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂      
wandb:    label_loss █▆▄▅▃▃▃▂▂▃▃▃▁▁▂▂▂▂▁▂▁▂▂▂▁▃▃▂▂▃▂         
wandb: test_accuracy ███████████▁▁▁▁
wandb:     test_loss █▄▂▁▁▂▁▄▃▃▄    
wandb:    train_loss █▇▆▅▅▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▂▂▂▂▂▂▂       
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run sleek-sweep-21 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/slntb3iw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_184937-slntb3iw/logs
wandb: Agent Starting Run: 5o091yko with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.03963888303034263
wandb: 	learning_rate: 0.00708439208837655
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_185138-5o091yko
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/5o091yko
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▁                                     
wandb:    label_loss ▁                                       
wandb: test_accuracy █▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁              
wandb:    train_loss █▅▂▁                                    
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run valiant-sweep-22 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/5o091yko
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_185138-5o091yko/logs
wandb: Agent Starting Run: tggnv2pq with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4348968802039373
wandb: 	learning_rate: 0.005758950494713926
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_185405-tggnv2pq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/tggnv2pq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇█
wandb:    image_loss ▅▂▂▁▁▁▂▁▃▂▂▃▃▃▄▄▃▄▄█                    
wandb:    label_loss █▅▅▁▃▃▂▂▁▁▂▂▂▂▃▃▂▁▁                     
wandb: test_accuracy ██████▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▄▁▂▅▆█         
wandb:    train_loss █▂▂▁▁▃▃▁▃▄▄▄▃▄▄                         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run polished-sweep-23 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/tggnv2pq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_185405-tggnv2pq/logs
wandb: Agent Starting Run: 8j3jrveo with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3614114985554792
wandb: 	learning_rate: 0.004282403224474869
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_185606-8j3jrveo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/8j3jrveo
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▂▂▁▁▁▂▁▂▂▂▁▂▂▂▂▂▂▂▃▂▂▂▂▃               
wandb:    label_loss ▆▃█▃▅▃▃▆▂▃▅▅▂▂▂▂▃▃▁▁▃▄▁▂▁▃▂▂▂▃▁         
wandb: test_accuracy ██████████▁▁▁▁▁
wandb:     test_loss ▃▁▄▅▃▁▆█▇▃     
wandb:    train_loss █▂▂▁▁▂▁▁▁▁▁▂▂▂▂▂▁▂▁▃▁▂▁▂▂▁▁▁▁▁▁▁▂       
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run pious-sweep-24 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/8j3jrveo
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_185606-8j3jrveo/logs
wandb: Agent Starting Run: 3kkg2dku with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.09824566513322636
wandb: 	learning_rate: 0.003779348536557875
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_185803-3kkg2dku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/3kkg2dku
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 140-149, summary, console lines 150-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████
wandb:    image_loss █▃▁▁▁▁▂▂▁▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▂▂▂▂▁▂▁▂▂▂▃▂▂▂▂
wandb:    label_loss ▆█▇▅▄▂▂▄▆▃▄▁▁▃▂▃▂▁▁▁▁▄▄▂▃▁▂▃▃▄▂▂▂▂▁▂▁▁▁▂
wandb: test_accuracy ▁▃▂▃▆▆██▆▆█▇█▇▅
wandb:     test_loss ▂▃▄▆▂▁▃▁▅▂▁▅▂█▇
wandb:    train_loss █▃▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01254
wandb:    label_loss 0.00343
wandb: test_accuracy 95.3
wandb:     test_loss 0.01646
wandb:    train_loss 0.01403
wandb: 
wandb: 🚀 View run effortless-sweep-25 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/3kkg2dku
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_185803-3kkg2dku/logs
wandb: Agent Starting Run: nda9465v with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.08522797609814703
wandb: 	learning_rate: 0.002353673514723234
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_190004-nda9465v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/nda9465v
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇████
wandb:    image_loss █▅▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▂▁▂▁
wandb:    label_loss █▅▃▄▅▄▃▃▃▄▃▃▃▁▂▂▂▁▃▂▂▁▁▃▁▃▁▂▂▂▂▂▂▁▂▃▂▂▃▄
wandb: test_accuracy ▁▄▃▅▇▇▆▇▆██████
wandb:     test_loss █▅▄▄▄▃▂▃▄▂▂▁▂▂▁
wandb:    train_loss █▆▅▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00831
wandb:    label_loss 0.0232
wandb: test_accuracy 96.7
wandb:     test_loss 0.00883
wandb:    train_loss 0.00869
wandb: 
wandb: 🚀 View run laced-sweep-26 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/nda9465v
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_190004-nda9465v/logs
wandb: Agent Starting Run: 5vtpbfq2 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2875927987143829
wandb: 	learning_rate: 0.005008000407871404
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_190206-5vtpbfq2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/5vtpbfq2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss █▃▃▃▁▂▃▃▅▅▅▇▅▃                          
wandb:    label_loss ▆▄▆▆▂█▂▁▃▅▂▂▄                           
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▂█▇           
wandb:    train_loss █▂▁▁▁▁▁▁▁▁                              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run dashing-sweep-27 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/5vtpbfq2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_190206-5vtpbfq2/logs
wandb: Agent Starting Run: 96hos5vs with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.10647786537646792
wandb: 	learning_rate: 0.0016294605979337712
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_190438-96hos5vs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/96hos5vs
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇█████
wandb:    image_loss █▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▂▁▂▁▂▁▁▁
wandb:    label_loss █▆▅▄▆▄▄▂▅▁▅▂▂▄▂▂▁▁▂▂▂▂▁▁▁▁▄▂▁▁▁▁▂▃▁▁▂▃▁▁
wandb: test_accuracy ▁▅▅▇▇██▇▇▇██▅██
wandb:     test_loss █▅▅▃▃▁▂▂▂▃▁▁▃▂▁
wandb:    train_loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00852
wandb:    label_loss 0.01255
wandb: test_accuracy 97.12
wandb:     test_loss 0.0094
wandb:    train_loss 0.00906
wandb: 
wandb: 🚀 View run dark-sweep-28 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/96hos5vs
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_190438-96hos5vs/logs
wandb: Agent Starting Run: svfx2883 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07387537110410297
wandb: 	learning_rate: 0.0024089445339615764
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_190704-svfx2883
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/svfx2883
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████
wandb:    image_loss █▄▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁
wandb:    label_loss █▅▅▆▄▄▃▂▃▃▃▂▃▃▃▁▃▂▂▂▂▃▄▁▄▂▃▄▂▂▁▂▂▁▂▃▃▁▁▂
wandb: test_accuracy ▁▄▆▆▅▇▇▇▇▆█████
wandb:     test_loss █▅▄▃▃▂▃▂▂▂▂▁▁▁▁
wandb:    train_loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00664
wandb:    label_loss 0.00675
wandb: test_accuracy 96.81
wandb:     test_loss 0.00725
wandb:    train_loss 0.00683
wandb: 
wandb: 🚀 View run glamorous-sweep-29 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/svfx2883
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_190704-svfx2883/logs
wandb: Agent Starting Run: zy0i8h07 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4906835758138606
wandb: 	learning_rate: 0.0009476372714589742
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_190851-zy0i8h07
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/zy0i8h07
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇██
wandb:    image_loss █▅▄▄▃▃▃▃▂▂▂▂▂▂▁▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▃█▄▂▃▃▁▂▃▄▃▃▃▂▁▂▁▁▂▂▁▁▁▂▂▃▁▁▂▂▁▂▁▁▂▂▁▃▁▁
wandb: test_accuracy ▁▁▄▅▆▇█▇▆█▇▇▇▇▇
wandb:     test_loss █▆▄▃▂▂▁▁▂▁▁▁▁▁▁
wandb:    train_loss █▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00778
wandb:    label_loss 0.0002
wandb: test_accuracy 97.48
wandb:     test_loss 0.01252
wandb:    train_loss 0.00864
wandb: 
wandb: 🚀 View run efficient-sweep-30 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/zy0i8h07
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_190851-zy0i8h07/logs
wandb: Agent Starting Run: gnpsve7l with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3861202777143246
wandb: 	learning_rate: 0.005044369445107678
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_191052-gnpsve7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/gnpsve7l
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇█████
wandb:    image_loss █▆▅▅▄▃▂▂▂▂▁▁▂▁▂▂▂▂▂▂▄▂▃▃▃▃▄▃▄▃▃▄        
wandb:    label_loss █▂▂▂▁▄▁▁▂▃▂▂▁▃▂▁▃▂▂▁▁▂▁▂▁▁▁▂▂▃▁▃▂▁      
wandb: test_accuracy ████████████▁▁▁
wandb:     test_loss █▃▁▁▂▄▃▃▅▆▅▄   
wandb:    train_loss █▅▄▃▂▂▂▂▂▂▁▁▁▁▂▁▂▂▂▃▂▃▃▃▃▄▃▃▄▃▂▃▃       
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run icy-sweep-31 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/gnpsve7l
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_191052-gnpsve7l/logs
wandb: Agent Starting Run: 96fs20ea with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.44380380934550623
wandb: 	learning_rate: 0.0005579176231748824
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_191254-96fs20ea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/96fs20ea
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ▆█▅▄▄▄▃▃▃▃▂▂▃▂▂▂▃▁▂▂▁▂▂▂▁▁▂▁▁▁▂▂▁▁▁▁▁▁▂▁
wandb: test_accuracy ▁▄▆▇█▆▇█▇▇█████
wandb:     test_loss █▅▄▃▂▂▂▁▂▂▁▁▁▁▁
wandb:    train_loss █▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00774
wandb:    label_loss 0.00222
wandb: test_accuracy 97.6
wandb:     test_loss 0.01133
wandb:    train_loss 0.00844
wandb: 
wandb: 🚀 View run glowing-sweep-32 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/96fs20ea
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_191254-96fs20ea/logs
wandb: Agent Starting Run: 89qg4py2 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.01761837986840059
wandb: 	learning_rate: 0.004599850093467512
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_191440-89qg4py2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/89qg4py2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▃▃▃▂▂▁▃▂▄▅▄▅▅▄▄▃▅▆▅                    
wandb:    label_loss █▁▂▅▁▃▃▂▂▂▂▃▂▁▃▄▄▂▂▂                    
wandb: test_accuracy ██████▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▁▆█▇▆         
wandb:    train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁                          
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run wild-sweep-33 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/89qg4py2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_191440-89qg4py2/logs
wandb: Agent Starting Run: qwjkteqa with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2720146738963529
wandb: 	learning_rate: 0.00467069638752424
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_191712-qwjkteqa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/qwjkteqa
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▄▂▂▂▁▁▁▁▁▂▂▂▃▂▂▃▃▂▂▃▃▂▃▃▄▃▃            
wandb:    label_loss █▄▄▂▃▅▂▂▄▃▂▂▁▅▄▁▄▁▃▁▃▃▂▂▁▂▂▂▂▂          
wandb: test_accuracy ██████████▁▁▁▁▁
wandb:     test_loss ▁▁▂▃▅▇▅▅▆█     
wandb:    train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁             
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run dulcet-sweep-34 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/qwjkteqa
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_191712-qwjkteqa/logs
wandb: Agent Starting Run: pg5knnys with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3451342140404729
wandb: 	learning_rate: 0.0024795307565666407
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_191914-pg5knnys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/pg5knnys
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇███
wandb:    image_loss █▆▅▅▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▄▄▅▃▆▅▃▄▅▃▅▂▃▄▄▁▄▂▂▂▂▁▃▂▃▂▂▂▃▂▂▃▂▂▂▁▂▁
wandb: test_accuracy ▁▃▅▆▆▆█▇▇▇▇▇██▆
wandb:     test_loss █▆▃▃▃▃▃▂▂▃▂▂▁▁▂
wandb:    train_loss █▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0096
wandb:    label_loss 0.00165
wandb: test_accuracy 97.01
wandb:     test_loss 0.0139
wandb:    train_loss 0.01093
wandb: 
wandb: 🚀 View run solar-sweep-35 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/pg5knnys
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_191914-pg5knnys/logs
wandb: Agent Starting Run: fvxjew30 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.48693524930068
wandb: 	learning_rate: 0.001216656343162193
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_192100-fvxjew30
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/fvxjew30
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▇█▄▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▃▂▂▁▃▂▂▂▂▂▂▃▂▁▂▂▂▁▂▂▂▂
wandb:    label_loss ▂▆▅▂▂▂▁▆▁▁▂▃▃▄▂▄▆▃▂▁▁▁▅▁▂▂▁█▁▁▂▁▄▁▁▁▃▂▁▁
wandb: test_accuracy ▁▅▆▆▆▇▇▇▆▇▇█▇█▇
wandb:     test_loss █▅▃▃▄▂▂▁▂▂▂▂▁▁▂
wandb:    train_loss █▅▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01106
wandb:    label_loss 0.00724
wandb: test_accuracy 97.38
wandb:     test_loss 0.01576
wandb:    train_loss 0.01119
wandb: 
wandb: 🚀 View run grateful-sweep-36 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/fvxjew30
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_192100-fvxjew30/logs
wandb: Agent Starting Run: cg2jadnh with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.09902601082866114
wandb: 	learning_rate: 0.001111084024886691
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_192327-cg2jadnh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/cg2jadnh
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▆▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▂▁▁
wandb:    label_loss ▅█▅▂▃▂▅▄▅▂▂▂▄▂▃▂▁▁▂▂▃▂▂▁▁▂▁▂▁▁▃▁▂▂▁▃▂▄▂▃
wandb: test_accuracy ▁▄▅▆▇▇▇▇█▇▇████
wandb:     test_loss █▆▃▃▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss █▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00526
wandb:    label_loss 0.00182
wandb: test_accuracy 97.28
wandb:     test_loss 0.00666
wandb:    train_loss 0.00639
wandb: 
wandb: 🚀 View run quiet-sweep-37 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/cg2jadnh
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_192327-cg2jadnh/logs
wandb: Agent Starting Run: ydey9xjd with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1815990718686843
wandb: 	learning_rate: 0.006748746166968361
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_192529-ydey9xjd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/ydey9xjd
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss ██▇▅▄▄▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb:    label_loss █▅▂▃▂▁▂▂▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▂▂▂▂
wandb: test_accuracy ▁▅▆▇██████▇▇██▇
wandb:     test_loss █▅▃▂▂▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0145
wandb:    label_loss 0.01496
wandb: test_accuracy 96.08
wandb:     test_loss 0.01544
wandb:    train_loss 0.01629
wandb: 
wandb: 🚀 View run worthy-sweep-38 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/ydey9xjd
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_192529-ydey9xjd/logs
wandb: Agent Starting Run: zsnh41pq with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4132771113656818
wandb: 	learning_rate: 0.0019817346669853296
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_192711-zsnh41pq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/zsnh41pq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▃▃▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▁▁▁▂▁▁▂▂▁▁▁▂▂▁▁▂▂
wandb:    label_loss █▇▂▄▂▂▁▂▃▂▃▁▁▅▇▂▂▁▁▂▂▂▃▁▁▁▁▁▂▁▁▂▂▁▃▁▁▂▅▃
wandb: test_accuracy ▁▅▅▆▅▆▇▇▇▇▇▇█▇█
wandb:     test_loss █▄▄▂▃▃▁▁▂▁▂▁▁▂▂
wandb:    train_loss █▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01381
wandb:    label_loss 0.00409
wandb: test_accuracy 97.71
wandb:     test_loss 0.01658
wandb:    train_loss 0.01492
wandb: 
wandb: 🚀 View run stellar-sweep-39 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/zsnh41pq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_192711-zsnh41pq/logs
wandb: Agent Starting Run: kla0tgeo with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4664514764282637
wandb: 	learning_rate: 0.002505069121747381
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_192913-kla0tgeo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_elu/sweeps/u6ox6yja
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_elu/runs/kla0tgeo
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▄▃▄▃▂▂▂▁▂▂▁▂▂▂▂▁▂▁▂▁▁▂▂▁▁▁▁▂▁▁▂▁▁▁▂▁▁▁
wandb:    label_loss ████▃▂▁▂▄▄▃▄▂▂▁▁▁▄▂▄▂▂▃▂▃▁▂▁▃▁▃▃▂▁▁▂▁▁▂▁
wandb: test_accuracy ▁▃▅▇▅▅▆█▆▆▆▆███
wandb:     test_loss █▆▃▃▄▄▃▂▃▃▃▂▃▁▁
wandb:    train_loss █▆▅▄▄▄▃▂▃▃▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▁▂▂▂▂▁▁▁▁▂▂▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01449
wandb:    label_loss 0.00194
wandb: test_accuracy 97.57
wandb:     test_loss 0.01842
wandb:    train_loss 0.01574
wandb: 
wandb: 🚀 View run brisk-sweep-40 at: https://wandb.ai/cavaokcava/auto_128_elu/runs/kla0tgeo
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_192913-kla0tgeo/logs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: nucc7eei with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4425600460509001
wandb: 	learning_rate: 0.0004978928488836495
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_193117-nucc7eei
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/nucc7eei
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█
wandb:    image_loss █▆▆▅▅▅▄▄▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁▁
wandb:    label_loss █▃▅▂▃▃▂▂▃▂▁▂▂▂▂▂▁▂▁▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇▇▇████████
wandb:     test_loss █▆▄▄▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▆▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01851
wandb:    label_loss 0.00962
wandb: test_accuracy 97.74
wandb:     test_loss 0.02318
wandb:    train_loss 0.0209
wandb: 
wandb: 🚀 View run stellar-sweep-1 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/nucc7eei
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_193117-nucc7eei/logs
wandb: Agent Starting Run: a4esqfri with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.22985850825791404
wandb: 	learning_rate: 0.003311432237793659
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_193319-a4esqfri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/a4esqfri
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▅▁▂▂▂▄█▄▂▃▅▄▄▅▄▃▄▄▂▂▄▄▃▆▅▃▄▂▃▄▂▅▂▂▅▂▂▅▂▂
wandb:    label_loss ▃▄▆▇▃▂▅▆▆█▁▇▄▇▂█▄▆▃▃▆▄▅▆▁▄▅▅▇▃▅▄▃▄▅▄▅▆▇▅
wandb: test_accuracy ▃█▁██▃█▄▃█▁█▄█▃
wandb:     test_loss █▃▅▅▅▄▁▃▅▄▅▃▅▁▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06333
wandb:    label_loss 0.23056
wandb: test_accuracy 10.28
wandb:     test_loss 0.12057
wandb:    train_loss 0.12044
wandb: 
wandb: 🚀 View run lunar-sweep-2 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/a4esqfri
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_193319-a4esqfri/logs
wandb: Agent Starting Run: 42a7eatr with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3921969019053308
wandb: 	learning_rate: 0.00020933367487066052
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_193520-42a7eatr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/42a7eatr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▇▆▆▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▅▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▁
wandb: test_accuracy ▁▆▇▇▇▇█████████
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▇▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03131
wandb:    label_loss 0.0102
wandb: test_accuracy 96.01
wandb:     test_loss 0.03573
wandb:    train_loss 0.03525
wandb: 
wandb: 🚀 View run fearless-sweep-3 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/42a7eatr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_193520-42a7eatr/logs
wandb: Agent Starting Run: bheffk74 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.14050082996301205
wandb: 	learning_rate: 0.0015961180431746929
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_193707-bheffk74
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/bheffk74
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ██▆▆▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▆▇▇███████████
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▇▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01123
wandb:    label_loss 0.00525
wandb: test_accuracy 98.3
wandb:     test_loss 0.01179
wandb:    train_loss 0.01121
wandb: 
wandb: 🚀 View run upbeat-sweep-4 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/bheffk74
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_193707-bheffk74/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: pvu3fj4f with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2563229968412628
wandb: 	learning_rate: 0.003677854186505827
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_193858-pvu3fj4f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/pvu3fj4f
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 140-149, summary, console lines 150-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████
wandb:    image_loss ▄▅▃▃▂▃▆▄▅▃▃▅▃▆▁▄▁▄▃▆▃▂▄▇▃▃▆▂█▅▅▃▃▄▆▆▆▄▅▂
wandb:    label_loss ▄▅▅▅▅▅█▅▆▄▄▄▇▅█▅▅▃▅▃█▁▅▄▄▄▄▄▄▄█▆▆▇▆▅▃▄▆▅
wandb: test_accuracy █▄▂█▁▄███████▄▃
wandb:     test_loss ▁▁█▁▅▃▄▂▂▂▅▃▆▃▄
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06709
wandb:    label_loss 0.23013
wandb: test_accuracy 10.1
wandb:     test_loss 0.12674
wandb:    train_loss 0.12715
wandb: 
wandb: 🚀 View run zesty-sweep-5 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/pvu3fj4f
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_193858-pvu3fj4f/logs
wandb: Agent Starting Run: e2g8u63d with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4667403727143344
wandb: 	learning_rate: 0.001682148274611215
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_194100-e2g8u63d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/e2g8u63d
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███
wandb:    image_loss ██▇▆▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▅▃▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▆▇████████████
wandb:     test_loss █▅▄▃▃▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▇▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01505
wandb:    label_loss 0.00113
wandb: test_accuracy 97.77
wandb:     test_loss 0.01877
wandb:    train_loss 0.01612
wandb: 
wandb: 🚀 View run logical-sweep-6 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/e2g8u63d
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_194100-e2g8u63d/logs
wandb: Agent Starting Run: lfnl0fo1 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.060107533654421785
wandb: 	learning_rate: 0.004183593760835917
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_194246-lfnl0fo1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/lfnl0fo1
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▄▄▃▄▅▅▆▅▅▅▂▃▆▅▇▆▆▃▅▆▄▇█▄▅▅▆▇▅▆▇▇▄▄▃▅▆▃▁▄
wandb:    label_loss ▄▄▃█▅▅▅▅▄▅▄▂▅▄▄▃▃▃▁▂▅▃▄▆▄▅▃▄▆▃▆▃▅▄▆▂▅▅▄▄
wandb: test_accuracy ███▁█▃████▁▁▃██
wandb:     test_loss ▁▂▇▃▇▆█▄▂▃▅█▄▅█
wandb:    train_loss ▇▃█▆▅▄▆▆▅▆▆▆▅▅▅▇▁▄█▆▄▆▆▅▅▄▅▇▇▆▇▇▅▆▇▃▅▅▅▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06741
wandb:    label_loss 0.22894
wandb: test_accuracy 11.35
wandb:     test_loss 0.08161
wandb:    train_loss 0.08143
wandb: 
wandb: 🚀 View run youthful-sweep-7 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/lfnl0fo1
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_194246-lfnl0fo1/logs
wandb: Agent Starting Run: wo1xcfx9 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.062088354792139144
wandb: 	learning_rate: 0.00044834936983064567
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_194447-wo1xcfx9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/wo1xcfx9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▆▆▄▃▃▃▃▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▂▁▂▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇▇▇████████
wandb:     test_loss █▅▄▄▃▃▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▇▇▇▆▅▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01058
wandb:    label_loss 0.0018
wandb: test_accuracy 98.13
wandb:     test_loss 0.01094
wandb:    train_loss 0.01087
wandb: 
wandb: 🚀 View run worthy-sweep-8 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/wo1xcfx9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_194447-wo1xcfx9/logs
wandb: Agent Starting Run: zseeiqpg with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.02743962760579094
wandb: 	learning_rate: 0.00844246188769668
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_194720-zseeiqpg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/zseeiqpg
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▅▅▂▂▃▃▂▅▃▅▅▃▄▄▅▃▂▁▂▅▄▅▄▅▃▂▃▅▄▃▂▃▃▆▇▅█▅▄▁
wandb:    label_loss █▁▄▁▃▅▇▅▇▇▅▅▆▃▅▃▃▃▃▄▆▂▄█▁▆▁▃▃▂▃▂▆▄▆▆▂▄▄▄
wandb: test_accuracy █▂███▁▁█▁██████
wandb:     test_loss ▃▄▅▁▅▄▄▄▃▆▆▆▆█▂
wandb:    train_loss ▅▅▄▂▇▃▇▄▄▄█▅▇▇▇▇▁▇▇▆▇▆▆▂▄▆▃▅▅▆▃▇█▅▆█▅▅▆▇
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07633
wandb:    label_loss 0.22923
wandb: test_accuracy 11.35
wandb:     test_loss 0.07387
wandb:    train_loss 0.07401
wandb: 
wandb: 🚀 View run rosy-sweep-9 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/zseeiqpg
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_194720-zseeiqpg/logs
wandb: Agent Starting Run: qzn251mv with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.03869294249917704
wandb: 	learning_rate: 0.0068221526460070315
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_194951-qzn251mv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/qzn251mv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇███
wandb:    image_loss ▁▄▆▆▅▂▄▁▅▃▄▆▄▇▇▅▃▇▄▁▃█▅▅█▆▃▄▄▇▄▃▅▂▄▅▄▄▃▅
wandb:    label_loss ▅▅▄▂▁▄▅▆▅▄▇▅▆▃▅▆▃▆█▅▅▅▃▄▅▆▄▄▆▅▄▇▄▅▆▅▇▆▄▇
wandb: test_accuracy ████▂███████▁██
wandb:     test_loss ▅█▄▆▁▁▄▂▃█▆▂▄▄▁
wandb:    train_loss ▅▄▅▂▂▄▃▆▃█▃▅▃▇▅▂▆▂▂▄▆▄▄▄▃█▃▃▄▂▆▆▇▃▁▄▂▅▇▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06903
wandb:    label_loss 0.23105
wandb: test_accuracy 11.35
wandb:     test_loss 0.07643
wandb:    train_loss 0.07626
wandb: 
wandb: 🚀 View run polished-sweep-10 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/qzn251mv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_194951-qzn251mv/logs
wandb: Agent Starting Run: u3zv728z with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4417879536502304
wandb: 	learning_rate: 0.0009360936653150716
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_195153-u3zv728z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/u3zv728z
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█████
wandb:    image_loss ██▆▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁▂▁▁▁▁▂▁▁▁
wandb:    label_loss ▆█▄▃▃▄▅▁▁▂▃▄▄▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb: test_accuracy ▁▄▅▇▇▇▇▇▇▇▇▇███
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▇▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01181
wandb:    label_loss 7e-05
wandb: test_accuracy 98.24
wandb:     test_loss 0.01574
wandb:    train_loss 0.01322
wandb: 
wandb: 🚀 View run icy-sweep-11 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/u3zv728z
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_195153-u3zv728z/logs
wandb: Agent Starting Run: cq5ee45u with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2908473560297809
wandb: 	learning_rate: 0.00262751369273019
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_195425-cq5ee45u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/cq5ee45u
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▇█▇▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ██▅▄▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁██████████████
wandb:     test_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss ██▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01136
wandb:    label_loss 0.00031
wandb: test_accuracy 97.95
wandb:     test_loss 0.01417
wandb:    train_loss 0.01177
wandb: 
wandb: 🚀 View run astral-sweep-12 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/cq5ee45u
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_195425-cq5ee45u/logs
wandb: Agent Starting Run: 1qyluc5a with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4459485147255832
wandb: 	learning_rate: 0.00859846169378422
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_195611-1qyluc5a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/1qyluc5a
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇████
wandb:    image_loss ▄▄▂▃▆▄▅▃▅▇▇▃▄▇▄▆▁▇█▅▆▃▆▄▅▃▂▄▄▂▇▃▅▃▄▃▆▄▅▂
wandb:    label_loss ▄▅▄▅█▄▅▄▅▅▃▅▇▅▃▄▅▂▁▂▃▃▆▄▆▄▆▂▄▆▄▅▂▅▄▅▇▃▆▂
wandb: test_accuracy ▁████████▂██▂█▁
wandb:     test_loss ▅▆█▄▇▅▆▇▁▄█▇▇▄▄
wandb:    train_loss ▄▁▂▄▃▃▂▄▆▃▃▄▃▃▃▃▅▄▄▄▆▅▄█▄▃▃▅▃▄▅▁▂▅▄▃▄▅▄▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07158
wandb:    label_loss 0.228
wandb: test_accuracy 10.1
wandb:     test_loss 0.17025
wandb:    train_loss 0.17016
wandb: 
wandb: 🚀 View run ancient-sweep-13 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/1qyluc5a
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_195611-1qyluc5a/logs
wandb: Agent Starting Run: k9x48w7e with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2663688794482133
wandb: 	learning_rate: 0.008246726123519968
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_195843-k9x48w7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/k9x48w7e
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 139-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▅▆▃▅▄▄▄▆▅▅▁▅▆▅█▄▄▅▅▅▂▇▅▃▅█▃▄▅▆▆▅▅▄▄▄▅▅▃▃
wandb:    label_loss ▄▄▃▅▂▄▅▃▅▄▂▁▅▅▅▅▅▅▅▅▅▄▃▄▄█▂▄▇▇▆▆▄▄▆▅▄▆▅▃
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▂▃▇▅▄▄█▁▃▆▄▃▄▁▅
wandb:    train_loss ▇▆▄▅▃▇▅▅▃▃▇▃▃▄▂▃▃▂▅▄▃▆▄▁▇▄█▃▅▄█▂▃█▄▆▅▆▃▇
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06532
wandb:    label_loss 0.22912
wandb: test_accuracy 11.35
wandb:     test_loss 0.12894
wandb:    train_loss 0.12873
wandb: 
wandb: 🚀 View run fragrant-sweep-14 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/k9x48w7e
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_195843-k9x48w7e/logs
wandb: Agent Starting Run: nqo29x71 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17712792598150995
wandb: 	learning_rate: 0.002656574613975854
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_200045-nqo29x71
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/nqo29x71
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ████▇▆▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss ███▆▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▇█████████████
wandb:     test_loss █▅▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss ████▇▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01075
wandb:    label_loss 0.00385
wandb: test_accuracy 97.97
wandb:     test_loss 0.01175
wandb:    train_loss 0.01087
wandb: 
wandb: 🚀 View run colorful-sweep-15 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/nqo29x71
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_200045-nqo29x71/logs
wandb: Agent Starting Run: avi7rejp with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.20605475851179655
wandb: 	learning_rate: 0.005204338500275703
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_200231-avi7rejp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/avi7rejp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss ▁▂▇▄▅▄▄▅▂▄▃█▄▅▅▃▆▄▅▃▅▆▄▃▄▃▄▄▇▂▇▅▄▃▂▁▂▂▃█
wandb:    label_loss ▂▂▁▂▂▄▆▃▇█▃▃▁▅▃▆▂▄▅▂▂▅▅▄▂▆▅▅▅▄▅▃▃▄▅▂▅▁▅▅
wandb: test_accuracy ████▃██▁▃▂████▃
wandb:     test_loss ▂▃▁▄▅▂▅▄▁▆▃▃▂▅█
wandb:    train_loss ▅▄▅▆▄▃▅▄▇▅▃▆▅▆█▄▅▅▆▂▅▆▃▅▅█▃▅▅▆▆▁▄▆▄▅▇▇▅▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06933
wandb:    label_loss 0.23018
wandb: test_accuracy 10.28
wandb:     test_loss 0.1154
wandb:    train_loss 0.11481
wandb: 
wandb: 🚀 View run colorful-sweep-16 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/avi7rejp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_200231-avi7rejp/logs
wandb: Agent Starting Run: xi30cdmn with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.28679479399057195
wandb: 	learning_rate: 0.006131947065056286
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_200433-xi30cdmn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/xi30cdmn
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇██
wandb:    image_loss ▁▇▄▂▆▂▄█▇▂▂▅▆▅▃▆▄▇▄▆▄▃▅▅▆▅▆▆▄▄▃▆▅▅▂▃▂▇▂▆
wandb:    label_loss ▄▄▆▄▃▆▅▄▄▆▃▇█▅▂▄▅▅▆▅▂▃▄▅▆▃▃▄▇▄▃▃▅▆▂▁▃▅▃▅
wandb: test_accuracy ███▃█████▃█▁▁█▃
wandb:     test_loss ▅▆▅▁▅▃▆▅▆▇▅▃▅▅█
wandb:    train_loss ▆█▆▃▅▅▂▃▅▅▄▂▃▅▅▂▄▆▅▆▅▅▅▅▅▇▂▄▃▄▅▁▃▅▄▅▅▂▃▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06736
wandb:    label_loss 0.22999
wandb: test_accuracy 10.28
wandb:     test_loss 0.13382
wandb:    train_loss 0.13313
wandb: 
wandb: 🚀 View run misunderstood-sweep-17 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/xi30cdmn
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_200433-xi30cdmn/logs
wandb: Agent Starting Run: a9tssbzv with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.21763258231491767
wandb: 	learning_rate: 0.009440440853261171
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_200705-a9tssbzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/a9tssbzv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 139-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▅▃▃▁▃▅▄▆▃▃▄▃▅▄▄▃▄▄▅▆▄▄▄▅▅▄▄█▄▄▄▄▅▄▄▄▅▆▃▄
wandb:    label_loss ▄▄▅▄▂▂▅▃▅▆▄▄▇▄▁▅▄▄▅▄▆▄▇▆▅▅▅▂▃▅▃▅█▂▃▄▃▁▄▅
wandb: test_accuracy ▂██▂▁██████████
wandb:     test_loss ▂▆▂▃▃▂▅▁▄▅▅▁█▅▂
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.065
wandb:    label_loss 0.22928
wandb: test_accuracy 11.35
wandb:     test_loss 0.11768
wandb:    train_loss 0.11772
wandb: 
wandb: 🚀 View run ruby-sweep-18 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/a9tssbzv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_200705-a9tssbzv/logs
wandb: Agent Starting Run: hu69ae7z with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3486298996571372
wandb: 	learning_rate: 0.0035194359152429106
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_200906-hu69ae7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/hu69ae7z
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇██
wandb:    image_loss ▅▄▂▂▅█▄▃▄▆▄▅▅▅▂▄▅▄▄▅▃▅▄▁▅▄▆▄▅▁▂▂▃▆▅▃▃▅▅▆
wandb:    label_loss ▄▆▃▁▂▅▇▅▄▃▃▃█▂▇▇▅▅▆▅▅█▄▄▁▃▄▃▃▅▆▅▅▅▂▅▄▅▅▅
wandb: test_accuracy ▁▂▂▂██████▂██▁█
wandb:     test_loss ▆▄▁▇▃▃▄▇▅▄▅▄▂█▁
wandb:    train_loss ▄▅▇▄▆▆█▁▆▅▆▆▄▆▅▅▅▆▅▆▅▄▆▇█▃▆▅▇▅▆▄▄▅▆▇▄▄▅█
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06996
wandb:    label_loss 0.23031
wandb: test_accuracy 11.35
wandb:     test_loss 0.14786
wandb:    train_loss 0.1479
wandb: 
wandb: 🚀 View run autumn-sweep-19 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/hu69ae7z
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_200906-hu69ae7z/logs
wandb: Agent Starting Run: 9s7cvq5n with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.14816375671890242
wandb: 	learning_rate: 0.0023822908238598953
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_201053-9s7cvq5n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/9s7cvq5n
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇██
wandb:    image_loss █▆▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆▇▇███▇█████
wandb:     test_loss █▅▄▄▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▇▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00965
wandb:    label_loss 0.00173
wandb: test_accuracy 98.03
wandb:     test_loss 0.01129
wandb:    train_loss 0.01059
wandb: 
wandb: 🚀 View run noble-sweep-20 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/9s7cvq5n
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_201053-9s7cvq5n/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4lwh78g2 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.49286362038402687
wandb: 	learning_rate: 0.004582680116314675
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_201329-4lwh78g2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/4lwh78g2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ▅▄█▇▄▄▆▃▅▇▆▆▆▇▃▅▃▅▆▄▆▅▅█▅▅▆▄▄▆▃▅▆▆▄▁▅▄▅▆
wandb:    label_loss ▂▃▇▄▅▇▃▄█▃▅▃▂▆▇▂▂▄▅▃▃▄▇▅▁▄▂▂▁▅▇▁▄▃▃▄▅▄▂▇
wandb: test_accuracy ▁████▄███▃▄▅▅██
wandb:     test_loss ▇▅█▅▃▄▄▂▁▇▆▅▇▃▄
wandb:    train_loss ▇▅▇█▇▇▃▆▇▄▆▅▅▄▁▇▅▇▇▅▅▄▇▇▂▇█▆▆▃▆▅█▆▇▇▅▅▅▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0718
wandb:    label_loss 0.23204
wandb: test_accuracy 11.35
wandb:     test_loss 0.18137
wandb:    train_loss 0.18097
wandb: 
wandb: 🚀 View run twilight-sweep-21 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/4lwh78g2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_201329-4lwh78g2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: a9gadzgh with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2963118231803016
wandb: 	learning_rate: 0.009487525934927266
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_201606-a9gadzgh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/a9gadzgh
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▆▄▃█▆▅▄▇▅▂█▆▅▂▁▃▅▇▄▇▅▄▇▇▁▅▆▇▄▅▆▄▆▆▆▁▄█▂▄
wandb:    label_loss ▆▆▂▆▆▂█▃▆▄▁▄▆▄▃▅▁█▆▅▄▅▄▆▄▆▄█▅▄▅▄▇▅▅▆▃▃▅▃
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▂▃▄▁█▂▂▅▄▂▃▆▅▄▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06632
wandb:    label_loss 0.2291
wandb: test_accuracy 11.35
wandb:     test_loss 0.13582
wandb:    train_loss 0.13527
wandb: 
wandb: 🚀 View run mild-sweep-22 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/a9gadzgh
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_201606-a9gadzgh/logs
wandb: Agent Starting Run: ug8fdsdz with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.48709487350680786
wandb: 	learning_rate: 0.005003610963176314
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_201753-ug8fdsdz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/ug8fdsdz
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇██
wandb:    image_loss ▂█▅▆▆▅▃▂▄▃▃▃▂▄▂▄▃▄▄▃▇▅▃▅▄▄▁▅▃▅▁▇▂▂▃▆▂▃▆▁
wandb:    label_loss █▅▅▅▄▇▇▇▅▅▅▆▅▇▄▄▆▅▅▄▅▅▆▄▆▆▄▄▆▃▄▆▃▂▁▆▅▆▃▆
wandb: test_accuracy ████▁██▁██████▂
wandb:     test_loss ▅▅▁█▁▄▄▄▁▄▃▅▃█▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06184
wandb:    label_loss 0.22881
wandb: test_accuracy 10.28
wandb:     test_loss 0.17989
wandb:    train_loss 0.17976
wandb: 
wandb: 🚀 View run hardy-sweep-23 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/ug8fdsdz
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_201753-ug8fdsdz/logs
wandb: Agent Starting Run: 23r0vka8 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.25345112594699903
wandb: 	learning_rate: 0.0005743163248349271
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_202025-23r0vka8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/23r0vka8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇███
wandb:    image_loss █▆▇▆▆▄▄▄▃▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁
wandb:    label_loss █▆▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁
wandb: test_accuracy ▁▄▆▆▆▇▇▇███████
wandb:     test_loss █▆▄▄▃▃▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01478
wandb:    label_loss 0.00143
wandb: test_accuracy 97.88
wandb:     test_loss 0.01478
wandb:    train_loss 0.01342
wandb: 
wandb: 🚀 View run rose-sweep-24 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/23r0vka8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_202025-23r0vka8/logs
wandb: Agent Starting Run: nsspff04 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.32508088822321024
wandb: 	learning_rate: 0.008334570813816288
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_202257-nsspff04
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/nsspff04
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 139-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇███
wandb:    image_loss ▆▅▅▅▃▅▅▆▅▃▅▃▄▅▅▅▆█▃▅▆▅▄▃▅▆▅▄▅▂██▄▄▃▇▆▁▄▄
wandb:    label_loss █▄▄▁▄▄▆▄▄▄▆▃▅▆▅▄▂▄▄█▂▂▄▂▄▄▅▆▂▃▅▄▆▄▄▂▅▃▅▅
wandb: test_accuracy ▁████████████▁█
wandb:     test_loss █▅▅▇▃▅▄▃▅█▂▁▅▃▅
wandb:    train_loss ▃▆▃▄▆▆▄▅▄▆▅▂▃▅▄▆▃▃▃▄▄▅▃▃▆▅▅▄▃▄▁█▅▄▅▆▃▃█▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06612
wandb:    label_loss 0.22982
wandb: test_accuracy 11.35
wandb:     test_loss 0.14242
wandb:    train_loss 0.142
wandb: 
wandb: 🚀 View run sweet-sweep-25 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/nsspff04
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_202257-nsspff04/logs
wandb: Agent Starting Run: 7ucfpwrp with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.22388897478676975
wandb: 	learning_rate: 0.00582702151957752
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_202459-7ucfpwrp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/7ucfpwrp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss ▆▇▃▆▃▂▄▂▆▃▄▃▅▅▄▇▁▅▃▆▁▇█▄▅█▇▁▅▇▅▄▂▄▄▂▄▅▂▆
wandb:    label_loss ▁▅▂▅▄▄▄▃▃▄▄▅▇▃▇▄▄▄▅▂▄█▅▃▃▅▆▆▃▆▃▅▅▃▃▃▇▆▇▆
wandb: test_accuracy █████▂██▁█▂███▂
wandb:     test_loss ▁▃▄▂▄▄▅▂▄▄▆▂█▁▂
wandb:    train_loss ▅▅▅▄▂▄▁▅█▄▃▆▅▆▆▃▆▆▄▅█▅▅▅▃▄▆▅▄▇▅▇▇█▂▄▃▅▄▆
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06823
wandb:    label_loss 0.23123
wandb: test_accuracy 10.09
wandb:     test_loss 0.11924
wandb:    train_loss 0.11893
wandb: 
wandb: 🚀 View run sparkling-sweep-26 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/7ucfpwrp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_202459-7ucfpwrp/logs
wandb: Agent Starting Run: 75d7vxis with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.05656275565064495
wandb: 	learning_rate: 0.00314119225829571
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_202700-75d7vxis
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/75d7vxis
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇██
wandb:    image_loss ▃▄▄▆▆█▇▁▃▅▂▇▅▅▄▁▃▄▇▅▇▂▄▅▅█▁▅▆▂▅▂▂▆▄▃▆▄▅▁
wandb:    label_loss ▂▄▃▃▃▂▆▆▅▃▄▁▆▄▁▇▅▆█▅▅▇▄▄▇▄▄▃▆▅▃▄▄▄▂▅▃▃▃▅
wandb: test_accuracy ██▂▁▁▁██▂█▁▂█▁▂
wandb:     test_loss ▆▆▄▃▄▆▁▅▂█▄▃▄▄▅
wandb:    train_loss ▅▄▄▄▆▄▁▅▅▂▅▇▃▄▂▅▄▃▅▅▄▃▆▇▆▄▂▅▆▃▁▆▅▅▅█▄▇▆▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07154
wandb:    label_loss 0.23087
wandb: test_accuracy 10.32
wandb:     test_loss 0.0809
wandb:    train_loss 0.0804
wandb: 
wandb: 🚀 View run golden-sweep-27 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/75d7vxis
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_202700-75d7vxis/logs
wandb: Agent Starting Run: lq7frde2 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17362013605758247
wandb: 	learning_rate: 0.002913578189226893
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_202932-lq7frde2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/lq7frde2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█
wandb:    image_loss ▇█▆▅▅▃▃▂▃▂▂▂▃▂▂▂▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁
wandb:    label_loss ██▅▂▃▂▂▂▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁
wandb: test_accuracy ▁██████████████
wandb:     test_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss ████▆▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02395
wandb:    label_loss 0.0144
wandb: test_accuracy 96.83
wandb:     test_loss 0.02467
wandb:    train_loss 0.02372
wandb: 
wandb: 🚀 View run stellar-sweep-28 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/lq7frde2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_202932-lq7frde2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: c31r5etl with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.08228421812916675
wandb: 	learning_rate: 0.0009219294146380968
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_203209-c31r5etl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/c31r5etl
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇▇████
wandb:    image_loss █▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▆▂▃▃▁▂▂▂▂▁▁▁▂▁▁▂▁▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇▇▇████████
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▇▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00827
wandb:    label_loss 0.00245
wandb: test_accuracy 98.74
wandb:     test_loss 0.00827
wandb:    train_loss 0.00818
wandb: 
wandb: 🚀 View run swift-sweep-29 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/c31r5etl
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_203209-c31r5etl/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: npuycssw with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3692335721267934
wandb: 	learning_rate: 0.007119769485603652
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_203446-npuycssw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/npuycssw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 139-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇███
wandb:    image_loss ▆█▄▇▄▄▇▆▆▇▆▅▆█▅▁▅▅▇▇▄▇▅▅▆▇▆▄▇▇▇▆▆▇▄▃▇▅▆█
wandb:    label_loss ▆▆▆▄▇▄▅▃▆▅▆▃▃▅▅▇▃▇▄▅▄▄▁▄▂▅▅▆▃▄▅▆▅▅▄▆█▅▄▅
wandb: test_accuracy █▂█▂████████▂█▁
wandb:     test_loss ▁▃█▃▂▂▅▂▃▂▆▂█▃▇
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06694
wandb:    label_loss 0.23109
wandb: test_accuracy 10.1
wandb:     test_loss 0.15274
wandb:    train_loss 0.15254
wandb: 
wandb: 🚀 View run kind-sweep-30 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/npuycssw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_203446-npuycssw/logs
wandb: Agent Starting Run: bcwyruu8 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.20776050190257717
wandb: 	learning_rate: 0.009624811385691007
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_203648-bcwyruu8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/bcwyruu8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▄▄▄▆▄▄▆▄▃▅▃▄▅▅▅▄▆▄▃▅▁▄▂▄▆█▅▃▄▃▅▅▅▃▃▆▅▄▄▄
wandb:    label_loss ▄▁▄▅▇▄▄▄▂▆▆▅▆▄█▅▅▆▁▄▄▅▃▇▃▃▄▃▆▅▃▅▄▆▄▄▄▄▃▄
wandb: test_accuracy ▂████████████▁█
wandb:     test_loss ▁▁▃▁▄▃▂▃▁▄█▆▂▄▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06688
wandb:    label_loss 0.23028
wandb: test_accuracy 11.35
wandb:     test_loss 0.1154
wandb:    train_loss 0.11504
wandb: 
wandb: 🚀 View run upbeat-sweep-31 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/bcwyruu8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_203648-bcwyruu8/logs
wandb: Agent Starting Run: aoy918dw with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4078714347919679
wandb: 	learning_rate: 0.008071814330404882
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_203834-aoy918dw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/aoy918dw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇████
wandb:    image_loss ▆▄▄▃▃▆▆▆▂█▄▃▄▆▅▂▁▃▂▃▆▆▃▇▄▅▄▆▆▃▃▄▄▅▄▅▄▂▄▄
wandb:    label_loss ▂▄▃▄▂▅▃▆▃▆▄▅▅▁▂▂▃▄▂▁▅▂▃█▃▆▃▄▂▄▅▃▂▃▄▂▄▂▅▂
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▆▁▂▂▆▃▆▅▄▇▂█▆▄
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06678
wandb:    label_loss 0.22978
wandb: test_accuracy 11.35
wandb:     test_loss 0.16145
wandb:    train_loss 0.16123
wandb: 
wandb: 🚀 View run lunar-sweep-32 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/aoy918dw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_203834-aoy918dw/logs
wandb: Agent Starting Run: vvt5xzhn with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3629731399028628
wandb: 	learning_rate: 0.0017996595900841223
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_204020-vvt5xzhn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/vvt5xzhn
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▇▇▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▇▆▅▃▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▇█████████████
wandb:     test_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01276
wandb:    label_loss 0.00439
wandb: test_accuracy 98.15
wandb:     test_loss 0.01587
wandb:    train_loss 0.01337
wandb: 
wandb: 🚀 View run pious-sweep-33 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/vvt5xzhn
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_204020-vvt5xzhn/logs
wandb: Agent Starting Run: ruwlxnj0 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3978942968426541
wandb: 	learning_rate: 0.0010760519546168558
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_204207-ruwlxnj0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/ruwlxnj0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇███
wandb:    image_loss ██▇▆▆▅▄▃▄▄▃▄▃▃▃▃▂▂▃▂▂▃▁▂▂▂▁▁▂▁▁▂▁▂▁▁▁▂▁▁
wandb:    label_loss █▆▅▂▃▃▁▁▂▁▂▂▁▂▂▂▁▁▁▄▄▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▆▆▇▇▇▇▇▇▇████
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▇▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01214
wandb:    label_loss 0.00013
wandb: test_accuracy 98.28
wandb:     test_loss 0.01493
wandb:    train_loss 0.01261
wandb: 
wandb: 🚀 View run stilted-sweep-34 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/ruwlxnj0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_204207-ruwlxnj0/logs
wandb: Agent Starting Run: y7swcgvv with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.34132773052734744
wandb: 	learning_rate: 0.002764383025338989
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_204439-y7swcgvv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/y7swcgvv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▇▇█▇▆▆▆▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁
wandb:    label_loss █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▇█████████████
wandb:     test_loss █▄▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss ████▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01776
wandb:    label_loss 0.00093
wandb: test_accuracy 97.69
wandb:     test_loss 0.01981
wandb:    train_loss 0.01781
wandb: 
wandb: 🚀 View run valiant-sweep-35 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/y7swcgvv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_204439-y7swcgvv/logs
wandb: Agent Starting Run: clwi2r1o with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1309406986620068
wandb: 	learning_rate: 0.008628806378256838
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_204640-clwi2r1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/clwi2r1o
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:    image_loss ▄▄▆▄▂▂▄▂▇▃▅▄▁▄▂▆▆▄▆▄▅▄▅▆▆▃▁▅▃▁█▄▃▃▇▂▇▂▂▃
wandb:    label_loss ▅▁▅▅▇▃▅▅▅▅▃▅▅▇▅▃▆▁▄▄▅▆▂▆▂▃▂▅▄▃▆▅▅▄▅█▅▅▃▄
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▃▃▃▇▃▃▆▇▆▇▁▆██▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06582
wandb:    label_loss 0.23011
wandb: test_accuracy 11.35
wandb:     test_loss 0.0977
wandb:    train_loss 0.0973
wandb: 
wandb: 🚀 View run misunderstood-sweep-36 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/clwi2r1o
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_204640-clwi2r1o/logs
wandb: Agent Starting Run: 25qht6ta with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2934911859406342
wandb: 	learning_rate: 0.00936948077279572
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_204827-25qht6ta
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/25qht6ta
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▅▄▅▅▄▅▃▄▄▂▆▄▃▂▃▄▄▂▅▃▆▅▃▃▄▅▆▃▄█▃▆▅▃▃▁▅▅▃▆
wandb:    label_loss ▄▅▆▃▅▅▄▇▂▁▃▄▅▄▃▃▁▇█▄▅▇▄▄▄▄▁▆▃▆▃▆▅▆▂▆▅▄▅▄
wandb: test_accuracy █▂████████▂▁███
wandb:     test_loss ▄▅▆▆█▇▁▆▅▄▇▇▆▆▃
wandb:    train_loss ▅▅▆▅▆▅▄▃▃▇▄▄▇█▁▃█▃▆▅▅▅█▅▄▃▅▆▇▄▄▃▇▆▄▃▃▅▃▆
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06495
wandb:    label_loss 0.23091
wandb: test_accuracy 11.35
wandb:     test_loss 0.13516
wandb:    train_loss 0.13519
wandb: 
wandb: 🚀 View run flowing-sweep-37 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/25qht6ta
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_204827-25qht6ta/logs
wandb: Agent Starting Run: 5q9c9ya9 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.433213845032994
wandb: 	learning_rate: 0.0042649920247488675
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_205058-5q9c9ya9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/5q9c9ya9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss ▅▃▄▃▂▅▃▄▁▆▄▂▅▂▅▅▄▅▂▆▂▅▅▄▄▇▅▃▆▆▂▆▃▅▄▃▃▄▄█
wandb:    label_loss ▅▁▄▄▇▇▅▇▆▄▅▆█▂▆█▇▅▅▄▅▇▆▄▄██▆▅▄▇▄▆▅▆▃▇▅▄▆
wandb: test_accuracy ▃█▂█████▁▄█████
wandb:     test_loss ▃▁▄▃▅▄▁▅█▂▂▄▅▃▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06788
wandb:    label_loss 0.23068
wandb: test_accuracy 11.35
wandb:     test_loss 0.16753
wandb:    train_loss 0.16734
wandb: 
wandb: 🚀 View run feasible-sweep-38 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/5q9c9ya9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_205058-5q9c9ya9/logs
wandb: Agent Starting Run: va04gblo with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4696098597751436
wandb: 	learning_rate: 0.0030867240417689168
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_205245-va04gblo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/va04gblo
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█
wandb:    image_loss ▅▆▅▄▆▄▄▄▄▄▄▄▆▇▂█▄▂▆▃▅▄▅▅▆▅▂▆▅▅▄▅▁▅▃▆▅▂▆▃
wandb:    label_loss ▆▄▄█▅▃▁▆▅▄▅▇▅▆▆▇▆▃█▅▆▅▆▆▇▅▆▃▄▃▆▄▄▅▄▃▇▅▄▄
wandb: test_accuracy ▁███▁██████████
wandb:     test_loss ▂▆▂▃▇█▅▁▄▂▂▄▅▁▇
wandb:    train_loss █▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06589
wandb:    label_loss 0.22977
wandb: test_accuracy 11.35
wandb:     test_loss 0.17581
wandb:    train_loss 0.17551
wandb: 
wandb: 🚀 View run chocolate-sweep-39 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/va04gblo
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_205245-va04gblo/logs
wandb: Agent Starting Run: j35eiu47 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4214638923661333
wandb: 	learning_rate: 0.005366913549751799
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_205431-j35eiu47
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_128_sigmoid/sweeps/dkdcb5fd
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/j35eiu47
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▂█▄▃▄▅▂▂▁▄▅▅▅▂▄▃▃█▄▅▃▄▅▂▄▆▃▃▄▃▃▅▃▃▄▅▄▁▃▄
wandb:    label_loss ▂▂▂▃▁▃▂▄▃▂▁▃▅▄▂▃▃▃▅▂▁▂▄▃▃▂▃█▂▃▃▃▂▂▁▅▁▃▄▂
wandb: test_accuracy ███▂███▂█▂▁▁███
wandb:     test_loss ▁▆▄▃▁▅▄▂▅▅█▆▃▃▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06803
wandb:    label_loss 0.22981
wandb: test_accuracy 11.35
wandb:     test_loss 0.1647
wandb:    train_loss 0.16425
wandb: 
wandb: 🚀 View run ethereal-sweep-40 at: https://wandb.ai/cavaokcava/auto_128_sigmoid/runs/j35eiu47
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_128_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_205431-j35eiu47/logs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: qk7d6mek with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.041715829374278424
wandb: 	learning_rate: 0.007110398803786413
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_205625-qk7d6mek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/qk7d6mek
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▂▁▁▁     
wandb:    label_loss █▅▂▃▃▁▂▂▁▁▂▁▁▁▂▁▁▁▁▁▂▁▂▁▁▁▁▂▁▁▂▁▂▂▁▂▁   
wandb: test_accuracy ▇████████████▁▁
wandb:     test_loss █▅▂▁▁▁▁▁▁▂▂▂▂  
wandb:    train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run cool-sweep-1 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/qk7d6mek
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_205625-qk7d6mek/logs
wandb: Agent Starting Run: kcda3502 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2992563917367386
wandb: 	learning_rate: 0.0015413530141767551
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_205826-kcda3502
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/kcda3502
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████
wandb:    image_loss █▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁
wandb:    label_loss █▆▆▃▂▃▃▄▃▂▃▁▂▂▂▂▁▂▁▁▁▂▁▁▁▂▃▁▂▁▂▁▁▂▁▃▂▁▂▁
wandb: test_accuracy ▁▃▅▅▆█▄▆▇▆▆██▇▇
wandb:     test_loss █▅▄▃▃▂▃▂▂▂▂▁▁▁▁
wandb:    train_loss █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01101
wandb:    label_loss 0.00567
wandb: test_accuracy 97.58
wandb:     test_loss 0.01304
wandb:    train_loss 0.01122
wandb: 
wandb: 🚀 View run proud-sweep-2 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/kcda3502
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_205826-kcda3502/logs
wandb: Agent Starting Run: fb59wu4y with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3940197711549751
wandb: 	learning_rate: 0.005114337734214577
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_210013-fb59wu4y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/fb59wu4y
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▄▄▃▃▂▂▁▂▁▁▁▂▂▂▁▂▁▁▂▂▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁
wandb:    label_loss █▄▇▅▄▁▆▂▃▃▄▂▅▄▅▂▂▅▂▂▃▃▄▃▂▃▃▁▁▃▃▁▁▁▃▄▃▂▁▃
wandb: test_accuracy ▁▄▆▆▇▆█▆▇▆▇▇██▇
wandb:     test_loss █▄▂▂▂▂▁▁▁▄▃▂▂▁▁
wandb:    train_loss █▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02244
wandb:    label_loss 0.00716
wandb: test_accuracy 97.09
wandb:     test_loss 0.02527
wandb:    train_loss 0.02394
wandb: 
wandb: 🚀 View run visionary-sweep-3 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/fb59wu4y
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_210013-fb59wu4y/logs
wandb: Agent Starting Run: w6zu53vq with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.46979034462985986
wandb: 	learning_rate: 0.0012853465774992916
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_210159-w6zu53vq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/w6zu53vq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆█▇█▄▆▆▄▃▃▂▂▂▁▃▃▂▂▁▁▃▃▂▁▁▁▁▂▄▁▃▂▁▁▂▂▂▂▂
wandb: test_accuracy ▁▂▄▅▇▅▆▇▅▇█▇██▇
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01117
wandb:    label_loss 0.00371
wandb: test_accuracy 97.76
wandb:     test_loss 0.01509
wandb:    train_loss 0.01328
wandb: 
wandb: 🚀 View run glamorous-sweep-4 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/w6zu53vq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_210159-w6zu53vq/logs
wandb: Agent Starting Run: ejhbpu17 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07049430064743978
wandb: 	learning_rate: 0.0060168949468972915
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_210345-ejhbpu17
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/ejhbpu17
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇████
wandb:    image_loss █▅▅▄▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▃▃▂▂▂▂▂▁▂▁▁▁▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb: test_accuracy ▁▅▇▇▇██▇▇██▇███
wandb:     test_loss █▅▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01279
wandb:    label_loss 0.00552
wandb: test_accuracy 96.2
wandb:     test_loss 0.01378
wandb:    train_loss 0.01397
wandb: 
wandb: 🚀 View run eternal-sweep-5 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/ejhbpu17
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_210345-ejhbpu17/logs
wandb: Agent Starting Run: ffpykur8 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2096196820089799
wandb: 	learning_rate: 0.0024519854624324906
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_210532-ffpykur8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/ffpykur8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▃▄▃▄▃▂▂▃▃▂▂▂▂▄▂▁▂▁▂▂▁▂▁▁▂▂▂▃▁▂▁▁▁▃▁▁▁▃
wandb: test_accuracy ▁▂▄▆▇▇▅▅▆█▇▆▇█▇
wandb:     test_loss █▅▅▃▃▃▃▃▂▁▂▂▁▁▁
wandb:    train_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01098
wandb:    label_loss 0.01052
wandb: test_accuracy 97.46
wandb:     test_loss 0.01293
wandb:    train_loss 0.01178
wandb: 
wandb: 🚀 View run fancy-sweep-6 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/ffpykur8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_210532-ffpykur8/logs
wandb: Agent Starting Run: qzsynuse with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.09563698340966016
wandb: 	learning_rate: 0.00594621564812813
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_210718-qzsynuse
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/qzsynuse
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-284, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▂▁▁▂▁▁▁▅▃▃▃▃▆▂                        
wandb:    label_loss ▄▁▁▄██▄▁▅▆▇▃▃█▁                         
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▂▅█           
wandb:    train_loss █▃▁▁▂▂▂▂▂                               
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run efficient-sweep-7 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/qzsynuse
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_210718-qzsynuse/logs
wandb: Agent Starting Run: jtcz40kt with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1871388051039674
wandb: 	learning_rate: 0.008626008443103414
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_210956-jtcz40kt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/jtcz40kt
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 258-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss █▇▅▃▃▁▂▁▁▁▂▃▃▃▄                         
wandb:    label_loss █▇▃▂▁▂▂▁▂▁▄                             
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▁▄▆           
wandb:    train_loss █▂▂▁▁▁▁▁▁▁▁                             
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run blooming-sweep-8 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/jtcz40kt
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_210956-jtcz40kt/logs
wandb: Agent Starting Run: 5dibf4tr with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.10659320640309984
wandb: 	learning_rate: 0.006522568465877916
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_211233-5dibf4tr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/5dibf4tr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▁▁▁▁▁▂▃▃▂▄▃▃▄                          
wandb:    label_loss █▅▄▃▃▂▂▂▅▂▂▁▁▁▁                         
wandb: test_accuracy █████▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▂▁▅██          
wandb:    train_loss █▂▂▁▁▂▂▃▄▃▄▅▆▅▃▄▃                       
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run graceful-sweep-9 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/5dibf4tr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_211233-5dibf4tr/logs
wandb: Agent Starting Run: 92ijit6u with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.19300973899888052
wandb: 	learning_rate: 0.009974319565613562
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_211510-92ijit6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/92ijit6u
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▁▁                                     
wandb:    label_loss ▆█▃▁                                    
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▆▅▁                                    
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run wandering-sweep-10 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/92ijit6u
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_211510-92ijit6u/logs
wandb: Agent Starting Run: nri6b33f with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.04120703293539896
wandb: 	learning_rate: 0.00472434766063383
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_211747-nri6b33f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/nri6b33f
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▆▄▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▄▃▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇▇▇█▇▇▇█▇▇█
wandb:     test_loss █▄▃▂▂▁▁▁▁▁▁▁▂▁▁
wandb:    train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01146
wandb:    label_loss 0.01089
wandb: test_accuracy 96.28
wandb:     test_loss 0.01171
wandb:    train_loss 0.01163
wandb: 
wandb: 🚀 View run silver-sweep-11 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/nri6b33f
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_211747-nri6b33f/logs
wandb: Agent Starting Run: hfdqsyik with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.06520110848966054
wandb: 	learning_rate: 0.006845057848823111
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_211933-hfdqsyik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/hfdqsyik
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇███
wandb:    image_loss █▆▄▄▄▂▂▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▃▂▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇█▇█▇██▇▇▇▇
wandb:     test_loss █▅▃▂▂▁▁▁▁▁▁▁▁▁▂
wandb:    train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01342
wandb:    label_loss 0.00993
wandb: test_accuracy 95.46
wandb:     test_loss 0.01464
wandb:    train_loss 0.01391
wandb: 
wandb: 🚀 View run ancient-sweep-12 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/hfdqsyik
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_211933-hfdqsyik/logs
wandb: Agent Starting Run: y6l6dpm0 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3967212706666306
wandb: 	learning_rate: 0.006366245994760507
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_212120-y6l6dpm0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/y6l6dpm0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-283, console lines 205-229
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█
wandb:    image_loss █▃▂▂▁▂▁▁▃▅▇                             
wandb:    label_loss █▆▆▃▅▅▆▁▂                               
wandb: test_accuracy ███▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▄▁█            
wandb:    train_loss █▂▁▁▁▃▂▃▃                               
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run splendid-sweep-13 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/y6l6dpm0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_212120-y6l6dpm0/logs
wandb: Agent Starting Run: iwjohrp2 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4701426893863593
wandb: 	learning_rate: 0.005097425581380871
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_212357-iwjohrp2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/iwjohrp2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:    image_loss █▆▅▄▃▂▂▂▂▂▁▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▂▂▁▁▂▁▁▁▁▁▁▂
wandb:    label_loss █▅▄▅▅▂▃▃▂▃▁▃▃▃▂▁▂▁▂▂▂▁▃▂▃▁▂▂▃▁▁▁▁▂▁▁▃▁▁▂
wandb: test_accuracy ▁▅▄▇▅▇▆▇▇▆▇▇▇█▇
wandb:     test_loss █▄▃▁▂▂▂▂▂▂▃▃▂▁▂
wandb:    train_loss █▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02257
wandb:    label_loss 0.00578
wandb: test_accuracy 97.16
wandb:     test_loss 0.02773
wandb:    train_loss 0.02436
wandb: 
wandb: 🚀 View run resilient-sweep-14 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/iwjohrp2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_212357-iwjohrp2/logs
wandb: Agent Starting Run: cf2xm0dw with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2501340946344641
wandb: 	learning_rate: 0.0025133536429430797
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_212543-cf2xm0dw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/cf2xm0dw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▇▅▅▃▂▃▃▃▃▃▃▃▃▃▂▃▃▂▂▁▂▂▂▂▂▃▂▁▂▂▁▂▂▃▃▂▁▁▁
wandb:    label_loss █▂▃▆▃▅▁▃▁▃▂▁▁▂▅▁▂▄▁▂▂▁▂▂▃▂▂▁▂▂▅▂▁▂▂▁▂▁▁▁
wandb: test_accuracy ▁▅▇▆█▇▆█▅▅██▇▇▇
wandb:     test_loss █▄▃▄▂▂▂▃▄▃▂▂▂▁▂
wandb:    train_loss █▆▅▅▃▃▃▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0132
wandb:    label_loss 0.00178
wandb: test_accuracy 97.07
wandb:     test_loss 0.01823
wandb:    train_loss 0.01651
wandb: 
wandb: 🚀 View run charmed-sweep-15 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/cf2xm0dw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_212543-cf2xm0dw/logs
wandb: Agent Starting Run: b9u9jvh8 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3799483072046483
wandb: 	learning_rate: 0.002738819764151425
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_212750-b9u9jvh8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/b9u9jvh8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂▁▁▂▂▂▁
wandb:    label_loss ▃▅▆▅▂█▅▆▁▂▆▃▄▅▁▂▁▂▃▃▃▄▂▁▄▁▂▄▁▁▁▁▃▁▁▂▄▂▄▁
wandb: test_accuracy ▁▃▅▆▇▇▅▇▆▇▇█▇█▇
wandb:     test_loss █▅▄▃▃▂▃▂▂▃▂▂▂▁▂
wandb:    train_loss █▇▆▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▁▂▂▂▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01685
wandb:    label_loss 0.02086
wandb: test_accuracy 97.43
wandb:     test_loss 0.02198
wandb:    train_loss 0.01963
wandb: 
wandb: 🚀 View run decent-sweep-16 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/b9u9jvh8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_212750-b9u9jvh8/logs
wandb: Agent Starting Run: a80w5rzq with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.10455329852263008
wandb: 	learning_rate: 0.006706177869645375
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_212957-a80w5rzq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/a80w5rzq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█
wandb:    image_loss █▃▃▃▃▂▁▁▂▁▂▁▁▁▁▂▁▂▂▂▃▂▂▂▂▂▂▂▂▂          
wandb:    label_loss █▂▂▁▃▂▁▁▁▁▂▁▁▂▁▂▂▂▂▂▂▁▁▁▂▂▂▂▂▂▂         
wandb: test_accuracy ██████████▁▁▁▁▁
wandb:     test_loss █▂▁▁▂▂▃▄▄▅     
wandb:    train_loss █▆▆▅▃▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁▂▂▂▁▂▂▂▂         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run playful-sweep-17 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/a80w5rzq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_212957-a80w5rzq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: f4micro9 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4232330846377447
wandb: 	learning_rate: 0.006691277546840153
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_213208-f4micro9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/f4micro9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▁▂▃▁▃▆▃█                                
wandb:    label_loss ▆▃█▄▂▁▁▃▃▆▅▆▁▃▄▃▄                       
wandb: test_accuracy ████▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▃██           
wandb:    train_loss ▅▄▂▁▂▃█▅▅                               
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run electric-sweep-18 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/f4micro9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_213208-f4micro9/logs
wandb: Agent Starting Run: 3uvtfg7k with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4090406846735016
wandb: 	learning_rate: 0.007930345981586723
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_213447-3uvtfg7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/3uvtfg7k
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▇█▆▆▆▄▃▂▂▁▂▂▁▁▁▁▁▁▁▂▂▂▂▃▂▃▃▃▃▂          
wandb:    label_loss █▄▂▂▂▁▂▂▁▁▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▂▁▂▂▁          
wandb: test_accuracy ▇██████████▁▁▁▁
wandb:     test_loss ▇▄▂▁▁▁▁▂▃▄█    
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁          
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run expert-sweep-19 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/3uvtfg7k
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_213447-3uvtfg7k/logs
wandb: Agent Starting Run: 4gbb1jnq with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.31941157356660493
wandb: 	learning_rate: 0.005711715145886334
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_213653-4gbb1jnq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/4gbb1jnq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ▁▁▃▃▃█▄▃█▂▅▅▄▄                          
wandb:    label_loss █▂▅▄▄▂▂▂▂▂▇▁▃▁▂▂▂                       
wandb: test_accuracy ██████▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▅▅▅▂▁         
wandb:    train_loss █▃▃▃▃▁▁▂▂▄▂▃▂▂▃                         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run scarlet-sweep-20 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/4gbb1jnq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_213653-4gbb1jnq/logs
wandb: Agent Starting Run: 3vn3fiou with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.04849688929851992
wandb: 	learning_rate: 0.0010731182447957322
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_213930-3vn3fiou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/3vn3fiou
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-283, console lines 205-224
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇███
wandb:    image_loss █▇▄▄▄▃▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▂▂▁▁▁▁▂▂▁▁▁▂▂▁
wandb:    label_loss ▆██▅▃▃▄▁▁▃▃▆▃▂▂▂▂▆▁▄▁▂▃▇▃▁▃▃▄▄▁▁▂▃▂▂▂▁▁▁
wandb: test_accuracy ▁▃▂▆▆▇▆▇▇▆▇█▇▇█
wandb:     test_loss █▅▄▃▃▂▂▂▁▁▂▁▁▁▁
wandb:    train_loss █▅▄▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00831
wandb:    label_loss 0.01243
wandb: test_accuracy 96.96
wandb:     test_loss 0.00874
wandb:    train_loss 0.0088
wandb: 
wandb: 🚀 View run kind-sweep-21 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/3vn3fiou
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_213930-3vn3fiou/logs
wandb: Agent Starting Run: f8pm08ki with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.26604810834037557
wandb: 	learning_rate: 0.006932675701959691
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_214208-f8pm08ki
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/f8pm08ki
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 134-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▆▅▂▂▂▂▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▂▃               
wandb:    label_loss █▄▄▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂▁▂▂▂          
wandb: test_accuracy ██████████▁▁▁▁▁
wandb:     test_loss █▂▁▂▁▃▃▂▂▄     
wandb:    train_loss █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁            
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run lucky-sweep-22 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/f8pm08ki
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_214208-f8pm08ki/logs
wandb: Agent Starting Run: r5rzj3oy with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.22332974213242035
wandb: 	learning_rate: 0.008586554110265293
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_214414-r5rzj3oy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/r5rzj3oy
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▃▃▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▇▇▇████████▇█
wandb:     test_loss █▅▄▃▂▂▂▁▁▁▁▁▂▂▁
wandb:    train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01994
wandb:    label_loss 0.00569
wandb: test_accuracy 96.63
wandb:     test_loss 0.02233
wandb:    train_loss 0.02076
wandb: 
wandb: 🚀 View run sparkling-sweep-23 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/r5rzj3oy
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_214414-r5rzj3oy/logs
wandb: Agent Starting Run: 3j7ic94d with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2379079214181387
wandb: 	learning_rate: 0.008867177015386635
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_214601-3j7ic94d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/3j7ic94d
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇████
wandb:    image_loss █▆▅▅▄▃▃▃▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▂▂▂▂▃▃▂▂         
wandb:    label_loss █▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▁▂▁▁▂▁▂▁▁        
wandb: test_accuracy ▇▇█████████▁▁▁▁
wandb:     test_loss █▅▂▁▁▂▂▂▂▂▂    
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁            
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run dazzling-sweep-24 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/3j7ic94d
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_214601-3j7ic94d/logs
wandb: Agent Starting Run: zj0g2nl2 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.414951659948858
wandb: 	learning_rate: 0.0025194224145980997
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_214808-zj0g2nl2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/zj0g2nl2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▄▄▄▃▄▃▃▂▃▃▂▃▃▃▃▃▂▃▂▂▂▂▃▂▂▂▃▂▂▂▁▁▂▂▂▁▃▂
wandb:    label_loss █▃▅▂▇▂█▄▅▇▆▂▁▇▂▅▂▂▁█▁█▁▄▁▁▁▁▁▁▂▄▁▁▁▃▃▁▁▅
wandb: test_accuracy ▁▅▆▄▆▅█▆▆█▆█▇▅▆
wandb:     test_loss █▅▄▅▃▄▂▃▂▂▃▃▁▃▃
wandb:    train_loss █▅▅▄▄▂▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▃▂▂▂▁▁▁▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02127
wandb:    label_loss 0.00288
wandb: test_accuracy 96.82
wandb:     test_loss 0.02785
wandb:    train_loss 0.0242
wandb: 
wandb: 🚀 View run lucky-sweep-25 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/zj0g2nl2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_214808-zj0g2nl2/logs
wandb: Agent Starting Run: 98lt52ok with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.15144298826911118
wandb: 	learning_rate: 0.006973359960097436
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_215045-98lt52ok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/98lt52ok
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇████
wandb:    image_loss ███▇▇▇▆▅▆▅▄▄▄▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁
wandb:    label_loss ██▇▆▆▅▄▄▃▃▂▂▂▂▃▂▂▂▂▁▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb: test_accuracy ▁▃▆▇▇██████████
wandb:     test_loss █▇▅▄▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01798
wandb:    label_loss 0.00991
wandb: test_accuracy 96.75
wandb:     test_loss 0.01861
wandb:    train_loss 0.01917
wandb: 
wandb: 🚀 View run restful-sweep-26 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/98lt52ok
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_215045-98lt52ok/logs
wandb: Agent Starting Run: edi0pvw4 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17921139140705472
wandb: 	learning_rate: 0.008174767064187469
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_215231-edi0pvw4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/edi0pvw4
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▆▅▃▃▂▂▂▂▂▁▁▁▁▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▃▃▂▃       
wandb:    label_loss ██▅▅▃▂▃▁▁▂▁▂▁▂▁▂▂▁▂▂▂▂▂▂▁▂▁▂▂▂▂         
wandb: test_accuracy ▇██████████▁▁▁▁
wandb:     test_loss █▄▂▂▁▁▂▂▂▃▃    
wandb:    train_loss █▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run confused-sweep-27 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/edi0pvw4
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_215231-edi0pvw4/logs
wandb: Agent Starting Run: w2hn9xn0 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.0923986697804856
wandb: 	learning_rate: 0.0006923066037582858
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_215438-w2hn9xn0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/w2hn9xn0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss ▇█▆▆▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▂▂▂▂▁▂▂
wandb:    label_loss ▅▃▅▄▃▄▄▁▂▂▁▂▂█▂▁▂▂▂▂▁▁▁▁▃▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁
wandb: test_accuracy ▁▂▄▄▆▆▇▇▆▆▆▇█▆█
wandb:     test_loss █▆▄▄▃▃▂▂▂▂▂▂▁▁▁
wandb:    train_loss █▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00741
wandb:    label_loss 0.00291
wandb: test_accuracy 97.73
wandb:     test_loss 0.00809
wandb:    train_loss 0.00806
wandb: 
wandb: 🚀 View run solar-sweep-28 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/w2hn9xn0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_215438-w2hn9xn0/logs
wandb: Agent Starting Run: wm5uad3s with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.15815800584474343
wandb: 	learning_rate: 0.006765701789253677
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_215715-wm5uad3s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/wm5uad3s
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▇▇▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▅▄▃▂▂▂▁▂▁▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▆▇▇███████████
wandb:     test_loss █▅▄▃▂▂▂▁▁▁▁▁▁▁▁
wandb:    train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01726
wandb:    label_loss 0.00632
wandb: test_accuracy 96.27
wandb:     test_loss 0.01773
wandb:    train_loss 0.01848
wandb: 
wandb: 🚀 View run lively-sweep-29 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/wm5uad3s
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_215715-wm5uad3s/logs
wandb: Agent Starting Run: u4sautaa with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.17844422579076008
wandb: 	learning_rate: 0.005210160495902776
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_215901-u4sautaa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/u4sautaa
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-283, console lines 205-229
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▃▂▁▃▃▄▁▃▇▄▃▃▃▄▅▄▄▅                    
wandb:    label_loss █▂▂▂▁▂▁▁▂▁▂▁▂▃▂                         
wandb: test_accuracy ██████▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▆█▇▄▆         
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁                         
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run frosty-sweep-30 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/u4sautaa
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_215901-u4sautaa/logs
wandb: Agent Starting Run: u8xjftvq with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.14226277518170738
wandb: 	learning_rate: 0.006975421009594746
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_220138-u8xjftvq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/u8xjftvq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 134-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇███
wandb:    image_loss █▆▄▂▂▂▂▁▁▁▁▁▁▁▁▂▁▂▁▂▂▁▂▂▂▂▃▂▃▂▂▂▂▂▂▂    
wandb:    label_loss █▆▃▂▂▂▂▁▂▂▁▁▁▂▂▂▁▁▁▁▂▂▁▃▂▁▂▂▁▂▁▁▁▂▁▁▁▁  
wandb: test_accuracy █████████████▁▁
wandb:     test_loss █▃▂▁▁▁▂▃▅▄▄▄▅  
wandb:    train_loss █▇▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂  
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run youthful-sweep-31 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/u8xjftvq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_220138-u8xjftvq/logs
wandb: Agent Starting Run: nwneazks with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.0775098867327953
wandb: 	learning_rate: 0.007145952811829125
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_220345-nwneazks
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/nwneazks
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss █▆▃▃▃▂▂▂▁▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃            
wandb:    label_loss █▅▄▂▄▃▃▃▂▄▃▁▆▄▅▃▂▄▄▃▂▅▃▂▃▂▂▂            
wandb: test_accuracy ██████████▁▁▁▁▁
wandb:     test_loss █▂▁▁▂▂▃▄▄▅     
wandb:    train_loss █▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃        
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run giddy-sweep-32 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/nwneazks
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_220345-nwneazks/logs
wandb: Agent Starting Run: n44qtlno with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.45371230948757785
wandb: 	learning_rate: 0.0009809492071765563
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_220552-n44qtlno
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/n44qtlno
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▆▅▅▅▅▄▄▄▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb:    label_loss ▇▅█▂▃▃▂▄▃▂▂▆▄▁▄▁▂▁▄▄▁▁▂▁▁▁▁▂▂▂▁▂▁▂▁▁▁▁▁▁
wandb: test_accuracy ▁▅▅▅▇▅▇▆▇▇▇▇▆▆█
wandb:     test_loss █▅▄▃▃▃▂▂▂▂▂▁▂▂▁
wandb:    train_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01121
wandb:    label_loss 0.00043
wandb: test_accuracy 98.03
wandb:     test_loss 0.01391
wandb:    train_loss 0.01442
wandb: 
wandb: 🚀 View run whole-sweep-33 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/n44qtlno
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_220552-n44qtlno/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: axoftoqs with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.15303733327110375
wandb: 	learning_rate: 0.007132293275225984
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_220803-axoftoqs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/axoftoqs
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ██▇▆▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▆▆▅▃▂▂▂▁▁▁▁▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁
wandb: test_accuracy ▁▆▇▇███▇███████
wandb:     test_loss █▅▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01828
wandb:    label_loss 0.01131
wandb: test_accuracy 96.42
wandb:     test_loss 0.01947
wandb:    train_loss 0.02007
wandb: 
wandb: 🚀 View run tough-sweep-34 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/axoftoqs
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_220803-axoftoqs/logs
wandb: Agent Starting Run: k7rdgmod with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.11481568766407069
wandb: 	learning_rate: 0.002560230559033602
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_220950-k7rdgmod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/k7rdgmod
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 134-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▇▆▅▄▄▅▃▂▃▂▂▃▂▂▂▂▃▂▂▂▁▂▃▁▁▁▂▁▁▁▁▂▁▂▂▁▂▂
wandb:    label_loss █▅▅▂▂▃▂▂▂▂▃▃▃▁▁▂▁▃▃▂▁▃▁▁▁▂▂▂▃▁▁▃▁▁▃▁▂▄▁▂
wandb: test_accuracy ▁▁▃▃▅▅▄▇▆██▆▇▆▇
wandb:     test_loss █▇▄▄▃▃▂▂▁▂▂▁▂▂▁
wandb:    train_loss █▆▅▄▄▄▃▄▄▃▃▂▂▃▂▂▂▂▃▃▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01343
wandb:    label_loss 0.00655
wandb: test_accuracy 97.2
wandb:     test_loss 0.01314
wandb:    train_loss 0.01311
wandb: 
wandb: 🚀 View run fluent-sweep-35 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/k7rdgmod
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_220950-k7rdgmod/logs
wandb: Agent Starting Run: crfiisyq with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.023039414931527613
wandb: 	learning_rate: 0.0036544911221183217
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_221156-crfiisyq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/crfiisyq
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇████
wandb:    image_loss █▇▄▃▄▃▄▃▃▂▂▂▂▂▁▂▁▁▁▂▁▁▂▂▁▂▂▁▁▂▁▂▁▂▁▁▂▁▂▁
wandb:    label_loss ▅▄▄▄▃▃▄▃▂▄▃▅▃▃▅▄▂▆▃▁▃▅▁▂▃▇▅▂▁▁▂▄▆▁▃▂▂▁█▁
wandb: test_accuracy ▁▃▆▅▆▅█▇▇▆▇██▆█
wandb:     test_loss █▆▅▅▃▃▄▂▂▃▃▁▂▃▁
wandb:    train_loss █▇▆▆▆▅▅▃▄▃▂▂▂▃▂▂▃▁▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▂▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01456
wandb:    label_loss 0.00842
wandb: test_accuracy 94.39
wandb:     test_loss 0.01517
wandb:    train_loss 0.01583
wandb: 
wandb: 🚀 View run trim-sweep-36 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/crfiisyq
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_221156-crfiisyq/logs
wandb: Agent Starting Run: m8p3wq7t with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.004949989205252525
wandb: 	learning_rate: 0.0024307391463315054
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_221434-m8p3wq7t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/m8p3wq7t
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 134-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇████
wandb:    image_loss █▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▃▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁
wandb:    label_loss ▅▇▆▃▃▆▆▄█▂▃▅▃▄▁▃▄▃▂▄▂▄▆▅▂▄▄▂▁▁▂▅▂▃▃▃▄▂▂▂
wandb: test_accuracy ▁▄▅▄▆▅▆▆▇▆▇▇▆██
wandb:     test_loss █▅▅▅▅▄▃▃▂▂▂▁▁▁▁
wandb:    train_loss █▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.00933
wandb:    label_loss 0.01449
wandb: test_accuracy 93.19
wandb:     test_loss 0.00897
wandb:    train_loss 0.00896
wandb: 
wandb: 🚀 View run giddy-sweep-37 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/m8p3wq7t
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_221434-m8p3wq7t/logs
wandb: Agent Starting Run: asl7yoy3 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3754948622057559
wandb: 	learning_rate: 0.008935727925977284
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_221640-asl7yoy3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/asl7yoy3
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 134-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇█
wandb:    image_loss █▆▅▃▁▁▁                                 
wandb:    label_loss █▆▅▅▄▃▂▂▁▁                              
wandb: test_accuracy ▆██▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss █▂▁            
wandb:    train_loss █▄▃▃▃▂▁▁▁▁                              
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss nan
wandb:    label_loss nan
wandb: test_accuracy 9.8
wandb:     test_loss nan
wandb:    train_loss nan
wandb: 
wandb: 🚀 View run serene-sweep-38 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/asl7yoy3
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_221640-asl7yoy3/logs
wandb: Agent Starting Run: euh0ttcv with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.38772344166451395
wandb: 	learning_rate: 0.0027226809969592867
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_221847-euh0ttcv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/euh0ttcv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▅▅▄▅▆▄▄▄▃▃▄▃▃▃▄▃▃▃▃▅▂▄▃▃▃▃▃▃▃▃▂▄▃▃▂▁▂
wandb:    label_loss ▅█▄▃▃▃▁▆▃▂▂▁▃▃▄▂▂▂▃▃▁▂▂▂▂▂▃▁▃▃▄▂▁▃▃▁▁▂▁▂
wandb: test_accuracy ▁▄▄▆▇▆▆█▅▇▇▇▆█▇
wandb:     test_loss █▅▄▃▂▃▂▂▄▂▃▂▃▁▃
wandb:    train_loss █▅▅▅▄▄▄▄▃▃▃▃▂▃▃▂▂▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02101
wandb:    label_loss 0.00935
wandb: test_accuracy 97.25
wandb:     test_loss 0.02409
wandb:    train_loss 0.0202
wandb: 
wandb: 🚀 View run magic-sweep-39 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/euh0ttcv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_221847-euh0ttcv/logs
wandb: Agent Starting Run: hadl892s with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.06900917774258197
wandb: 	learning_rate: 0.0063736121199117505
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_222054-hadl892s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_elu
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_elu/sweeps/hw4mpt9u
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_elu/runs/hadl892s
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██
wandb:    image_loss █▇▅▅▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    label_loss █▅▄▃▃▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂
wandb: test_accuracy ▁▅▇▇▇▇▇▇▇▇█▇██▇
wandb:     test_loss █▅▃▂▂▂▁▁▁▁▁▁▁▁▁
wandb:    train_loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.01347
wandb:    label_loss 0.01383
wandb: test_accuracy 95.34
wandb:     test_loss 0.01494
wandb:    train_loss 0.01392
wandb: 
wandb: 🚀 View run deft-sweep-40 at: https://wandb.ai/cavaokcava/auto_64_elu/runs/hadl892s
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_elu
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_222054-hadl892s/logs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: cavaokcava. Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: ynvyheyr with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4901127396787131
wandb: 	learning_rate: 0.009935660121549528
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_222245-ynvyheyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-1
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/ynvyheyr
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████
wandb:    image_loss ▇▅▄▄▄▅█▆▄▆▂▄▄▆▃▇▃▆▃▄▇█▄▅▆▅▅▄▂▅▆▁▃▅▂▃▅▂▄█
wandb:    label_loss ▅▅▅▅▄▄▅▃▆▄▅▄▅▅▅▇▅█▃▄▄▄▅▅▄▄▄▃▃▄▇▆▇▅▇▄▁▄▄▅
wandb: test_accuracy ██▂█▂███████▁██
wandb:     test_loss ▂▆▃▅▂▁▇▇▁▅▆▆█▂▇
wandb:    train_loss ▁▅▅▅▆▅▃▄▃▅▄▂▅▃▃▄▄▅▆█▅▃▆▄▆▅▅▆▃▅▅▇▄▅█▅▆▄▄▆
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06564
wandb:    label_loss 0.23056
wandb: test_accuracy 11.35
wandb:     test_loss 0.18044
wandb:    train_loss 0.18034
wandb: 
wandb: 🚀 View run skilled-sweep-1 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/ynvyheyr
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_222245-ynvyheyr/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: dnxap8l4 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1467714146259303
wandb: 	learning_rate: 0.0009623243767364584
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_222457-dnxap8l4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-2
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/dnxap8l4
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 135-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ██▆▇▆▄▅▃▃▄▄▄▄▄▄▃▂▃▃▃▃▃▂▃▃▂▂▃▃▂▂▃▂▃▃▂▂▁▃▂
wandb:    label_loss █▆▅▄▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁
wandb: test_accuracy ▁▆▇▇███████████
wandb:     test_loss █▅▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▇▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03557
wandb:    label_loss 0.00636
wandb: test_accuracy 96.02
wandb:     test_loss 0.03799
wandb:    train_loss 0.03694
wandb: 
wandb: 🚀 View run resilient-sweep-2 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/dnxap8l4
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_222457-dnxap8l4/logs
wandb: Agent Starting Run: 9febqps0 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3887210769913571
wandb: 	learning_rate: 0.009023733900833403
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_222659-9febqps0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-3
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/9febqps0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▂▂▆▅▄▅▅▁▄▇▅▅▄▁▆▂▃▅▃▅▅▄▂▁▃▄▃▄▂▆▆▃▃▄▆▃█▃▃▂
wandb:    label_loss ▄▄▇█▄▁▅▆▁▇▃▃▃▅▄▅▃▃▆▅▅▆▅▄▄▆▄▅▄▃▄▄▄▄▂▄▆▄▂▅
wandb: test_accuracy █▁█▂████▁██████
wandb:     test_loss ▂▂▃▄▄▃█▆▃▂▃█▁▄▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06891
wandb:    label_loss 0.22926
wandb: test_accuracy 11.35
wandb:     test_loss 0.15706
wandb:    train_loss 0.15705
wandb: 
wandb: 🚀 View run serene-sweep-3 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/9febqps0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_222659-9febqps0/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 07wdz6vu with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.12425726004415656
wandb: 	learning_rate: 0.0029232259083990008
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_222910-07wdz6vu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-4
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/07wdz6vu
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ███████████▇▇▆▅▅▄▄▄▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂
wandb:    label_loss █████████████▇▅▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▁▁▁▄▇█████████
wandb:     test_loss ████▆▄▃▂▂▂▂▁▁▁▁
wandb:    train_loss █▇▇▇▇▇▇▇▇▇▇▇▇▇▇▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02981
wandb:    label_loss 0.00562
wandb: test_accuracy 96.75
wandb:     test_loss 0.03043
wandb:    train_loss 0.02983
wandb: 
wandb: 🚀 View run swift-sweep-4 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/07wdz6vu
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_222910-07wdz6vu/logs
wandb: Agent Starting Run: 3y12dnpb with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.26406644295457915
wandb: 	learning_rate: 0.002216052560284775
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_223057-3y12dnpb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-5
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/3y12dnpb
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇███
wandb:    image_loss █▇▇▆▆▆▅▆▄▄▄▄▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▂▁▂
wandb:    label_loss ██▄▄▃▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▇█████████████
wandb:     test_loss █▅▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:    train_loss █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03263
wandb:    label_loss 0.00152
wandb: test_accuracy 97.51
wandb:     test_loss 0.03537
wandb:    train_loss 0.03244
wandb: 
wandb: 🚀 View run stellar-sweep-5 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/3y12dnpb
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_223057-3y12dnpb/logs
wandb: Agent Starting Run: 2tgch9j7 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.03938535616529426
wandb: 	learning_rate: 0.001484665682499837
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_223244-2tgch9j7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-6
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/2tgch9j7
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇███
wandb:    image_loss ███▆▆▅▅▄▄▃▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▂▁▁
wandb:    label_loss ██▇▆▅▄▃▂▃▂▂▂▂▁▂▂▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▇▇▇██████████
wandb:     test_loss █▆▄▃▃▃▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02506
wandb:    label_loss 0.01229
wandb: test_accuracy 95.95
wandb:     test_loss 0.02595
wandb:    train_loss 0.02582
wandb: 
wandb: 🚀 View run logical-sweep-6 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/2tgch9j7
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_223244-2tgch9j7/logs
wandb: Agent Starting Run: m31jxmb0 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.34645961721897217
wandb: 	learning_rate: 0.002020066690570314
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_223430-m31jxmb0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-7
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/m31jxmb0
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█
wandb:    image_loss █▇▆▅▆▅▄▅▅▅▅▅▅▅▄▄▃▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▂▁▁▂▁
wandb:    label_loss █▇▃▂▃▂▁▂▁▁▂▁▁▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▇▇▇███████████
wandb:     test_loss █▅▄▄▃▃▂▂▂▂▂▁▁▁▁
wandb:    train_loss █▆▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02878
wandb:    label_loss 0.00909
wandb: test_accuracy 97.5
wandb:     test_loss 0.03171
wandb:    train_loss 0.0287
wandb: 
wandb: 🚀 View run magic-sweep-7 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/m31jxmb0
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_223430-m31jxmb0/logs
wandb: Agent Starting Run: 1f159t5g with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2715463645538604
wandb: 	learning_rate: 0.005210444227991499
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_223632-1f159t5g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-8
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/1f159t5g
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████
wandb:    image_loss ▄▃▄▃▅▇▆▅▃▁▅▅▅▃▆▆▅▃▆▃▅▄▅▆▂▅█▃▅▃▃▃▄▅▄▃█▇▃▂
wandb:    label_loss ▆▆▃▄▅▅▅▅▅▅▆▄▅▆▇█▄█▁▅▄▁▄▃▃▅▄▅▄▅▆▄▅▄▄▁▄▅▄▅
wandb: test_accuracy ██████████████▁
wandb:     test_loss ▁▃█▅▃▄▄▃▃▁▃▄▅▄▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06564
wandb:    label_loss 0.23034
wandb: test_accuracy 9.74
wandb:     test_loss 0.1301
wandb:    train_loss 0.12995
wandb: 
wandb: 🚀 View run genial-sweep-8 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/1f159t5g
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_223632-1f159t5g/logs
wandb: Agent Starting Run: l8eog65z with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.34938830483075395
wandb: 	learning_rate: 0.004980460501223898
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_223818-l8eog65z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-9
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/l8eog65z
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████
wandb:    image_loss ▁█▆▄▆▅▂▅▃▂█▃▃▆▃▂▅▅▅▃▅▄▆▆▃▅▅█▇▅▄▄▄▄▂▆▃▄▂▆
wandb:    label_loss ▄▄█▅▆▆▃▅▅▆▄▆▁▆▆▆▃▆█▄▆▂▄▅▅▃▃▅▄▅▄▆▂▃▅▅▅▆▇▂
wandb: test_accuracy ▃█▃▁▂█▃█▃███▃██
wandb:     test_loss ▆▃▅▂▁▆▃▄▅▆▂▂▃█▇
wandb:    train_loss ▄▇▅█▅█▆▄▅▅▃▂▆▃▂▃▅▅▅▃▅▅▇▃▇▃▆▂▅▇▄▃▁▆▆▃▅▅▅▃
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06781
wandb:    label_loss 0.22858
wandb: test_accuracy 11.35
wandb:     test_loss 0.14826
wandb:    train_loss 0.14781
wandb: 
wandb: 🚀 View run daily-sweep-9 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/l8eog65z
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_223818-l8eog65z/logs
wandb: Agent Starting Run: m00hrrz3 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1575591916580789
wandb: 	learning_rate: 0.0016074426904034648
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_224055-m00hrrz3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-10
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/m00hrrz3
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▆▇▅▄▅▅▅▅▄▃▃▄▄▄▂▃▃▃▃▃▂▂▂▃▂▂▂▂▂▃▂▁▂▂▁▁▁▂▁
wandb:    label_loss █▄▂▂▁▂▁▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▄▅▆▇▇▇▇███████
wandb:     test_loss █▆▅▄▄▄▃▃▃▂▂▂▁▁▁
wandb:    train_loss █▇▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02388
wandb:    label_loss 0.01855
wandb: test_accuracy 97.63
wandb:     test_loss 0.02532
wandb:    train_loss 0.02479
wandb: 
wandb: 🚀 View run winter-sweep-10 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/m00hrrz3
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_224055-m00hrrz3/logs
wandb: Agent Starting Run: 3hi7i3su with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4105041619854866
wandb: 	learning_rate: 0.007434238974562293
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_224333-3hi7i3su
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-11
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/3hi7i3su
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-284, summary, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▂▄▅▅▄▅▄▅▃▇█▃▃▄▃▅▆▃▂▁▂▄▂▅▄▁▃▆▄▆▅█▅▅▅▇▅▄▃▅
wandb:    label_loss ▅▆▅▄▅▅▁▅█▃▆▆▆▆▇▅▆▅▃▆▅▅▄█▅▇▄▆▇▆▆▆▆▆▅▅▅█▂▄
wandb: test_accuracy ▄█▁███████▄█▂██
wandb:     test_loss ▄▄▄▃▆▄▃▃▁▁█▂▂▅▂
wandb:    train_loss ▁▃█▅▅▃▃▂█▃▇▄▇▆▄▃▃▄▁▃▆▄▅▄▁▆▇▇▃▃▂▆▂▄▆▄▂▇▅▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0724
wandb:    label_loss 0.22966
wandb: test_accuracy 11.35
wandb:     test_loss 0.16207
wandb:    train_loss 0.16186
wandb: 
wandb: 🚀 View run skilled-sweep-11 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/3hi7i3su
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_224333-3hi7i3su/logs
wandb: Agent Starting Run: pvpeli55 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.26275785264420337
wandb: 	learning_rate: 0.0068374081371897644
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_224610-pvpeli55
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-12
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/pvpeli55
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▂█▅▄▅█▃▁▂▄▂▅▂▅▅▄▃▇▄▃▅▃▅▂▄▂▆▃▃▄▆▄▅▂▂▃▃▃▅▃
wandb:    label_loss ▆▅▄▆▁▇▂▃▂█▆▃▅▅▄▆▂▆▄▅▄▅▆▅▁▆▅▅▄▆▂▅▃▄▄▅▅▃▂▄
wandb: test_accuracy ██████▁██▂█████
wandb:     test_loss ▁▁▂▆▂▃▂▂▅█▅▃▂▇▂
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06472
wandb:    label_loss 0.23011
wandb: test_accuracy 11.35
wandb:     test_loss 0.12803
wandb:    train_loss 0.12782
wandb: 
wandb: 🚀 View run royal-sweep-12 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/pvpeli55
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_224610-pvpeli55/logs
wandb: Agent Starting Run: m3lms2f5 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3587393739890349
wandb: 	learning_rate: 0.003831155851893937
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_224756-m3lms2f5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-13
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/m3lms2f5
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇████
wandb:    image_loss ▂▃▄▄▄▄█▃▄▄▃▅▅▄▅▃▆▄▅▃▂▄▄▃▄▄▄▅▂▇▆▄▄▁▃▄▅▆▄▃
wandb:    label_loss ▅▅▃▆█▆▄▆▅▇█▇▇▄▅▇▆▅▅▆█▄▆▆▇▁▅▄▆▄▄▇█▆▄▅▂▆▅▆
wandb: test_accuracy █▄█▁██▄██▃█▃▂█▂
wandb:     test_loss ▅█▃▅▂▂▃▁▂▇▃▄▅▁▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06541
wandb:    label_loss 0.23152
wandb: test_accuracy 9.82
wandb:     test_loss 0.15036
wandb:    train_loss 0.14984
wandb: 
wandb: 🚀 View run upbeat-sweep-13 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/m3lms2f5
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_224756-m3lms2f5/logs
wandb: Agent Starting Run: oflr4cvp with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.23357573884545524
wandb: 	learning_rate: 0.0028865786069769404
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_225002-oflr4cvp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-14
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/oflr4cvp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ██▇▆▆▆▄▄▄▄▃▃▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁
wandb:    label_loss ███▆▅▃▃▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▆▆████████████
wandb:     test_loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▇▆▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02759
wandb:    label_loss 0.0009
wandb: test_accuracy 97.42
wandb:     test_loss 0.02978
wandb:    train_loss 0.02829
wandb: 
wandb: 🚀 View run dutiful-sweep-14 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/oflr4cvp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_225002-oflr4cvp/logs
wandb: Agent Starting Run: 4oz0auz9 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3212907425480747
wandb: 	learning_rate: 0.008533359832573266
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_225149-4oz0auz9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-15
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/4oz0auz9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:    image_loss ▄▅▄▁▃▄▅▅█▅▄▅▄▂▂▅▃▃▆▅▂▄▅▂▃▇▅▅█▅▂▂▅▃▄▆▄▇▃▃
wandb:    label_loss ▄▇▆▂▅▃▆▄▄▅▅▃▅▄▂▅▂▄▅▅▄▅▃▄▃▇▃▅▅▄▄▃▁█▂▅▅▅▄▄
wandb: test_accuracy ██▁███████████▁
wandb:     test_loss ▁▂▃▃▅▄▂▃▄▄▄▅█▃▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.07008
wandb:    label_loss 0.23065
wandb: test_accuracy 10.28
wandb:     test_loss 0.14153
wandb:    train_loss 0.14111
wandb: 
wandb: 🚀 View run crisp-sweep-15 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/4oz0auz9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_225149-4oz0auz9/logs
wandb: Agent Starting Run: v86h9xu2 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.0805449415460881
wandb: 	learning_rate: 0.006092767262636422
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_225356-v86h9xu2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-16
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/v86h9xu2
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇██
wandb:    image_loss █▃▄█▄▅▁▆▆▂▄▃▆▇▆▅▅▄▅▆▅▅▅▅▅▇▆▇▄▇▂▅▄▂▅▅▂▃▆▄
wandb:    label_loss ▃▆▂▅▂▅▄▅▄▃▄▄▄▃▅▃▃▁▆▅▄▅▄▃▃▅▄▃▂▅▄▅█▃▄▄▅▃▃▄
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▂▁▃▄▃▄▃▆▅▂▁▃▄█▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06652
wandb:    label_loss 0.22999
wandb: test_accuracy 11.35
wandb:     test_loss 0.08613
wandb:    train_loss 0.08593
wandb: 
wandb: 🚀 View run fanciful-sweep-16 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/v86h9xu2
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_225356-v86h9xu2/logs
wandb: Agent Starting Run: qhqw4g73 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.49293587537769185
wandb: 	learning_rate: 0.009858944189435354
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_225542-qhqw4g73
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-17
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/qhqw4g73
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇██
wandb:    image_loss ▃▃█▂▂▁█▇▆▄▄█▄▅▃▇▇▅▆▅▅▆▂▇▇▂▂▄▂▆▁▃▄█▄▄▆▂▃▃
wandb:    label_loss ▄▄▇▅▁▆▅▆▅▅▄▇▃▆▆▅▄▆▅▄█▃▅▅▄▃▃▅▇▇▇▅▄▃▆▅▅▅▆▄
wandb: test_accuracy ▂█▁█▂▂████▁████
wandb:     test_loss ▅▅▃▅▄█▄▆▅▁█▆▅▂▄
wandb:    train_loss ▄▅▄▇▅▆▅▇▃█▆▄▁▆▂▄▇▄▅▄▄▆▇▅▄▅▄▆▅▆▆▄▅▄▇▄▅▄▅▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06819
wandb:    label_loss 0.23036
wandb: test_accuracy 11.35
wandb:     test_loss 0.18111
wandb:    train_loss 0.1809
wandb: 
wandb: 🚀 View run valiant-sweep-17 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/qhqw4g73
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_225542-qhqw4g73/logs
wandb: Agent Starting Run: npt44x2x with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.13464473093614893
wandb: 	learning_rate: 0.004525074446594865
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_225819-npt44x2x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-18
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/npt44x2x
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇██
wandb:    image_loss ▆▅▅█▆▆▃▅▃▂▇▄▇▆▅▇▅▇▇▆▁▄▅▃▅▆▅▅▂▆▅▃▄▄▆▄▄▅▅▃
wandb:    label_loss ▅▂▄▄▃▇▄▅▄▆▂▅▃▂▆▅▂█▃▃▃▃▆▅▄▁▂▆▅▇▄▁▇▃▄▂▄█▂▅
wandb: test_accuracy ▃██▃███▁█▃█████
wandb:     test_loss ▁▅▇▆▄▅▆▇▇▄▃▄▃▇█
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06522
wandb:    label_loss 0.2305
wandb: test_accuracy 11.35
wandb:     test_loss 0.09866
wandb:    train_loss 0.09844
wandb: 
wandb: 🚀 View run fancy-sweep-18 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/npt44x2x
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_225819-npt44x2x/logs
wandb: Agent Starting Run: 4qmlepwp with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.08931834791674303
wandb: 	learning_rate: 0.0010911651114019084
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_230006-4qmlepwp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-19
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/4qmlepwp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss █▇▇▆▆▅▅▅▄▄▄▃▃▃▄▃▃▃▂▃▂▂▃▂▂▂▂▂▂▁▂▁▂▂▁▁▁▂▁▁
wandb:    label_loss █▆▆▅▄▃▃▃▃▂▂▂▃▂▂▂▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▆▇██████████
wandb:     test_loss █▅▄▄▃▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▇▅▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03103
wandb:    label_loss 0.01392
wandb: test_accuracy 95.77
wandb:     test_loss 0.03346
wandb:    train_loss 0.03267
wandb: 
wandb: 🚀 View run toasty-sweep-19 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/4qmlepwp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_230006-4qmlepwp/logs
wandb: Agent Starting Run: 4xo9i5hw with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2143880595523885
wandb: 	learning_rate: 0.0022334027151836807
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_230154-4xo9i5hw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-20
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/4xo9i5hw
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 134-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ██▇▆▅▅▅▅▄▃▃▃▃▃▂▃▃▃▃▂▃▃▃▃▂▃▃▂▂▃▂▂▂▁▁▁▁▂▁▁
wandb:    label_loss █▄▄▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▅▆▇▇▇▇▇███████
wandb:     test_loss █▆▄▃▃▃▃▃▂▂▂▁▁▁▁
wandb:    train_loss █▆▅▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.02358
wandb:    label_loss 0.00035
wandb: test_accuracy 97.48
wandb:     test_loss 0.02864
wandb:    train_loss 0.02664
wandb: 
wandb: 🚀 View run dainty-sweep-20 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/4xo9i5hw
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_230154-4xo9i5hw/logs
wandb: Agent Starting Run: psrbonjf with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.32041483473124444
wandb: 	learning_rate: 0.004540307615758107
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_230400-psrbonjf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-21
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/psrbonjf
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:    image_loss ▆▇▇█▆▇▇▆▆▆▆▇▇▆▇▇▇▇█▆▇█▇▇▇▆▆▅▄▄▄▄▃▃▃▃▂▂▂▁
wandb:    label_loss ████████████████████████▇▄▃▃▂▂▁▂▂▁▁▁▁▁▁▁
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▅▇████
wandb:     test_loss █████████▄▂▂▂▁▁
wandb:    train_loss █▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▃▃▂▂▂▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03952
wandb:    label_loss 0.0151
wandb: test_accuracy 95.14
wandb:     test_loss 0.04735
wandb:    train_loss 0.0462
wandb: 
wandb: 🚀 View run desert-sweep-21 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/psrbonjf
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_230400-psrbonjf/logs
wandb: Agent Starting Run: doa743o6 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1072114886112247
wandb: 	learning_rate: 0.006187117405578926
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_230547-doa743o6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-22
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/doa743o6
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:    image_loss ▆▅▅▇▁▄▄▄▄▂▆█▅▄▅▅▄▄▇▅▇▃▅▆▄▆▃▄▅▆▆▅▆▅▆▄▄▃▇▄
wandb:    label_loss ▄▄▇▅▄▅▅▄▃▄▅▁▁▃▆▄▄▂▄▅▄▃▅▁▄▃▅▄▆▆▄▃▂▅█▂██▃▄
wandb: test_accuracy █▄█▄█▁█▄▄█▄██▄█
wandb:     test_loss ▇▁▇▅▂▂▄▃▇▆▁▃█▄▃
wandb:    train_loss ▂▂▅▆▃▇▅▅▅▅▄▇▁▄▅▃▇▄▅▆▆▄▇▆▇▆▄▄▄▅▇▆▅▆█▄▆▅▄▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06669
wandb:    label_loss 0.22794
wandb: test_accuracy 11.35
wandb:     test_loss 0.09233
wandb:    train_loss 0.09192
wandb: 
wandb: 🚀 View run hardy-sweep-22 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/doa743o6
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_230547-doa743o6/logs
wandb: Agent Starting Run: mkdupo0z with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2743586057515211
wandb: 	learning_rate: 0.0017609936071826414
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_230824-mkdupo0z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-23
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/mkdupo0z
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇████
wandb:    image_loss █▇▇▆▅▄▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▁▂▁▃▂▂▁▁▁
wandb:    label_loss █▇▅▃▃▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▇█████████████
wandb:     test_loss █▄▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▇▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03384
wandb:    label_loss 0.00138
wandb: test_accuracy 97.22
wandb:     test_loss 0.03758
wandb:    train_loss 0.03518
wandb: 
wandb: 🚀 View run snowy-sweep-23 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/mkdupo0z
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_230824-mkdupo0z/logs
wandb: Agent Starting Run: x1gl2uq1 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.08219072568792862
wandb: 	learning_rate: 0.007309497377038236
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_231010-x1gl2uq1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-24
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/x1gl2uq1
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 134-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▄▁▂▅▅▅▆▆▂▁▄▄▆▂▅▆▄▄▄▆▅▆▂▃▃▄▆▄█▇▅▅▇▃▅▄▇▅▅▃
wandb:    label_loss ▅▆▅▆▇▄▄▄▄▆▄▅▆▄▅▄▄▃▄█▄▆▅▃▃▁▃▄▄▄▆▁▅▄▆▅▄▄▄▅
wandb: test_accuracy ▁████████▁████▁
wandb:     test_loss ▄▅█▅▅▆▆▇▂▇▄▄▄▁▁
wandb:    train_loss ▆▅▄▄▆▄▇▅▅▄▅▆▅▃▅▅▃▃▄▅▆▃▅▄▂▄▆▅▄▇▅█▇▃▇▄▁▄▆▄
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06425
wandb:    label_loss 0.23062
wandb: test_accuracy 10.28
wandb:     test_loss 0.08644
wandb:    train_loss 0.0861
wandb: 
wandb: 🚀 View run rosy-sweep-24 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/x1gl2uq1
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_231010-x1gl2uq1/logs
wandb: Agent Starting Run: 2ri7ntyf with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.07113948474364257
wandb: 	learning_rate: 0.004449099415568714
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_231217-2ri7ntyf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-25
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/2ri7ntyf
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▃▅▁▄▇▅▆▆▅▄▄▅▆▅▅▇▅▇▇▂▆▅▇▄▇▇▆█▇▁▆▆▇▆▃▆▆▆▅▅
wandb:    label_loss ▃▅▁▅▄▄▆▁▆▆█▅▆▄▅▅▄▅▅▅▇█▆█▄▄▄█▄▆▃▆▆▆▆▅▅▅▃▇
wandb: test_accuracy ▄████████▁█▄███
wandb:     test_loss ▁▃▆▅▅▅▅▅▇▇▄▅▂█▂
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06726
wandb:    label_loss 0.23064
wandb: test_accuracy 11.35
wandb:     test_loss 0.08395
wandb:    train_loss 0.08391
wandb: 
wandb: 🚀 View run olive-sweep-25 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/2ri7ntyf
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_231217-2ri7ntyf/logs
wandb: Agent Starting Run: 24uwou3e with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4255211588656007
wandb: 	learning_rate: 0.003932695406033877
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_231403-24uwou3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-26
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/24uwou3e
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███
wandb:    image_loss ▃▆▅▂▄▃▄▃▅▅▁▅▂▆▄▄█▅▂▅▇▅▃▃▁▆▂▇▇▆▇▃▅█▃▇▃▆▅▅
wandb:    label_loss ▅▆▃▃▇▁▄▂▅▇▅▅▄▅▆▆▆█▅▆▄▆▅▄▂▁▄▄▄▃▄▅▇▇▆▅▃▃▅▄
wandb: test_accuracy ▁██████▁▁██████
wandb:     test_loss ▃▄▃▃▁▃▃▆▆█▅▃▄▇▄
wandb:    train_loss ▂▄▄▆▆▄▆▄▂▃▅▄▄▅▁▅▇▆▃▂▄▅▅▆▄▃▄▄▅▇█▄▆▅▄▇▄▃▅▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06753
wandb:    label_loss 0.22999
wandb: test_accuracy 11.35
wandb:     test_loss 0.16568
wandb:    train_loss 0.16544
wandb: 
wandb: 🚀 View run ethereal-sweep-26 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/24uwou3e
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_231403-24uwou3e/logs
wandb: Agent Starting Run: 57ralsxp with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2006642291495464
wandb: 	learning_rate: 0.009815805501425247
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_231550-57ralsxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-27
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/57ralsxp
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 133-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▇▃▁▃▅▆▅▃▂▃▄▂▃▃▄▂▅▄██▄▁▄▆▅▃▆▃▇▅▁▃▅▅▅▆▆▆▆▃
wandb:    label_loss ▆▆▇▇▃▆▅▆▇▄▆▂▃▆██▆▆▃▁▅▅▇▅▆▇█▆▅▅▅▇▆▇▆▇▅▇█▅
wandb: test_accuracy █▁█████████████
wandb:     test_loss ▆▁▄▅▆▁▅▄█▇▇▁▅▃▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06574
wandb:    label_loss 0.23025
wandb: test_accuracy 11.35
wandb:     test_loss 0.11382
wandb:    train_loss 0.11347
wandb: 
wandb: 🚀 View run hearty-sweep-27 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/57ralsxp
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_231550-57ralsxp/logs
wandb: Agent Starting Run: 9tieqmsg with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.16514922085529843
wandb: 	learning_rate: 0.0070781776004907185
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_231756-9tieqmsg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-28
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/9tieqmsg
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▄▅▅▆▄▃▄▆▄▄█▄▆▃▆▆▅▆▄▇▃▅▅▇▄▆▆▄▄▅▅▄▅▆▆▄▄█▅▁
wandb:    label_loss ▁▃▅▂▁▄▂▃▃▄▄▃▂▂▄▇▃█▆▃▃▃▅▃▄▆▄▆▂▆▅▅▄▄▂▂▇▄▃▄
wandb: test_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     test_loss ▁▃▃▃▃▆▅▁▁█▁▃▆▄▄
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06212
wandb:    label_loss 0.2304
wandb: test_accuracy 11.35
wandb:     test_loss 0.10559
wandb:    train_loss 0.10502
wandb: 
wandb: 🚀 View run silvery-sweep-28 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/9tieqmsg
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_231756-9tieqmsg/logs
wandb: Agent Starting Run: mxxso9m8 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.23337038162655688
wandb: 	learning_rate: 0.00499027321224753
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_231943-mxxso9m8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-29
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/mxxso9m8
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▄▃▃▄▅▆▃▅▅▇▆▅▇▄▅▃▅█▂▄█▅▆▄▅▄▆▆▅▃▄▃▆▁▇▄█▂▃▅
wandb:    label_loss █▃▇▆▅▅▄▆▁▄▇▇▄▃▄▇▅▆▅▇▆▄▅▆█▇█▄▃█▃▃▃▃█▃▄▆▅▅
wandb: test_accuracy ▃▁▂███▃████▁███
wandb:     test_loss ▄▅▄▄▅▂▁▄▃▅▆▂▅█▃
wandb:    train_loss ▃▆▆▅▅▅▄▅▃▄▄▃▄▄▆█▃▃▇▅▄▆▆▃▇▂▃▁▁█▅▂▃▄▃▂▄▃▃▂
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0676
wandb:    label_loss 0.23104
wandb: test_accuracy 11.35
wandb:     test_loss 0.12153
wandb:    train_loss 0.12114
wandb: 
wandb: 🚀 View run hardy-sweep-29 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/mxxso9m8
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_231943-mxxso9m8/logs
wandb: Agent Starting Run: 0n1ms33z with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.3745606977837887
wandb: 	learning_rate: 0.0047016588369006495
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_232220-0n1ms33z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-30
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/0n1ms33z
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇███
wandb:    image_loss ▃▃▅▆▅▅▅▂▃▇▁▂▃▃▄▁▇▅▃▃▁▄▂▂▅▂▅▄▃█▅▄▂▆▃▅▄▄▄▆
wandb:    label_loss ▄▆▅▆▅▆▂▃▄▃▆▅▆▅▇▅▂▅█▃▂▅▂▇▅▄▅▁▄▄▆▃▅▅▅▃▅▅▅▆
wandb: test_accuracy █▁███▃█████████
wandb:     test_loss ▃▅█▁▃▂▃▁▁▅█▃▃▂▂
wandb:    train_loss ▃▆▅▂▅▂▄▆▆███▅▄▄▄▅▅▆▅▇▄▂▁▆▅▇▇▆▄▄▄▅▃▄▆▅▄▄▅
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0684
wandb:    label_loss 0.23113
wandb: test_accuracy 11.35
wandb:     test_loss 0.15387
wandb:    train_loss 0.15364
wandb: 
wandb: 🚀 View run exalted-sweep-30 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/0n1ms33z
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_232220-0n1ms33z/logs
wandb: Agent Starting Run: yn5xt4xv with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.11493089661456002
wandb: 	learning_rate: 0.005902054903653949
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_232427-yn5xt4xv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-31
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/yn5xt4xv
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▅▆▆▅▃█▄▆▇▁▄▅▃▄▆▅▇▅▂▅▄▃▆▄▄▅▆▆▆▃▇▃▄▅▁▃▄▅▅█
wandb:    label_loss ▆▅▇▄▄▆▆▃▂▃▂▁▅▄▄▆▄█▆▅▄▄▅▃▅▂▆▃▆▃▆▄▅▆▅▅▆▇▇▅
wandb: test_accuracy █▁█████▂████▂▁█
wandb:     test_loss ▆▆▃▅▅▆▂▇█▃▂▃▁▅▄
wandb:    train_loss ▄▄▆▃▅▅▄▆▄▆▄▆▆▇▆▄▄▆█▆▁▅▃▃▇▅▇▄▅▆▄▄▇▅▆▆▄█▅▆
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06786
wandb:    label_loss 0.23243
wandb: test_accuracy 11.35
wandb:     test_loss 0.09412
wandb:    train_loss 0.0941
wandb: 
wandb: 🚀 View run bright-sweep-31 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/yn5xt4xv
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_232427-yn5xt4xv/logs
wandb: Agent Starting Run: dzuprj8m with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.18847384917087495
wandb: 	learning_rate: 0.00907695959714349
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_232706-dzuprj8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-32
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/dzuprj8m
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 257-284, console lines 205-234
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██
wandb:    image_loss ▇▄▄▃▆▅▂▄▄▅▅▁▂▂▁▅▃▂▅▄█▄▅▃▆▄▃▃▆▄█▃▃▇▄▃▇▄▇▂
wandb:    label_loss ▅▇▁▅▄▂▆▁▅▅▆▂▆▆▆▆▆▃▅▄▂▃▅▅▁▄▆█▆▃▄▂▂▄▄▁▆▂▆▅
wandb: test_accuracy ███▁▁█████▁███▁
wandb:     test_loss ▇▃█▅▅▂▇▇▂▁▂▅▃▂▄
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06445
wandb:    label_loss 0.2327
wandb: test_accuracy 10.28
wandb:     test_loss 0.11099
wandb:    train_loss 0.11052
wandb: 
wandb: 🚀 View run vibrant-sweep-32 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/dzuprj8m
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_232706-dzuprj8m/logs
wandb: Agent Starting Run: gd8n96vk with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.4212708170176388
wandb: 	learning_rate: 0.0011800685804406231
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_232943-gd8n96vk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-33
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/gd8n96vk
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb: uploading history steps 135-149, summary, console lines 140-159
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇██
wandb:    image_loss █▇▆▅▄▄▄▅▃▄▃▄▃▃▃▃▃▃▂▃▃▃▂▂▂▂▃▂▃▂▃▁▂▂▂▃▁▂▂▂
wandb:    label_loss █▆▅▁▃▂▂▂▃▁▁▂▂▁▁▁▂▁▁▁▂▁▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▆▇▇▇▇█████████
wandb:     test_loss █▅▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:    train_loss █▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.03473
wandb:    label_loss 0.00357
wandb: test_accuracy 96.9
wandb:     test_loss 0.04252
wandb:    train_loss 0.0398
wandb: 
wandb: 🚀 View run major-sweep-33 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/gd8n96vk
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_232943-gd8n96vk/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9cgo8qr9 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.38861532922866177
wandb: 	learning_rate: 0.009419660088789104
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_233155-9cgo8qr9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-34
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/9cgo8qr9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▄▂▂▆▂▅▄▃▁▇▄▄▄▄▆▄▆▆█▁▄▃▃▆▃▁█▄▆▆▇▅▅▅▅▅▃▅▄▃
wandb:    label_loss ▂▃▄▃▇▅▆▄▆▇▅▃▄▄▇▅▆▇▅▆█▃▇▅▇▆▁▄▅▄▆▄▆▄▄▃▄▆▆▅
wandb: test_accuracy ▁█▁█▁██████████
wandb:     test_loss ▂▂▃▁█▂▄▂▅▂▆▄▄▄▁
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06917
wandb:    label_loss 0.23067
wandb: test_accuracy 11.35
wandb:     test_loss 0.15701
wandb:    train_loss 0.15654
wandb: 
wandb: 🚀 View run deep-sweep-34 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/9cgo8qr9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_233155-9cgo8qr9/logs
wandb: Agent Starting Run: o8axilu9 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.11885548730968908
wandb: 	learning_rate: 0.005379923660302505
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_233401-o8axilu9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-35
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/o8axilu9
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇███
wandb:    image_loss ▆▅▅▄▅▆▇▆▃▅▄▄▆▇▄▄▃▅▅▄▆▆▆█▆█▆▅▆█▆▁▄▆▅▆▅▅▄▆
wandb:    label_loss ▅▅▃▂▅██▆▃▆▂▆█▃█▆▃▆▆▃▂▂▄▄▅▇▇▃▃▇▄▅▃▅▅▄▁▂▄▅
wandb: test_accuracy ████▃███▃███▁█▂
wandb:     test_loss ▂▃▃▂▃▄█▂▃▄▂▁▄▄▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06824
wandb:    label_loss 0.23063
wandb: test_accuracy 10.09
wandb:     test_loss 0.09499
wandb:    train_loss 0.0948
wandb: 
wandb: 🚀 View run dashing-sweep-35 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/o8axilu9
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_233401-o8axilu9/logs
wandb: Agent Starting Run: rj0adr9j with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.2625808626066751
wandb: 	learning_rate: 0.006375739764180648
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_233548-rj0adr9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-36
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/rj0adr9j
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:    image_loss ▂▆█▃▁▄▆▅█▅▅▇▆▂▆▄▅▆▄▆▃▃▃▄▆▄▆▄▂▅▅▅▃▄▂▄▇▄▃▂
wandb:    label_loss ▇▄▅█▅▆▅▆▆▄▅▅▆▅▄▄▃▆▅▆▅▂█▆▃▆█▂▅▆█▅▃▄▁▆▆▄▆▆
wandb: test_accuracy ███████████▁███
wandb:     test_loss ▃▅▂▃▂▃▄▁▄▃█▄▆▄▅
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06491
wandb:    label_loss 0.23054
wandb: test_accuracy 11.35
wandb:     test_loss 0.12801
wandb:    train_loss 0.12786
wandb: 
wandb: 🚀 View run sunny-sweep-36 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/rj0adr9j
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_233548-rj0adr9j/logs
wandb: Agent Starting Run: 7y00qgn7 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.1862195769469275
wandb: 	learning_rate: 0.003615878901015253
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_233734-7y00qgn7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-37
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/7y00qgn7
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█
wandb:    image_loss ▄▂▅▆▄▂▃▅▄▄▄▃▆▄▆▅▃▃▅▅▆▇▂▇█▄▄▃▅▂▇▁▆▅▂▅▅▆▃█
wandb:    label_loss ▆▇▇▇▅▆▃▅▁▂▆▄▆▆▆▆▇▅▆▃▃▄▄▅▄█▆▅▅▆▃▆▅▆▃▅▂▆▆▄
wandb: test_accuracy ▁█▂████▂▃███▃▂▂
wandb:     test_loss ▄▃▁▆▃▂▂▄▆▅▅▅▂▃█
wandb:    train_loss ▁▆▇▆█▆▅▇▅▄▅▆▆▆█▅▆▄▅▆▄▄▄▄▇▄▄▄▃█▇▃▇▅▅▇▆▅▄▆
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.0707
wandb:    label_loss 0.23013
wandb: test_accuracy 10.1
wandb:     test_loss 0.11067
wandb:    train_loss 0.11031
wandb: 
wandb: 🚀 View run good-sweep-37 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/7y00qgn7
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_233734-7y00qgn7/logs
wandb: Agent Starting Run: jgdgir9g with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.43798434016706506
wandb: 	learning_rate: 0.007160250870401927
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_233921-jgdgir9g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-38
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/jgdgir9g
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███
wandb:    image_loss ▆▅▄█▆▇▁▆▆▆▇▆▄▅▄▃▅▄▇█▃▃▆▄▄▃█▆▅▆▆▅▄▆▅▄▇▄▄▇
wandb:    label_loss ▆▄▃▂▁▅▃▅▄▅▁▆▅▄▇▂▆▄▇▆▃▅▅▄▄▆▅▅▅▃▄▄▄▃█▄▆▄▅▅
wandb: test_accuracy ████▂████████▁█
wandb:     test_loss ▃▃▃▄▆▆▄▁▆█▃▇▅▄▅
wandb:    train_loss ▄▃█▆▃▃▃▇▆▆▅▄▁▆▂▅▅▄▅▂▄█▄▂▃▄▄▃▆▃▂▃▆▄▆▃▅▅▃▇
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06464
wandb:    label_loss 0.23045
wandb: test_accuracy 11.35
wandb:     test_loss 0.16843
wandb:    train_loss 0.16791
wandb: 
wandb: 🚀 View run robust-sweep-38 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/jgdgir9g
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_233921-jgdgir9g/logs
wandb: Agent Starting Run: dxtz573i with config:
wandb: 	batch_size: 128
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.24966784906749623
wandb: 	learning_rate: 0.00978526995965662
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_234158-dxtz573i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-39
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/dxtz573i
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█
wandb:    image_loss ▄▇█▇▆▆▃▆▆▄▅▄▆▅▆▆▇▇▇▆▂▇█▇█▇▇▃▅▁▅▆▆▄▆█▆▄▆▄
wandb:    label_loss █▅▄▅▄▅▄▆▄▄▅▁▃▄▄▄▄▅▄▇▄▄▅▄▄▅▄▂▄▄▄▃▃▄▂▄▆▅▄▂
wandb: test_accuracy ████████████▁██
wandb:     test_loss ▅▄▁▂▃█▄▃▄▄▃▄▅▄▃
wandb:    train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.06628
wandb:    label_loss 0.2293
wandb: test_accuracy 11.35
wandb:     test_loss 0.12504
wandb:    train_loss 0.12476
wandb: 
wandb: 🚀 View run floral-sweep-39 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/dxtz573i
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_234158-dxtz573i/logs
wandb: Agent Starting Run: 30fmmdf5 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 15
wandb: 	lambda_weight: 0.46183714895669975
wandb: 	learning_rate: 0.0022004575409605705
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/okcava/projects/universal_advex/sweeps/wandb/run-20241230_234344-30fmmdf5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-40
wandb: ⭐️ View project at https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: 🧹 View sweep at https://wandb.ai/cavaokcava/auto_64_sigmoid/sweeps/ka9v245t
wandb: 🚀 View run at https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/30fmmdf5
/home/okcava/projects/universal_advex/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         epoch ▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇█
wandb:    image_loss ▇▇██▇▇▆▇▆▅▇▆▆▅▅▄▄▅▄▅▄▃▃▃▃▃▃▃▃▂▃▃▂▂▂▁▁▂▂▂
wandb:    label_loss ██▆▄▄▄▃▃▂▂▃▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb: test_accuracy ▁▃▇████████████
wandb:     test_loss █▆▄▃▃▂▂▂▂▂▁▁▁▁▁
wandb:    train_loss █▇▇▇▇▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:         epoch 14
wandb:    image_loss 0.032
wandb:    label_loss 0.00044
wandb: test_accuracy 97.89
wandb:     test_loss 0.03198
wandb:    train_loss 0.02905
wandb: 
wandb: 🚀 View run amber-sweep-40 at: https://wandb.ai/cavaokcava/auto_64_sigmoid/runs/30fmmdf5
wandb: ⭐️ View project at: https://wandb.ai/cavaokcava/auto_64_sigmoid
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241230_234344-30fmmdf5/logs
