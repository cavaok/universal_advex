
Training model with 1 iterations...

Initial magnitudes:
f1 weight magnitude: 0.017764167860150337
f2 weight magnitude: 0.017755938693881035

First forward pass magnitudes:
Input magnitude: 0.12708063423633575
f1 output magnitude: 0.15027926862239838
f2 output magnitude: 0.14800962805747986
hadamard output magnitude: 0.02305513620376587

Iteration 1 state magnitude: 0.02305513620376587
Iteration 1 loss: 0.16147395968437195

Gradient magnitudes:
f1 weight gradients: 7.086827281455044e-06
f2 weight gradients: 7.235578323161462e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.01774243265390396
f2 weight magnitude: 0.017733417451381683
Epoch [1/15], Batch [0/469], Loss: 0.1615
Epoch [1/15], Batch [100/469], Loss: 0.0189
Epoch [1/15], Batch [200/469], Loss: 0.0158
Epoch [1/15], Batch [300/469], Loss: 0.0119
Epoch [1/15], Batch [400/469], Loss: 0.0103
Epoch [1/15], Loss: 0.0203
Epoch [2/15], Batch [0/469], Loss: 0.0091
Epoch [2/15], Batch [100/469], Loss: 0.0079
Epoch [2/15], Batch [200/469], Loss: 0.0094
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_1.pth and f2_hadamard_1.pth

Training model with 2 iterations...

Initial magnitudes:
f1 weight magnitude: 0.01773148961365223
f2 weight magnitude: 0.0177380982786417

First forward pass magnitudes:
Input magnitude: 0.13307587802410126
f1 output magnitude: 0.15328021347522736
f2 output magnitude: 0.15174511075019836
hadamard output magnitude: 0.02391991578042507

Iteration 1 state magnitude: 0.02391991578042507
Iteration 1 loss: 0.1667155772447586

Iteration 2 state magnitude: 0.0006258274079300463
Iteration 2 loss: 0.16524651646614075

Gradient magnitudes:
f1 weight gradients: 8.049581992963795e-06
f2 weight gradients: 7.4806785050896e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017718490213155746
f2 weight magnitude: 0.017724014818668365
Epoch [1/15], Batch [0/469], Loss: 0.3320
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_2.pth and f2_hadamard_2.pth

Training model with 3 iterations...

Initial magnitudes:
f1 weight magnitude: 0.017763616517186165
f2 weight magnitude: 0.017734240740537643

First forward pass magnitudes:
Input magnitude: 0.12973089516162872
f1 output magnitude: 0.14799456298351288
f2 output magnitude: 0.14929534494876862
hadamard output magnitude: 0.022898122668266296

Iteration 1 state magnitude: 0.022898122668266296
Iteration 1 loss: 0.1648532897233963

Iteration 2 state magnitude: 0.0006175173912197351
Iteration 2 loss: 0.1626526564359665

Iteration 3 state magnitude: 0.00033867027377709746
Iteration 3 loss: 0.16264981031417847

Gradient magnitudes:
f1 weight gradients: 7.145088147808565e-06
f2 weight gradients: 7.17776811143267e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017750322818756104
f2 weight magnitude: 0.01771882176399231
Epoch [1/15], Batch [0/469], Loss: 0.4902
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_3.pth and f2_hadamard_3.pth

Training model with 4 iterations...

Initial magnitudes:
f1 weight magnitude: 0.0177511777728796
f2 weight magnitude: 0.017746971920132637

First forward pass magnitudes:
Input magnitude: 0.1291627287864685
f1 output magnitude: 0.14913953840732574
f2 output magnitude: 0.1497276872396469
hadamard output magnitude: 0.02300116792321205

Iteration 1 state magnitude: 0.02300116792321205
Iteration 1 loss: 0.16321441531181335

Iteration 2 state magnitude: 0.0005966036696918309
Iteration 2 loss: 0.1619933694601059

Iteration 3 state magnitude: 0.00032279733568429947
Iteration 3 loss: 0.1619950383901596

Iteration 4 state magnitude: 0.00032272134558297694
Iteration 4 loss: 0.16199511289596558

Gradient magnitudes:
f1 weight gradients: 6.659990503976587e-06
f2 weight gradients: 6.866919648018666e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017738088965415955
f2 weight magnitude: 0.01773473061621189
Epoch [1/15], Batch [0/469], Loss: 0.6492
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_4.pth and f2_hadamard_4.pth

Training model with 5 iterations...

Initial magnitudes:
f1 weight magnitude: 0.01774577796459198
f2 weight magnitude: 0.01775934360921383

First forward pass magnitudes:
Input magnitude: 0.12948450446128845
f1 output magnitude: 0.14925378561019897
f2 output magnitude: 0.15078529715538025
hadamard output magnitude: 0.02326594851911068

Iteration 1 state magnitude: 0.02326594851911068
Iteration 1 loss: 0.16342146694660187

Iteration 2 state magnitude: 0.0006135505391284823
Iteration 2 loss: 0.16216343641281128

Iteration 3 state magnitude: 0.000325868051731959
Iteration 3 loss: 0.16216620802879333

Iteration 4 state magnitude: 0.0003257931093685329
Iteration 4 loss: 0.16216610372066498

Iteration 5 state magnitude: 0.0003257899370510131
Iteration 5 loss: 0.16216610372066498

Gradient magnitudes:
f1 weight gradients: 7.3546998464735225e-06
f2 weight gradients: 6.852561909909127e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017732786014676094
f2 weight magnitude: 0.01774231344461441
Epoch [1/15], Batch [0/469], Loss: 0.8121
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_5.pth and f2_hadamard_5.pth

Training model with 6 iterations...

Initial magnitudes:
f1 weight magnitude: 0.01773202046751976
f2 weight magnitude: 0.0177414882928133

First forward pass magnitudes:
Input magnitude: 0.12409596145153046
f1 output magnitude: 0.14812617003917694
f2 output magnitude: 0.1442839652299881
hadamard output magnitude: 0.02210795134305954

Iteration 1 state magnitude: 0.02210795134305954
Iteration 1 loss: 0.1577167809009552

Iteration 2 state magnitude: 0.0005789365386590362
Iteration 2 loss: 0.15651729702949524

Iteration 3 state magnitude: 0.0003152381395921111
Iteration 3 loss: 0.1565137803554535

Iteration 4 state magnitude: 0.00031467192457057536
Iteration 4 loss: 0.15651379525661469

Iteration 5 state magnitude: 0.0003146735834889114
Iteration 5 loss: 0.15651379525661469

Iteration 6 state magnitude: 0.00031467355438508093
Iteration 6 loss: 0.15651379525661469

Gradient magnitudes:
f1 weight gradients: 5.859003067598678e-06
f2 weight gradients: 6.103540727053769e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017715204507112503
f2 weight magnitude: 0.017724178731441498
Epoch [1/15], Batch [0/469], Loss: 0.9403
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_6.pth and f2_hadamard_6.pth

Training model with 7 iterations...

Initial magnitudes:
f1 weight magnitude: 0.017756715416908264
f2 weight magnitude: 0.01771310344338417

First forward pass magnitudes:
Input magnitude: 0.12904955446720123
f1 output magnitude: 0.15022428333759308
f2 output magnitude: 0.15249739587306976
hadamard output magnitude: 0.023564012721180916

Iteration 1 state magnitude: 0.023564012721180916
Iteration 1 loss: 0.16327786445617676

Iteration 2 state magnitude: 0.0006192567525431514
Iteration 2 loss: 0.1616165041923523

Iteration 3 state magnitude: 0.00031292898347601295
Iteration 3 loss: 0.1616079956293106

Iteration 4 state magnitude: 0.0003124782524537295
Iteration 4 loss: 0.16160807013511658

Iteration 5 state magnitude: 0.00031247592414729297
Iteration 5 loss: 0.16160808503627777

Iteration 6 state magnitude: 0.0003124759823549539
Iteration 6 loss: 0.16160808503627777

Iteration 7 state magnitude: 0.00031247595325112343
Iteration 7 loss: 0.16160808503627777

Gradient magnitudes:
f1 weight gradients: 7.743031346763019e-06
f2 weight gradients: 7.1275699156103656e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017741374671459198
f2 weight magnitude: 0.017695236951112747
Epoch [1/15], Batch [0/469], Loss: 1.1329
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_7.pth and f2_hadamard_7.pth

Training model with 8 iterations...

Initial magnitudes:
f1 weight magnitude: 0.017739485949277878
f2 weight magnitude: 0.01774994283914566

First forward pass magnitudes:
Input magnitude: 0.12438701838254929
f1 output magnitude: 0.14647585153579712
f2 output magnitude: 0.1503428816795349
hadamard output magnitude: 0.022704269737005234

Iteration 1 state magnitude: 0.022704269737005234
Iteration 1 loss: 0.1584165096282959

Iteration 2 state magnitude: 0.0005913747590966523
Iteration 2 loss: 0.1568477898836136

Iteration 3 state magnitude: 0.00033644511131569743
Iteration 3 loss: 0.15684524178504944

Iteration 4 state magnitude: 0.0003363366995472461
Iteration 4 loss: 0.15684495866298676

Iteration 5 state magnitude: 0.00033633256680332124
Iteration 5 loss: 0.15684495866298676

Iteration 6 state magnitude: 0.00033633256680332124
Iteration 6 loss: 0.15684495866298676

Iteration 7 state magnitude: 0.00033633256680332124
Iteration 7 loss: 0.15684495866298676

Iteration 8 state magnitude: 0.00033633256680332124
Iteration 8 loss: 0.15684495866298676

Gradient magnitudes:
f1 weight gradients: 6.586340077774366e-06
f2 weight gradients: 6.078119895391865e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.01772371679544449
f2 weight magnitude: 0.017734328284859657
Epoch [1/15], Batch [0/469], Loss: 1.2563
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_8.pth and f2_hadamard_8.pth

Training model with 9 iterations...

Initial magnitudes:
f1 weight magnitude: 0.017741132527589798
f2 weight magnitude: 0.017739705741405487

First forward pass magnitudes:
Input magnitude: 0.1309979110956192
f1 output magnitude: 0.15330234169960022
f2 output magnitude: 0.1484978049993515
hadamard output magnitude: 0.023476986214518547

Iteration 1 state magnitude: 0.023476986214518547
Iteration 1 loss: 0.1642705798149109

Iteration 2 state magnitude: 0.0005948560428805649
Iteration 2 loss: 0.163070410490036

Iteration 3 state magnitude: 0.0003238391946069896
Iteration 3 loss: 0.16307435929775238

Iteration 4 state magnitude: 0.00032374745933339
Iteration 4 loss: 0.16307432949543

Iteration 5 state magnitude: 0.000323751475661993
Iteration 5 loss: 0.16307432949543

Iteration 6 state magnitude: 0.000323751475661993
Iteration 6 loss: 0.16307432949543

Iteration 7 state magnitude: 0.000323751475661993
Iteration 7 loss: 0.16307432949543

Iteration 8 state magnitude: 0.000323751475661993
Iteration 8 loss: 0.16307432949543

Iteration 9 state magnitude: 0.000323751475661993
Iteration 9 loss: 0.16307432949543

Gradient magnitudes:
f1 weight gradients: 6.872924586787121e-06
f2 weight gradients: 7.441735760949086e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017726823687553406
f2 weight magnitude: 0.017724882811307907
Epoch [1/15], Batch [0/469], Loss: 1.4689
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_9.pth and f2_hadamard_9.pth

Training model with 10 iterations...

Initial magnitudes:
f1 weight magnitude: 0.017758332192897797
f2 weight magnitude: 0.01772540621459484

First forward pass magnitudes:
Input magnitude: 0.12787160277366638
f1 output magnitude: 0.14628244936466217
f2 output magnitude: 0.1491827368736267
hadamard output magnitude: 0.022510571405291557

Iteration 1 state magnitude: 0.022510571405291557
Iteration 1 loss: 0.16213983297348022

Iteration 2 state magnitude: 0.0005661534378305078
Iteration 2 loss: 0.16054406762123108

Iteration 3 state magnitude: 0.00029799374169670045
Iteration 3 loss: 0.16055478155612946

Iteration 4 state magnitude: 0.000298095284961164
Iteration 4 loss: 0.16055473685264587

Iteration 5 state magnitude: 0.0002980979916173965
Iteration 5 loss: 0.16055473685264587

Iteration 6 state magnitude: 0.0002980979916173965
Iteration 6 loss: 0.16055473685264587

Iteration 7 state magnitude: 0.0002980979916173965
Iteration 7 loss: 0.16055473685264587

Iteration 8 state magnitude: 0.0002980979916173965
Iteration 8 loss: 0.16055473685264587

Iteration 9 state magnitude: 0.0002980979916173965
Iteration 9 loss: 0.16055473685264587

Iteration 10 state magnitude: 0.0002980979916173965
Iteration 10 loss: 0.16055473685264587

Gradient magnitudes:
f1 weight gradients: 6.60645628158818e-06
f2 weight gradients: 6.701697202515788e-06

Post-update weight magnitudes:
f1 weight magnitude: 0.017743300646543503
f2 weight magnitude: 0.017709627747535706
Epoch [1/15], Batch [0/469], Loss: 1.6071
Epoch [1/15], Batch [100/469], Loss: nan
Epoch [1/15], Batch [200/469], Loss: nan
Epoch [1/15], Batch [300/469], Loss: nan
Epoch [1/15], Batch [400/469], Loss: nan
Epoch [1/15], Loss: nan
Epoch [2/15], Batch [0/469], Loss: nan
Epoch [2/15], Batch [100/469], Loss: nan
Epoch [2/15], Batch [200/469], Loss: nan
Epoch [2/15], Batch [300/469], Loss: nan
Epoch [2/15], Batch [400/469], Loss: nan
Epoch [2/15], Loss: nan
Epoch [3/15], Batch [0/469], Loss: nan
Epoch [3/15], Batch [100/469], Loss: nan
Epoch [3/15], Batch [200/469], Loss: nan
Epoch [3/15], Batch [300/469], Loss: nan
Epoch [3/15], Batch [400/469], Loss: nan
Epoch [3/15], Loss: nan
Epoch [4/15], Batch [0/469], Loss: nan
Epoch [4/15], Batch [100/469], Loss: nan
Epoch [4/15], Batch [200/469], Loss: nan
Epoch [4/15], Batch [300/469], Loss: nan
Epoch [4/15], Batch [400/469], Loss: nan
Epoch [4/15], Loss: nan
Epoch [5/15], Batch [0/469], Loss: nan
Epoch [5/15], Batch [100/469], Loss: nan
Epoch [5/15], Batch [200/469], Loss: nan
Epoch [5/15], Batch [300/469], Loss: nan
Epoch [5/15], Batch [400/469], Loss: nan
Epoch [5/15], Loss: nan
Epoch [6/15], Batch [0/469], Loss: nan
Epoch [6/15], Batch [100/469], Loss: nan
Epoch [6/15], Batch [200/469], Loss: nan
Epoch [6/15], Batch [300/469], Loss: nan
Epoch [6/15], Batch [400/469], Loss: nan
Epoch [6/15], Loss: nan
Epoch [7/15], Batch [0/469], Loss: nan
Epoch [7/15], Batch [100/469], Loss: nan
Epoch [7/15], Batch [200/469], Loss: nan
Epoch [7/15], Batch [300/469], Loss: nan
Epoch [7/15], Batch [400/469], Loss: nan
Epoch [7/15], Loss: nan
Epoch [8/15], Batch [0/469], Loss: nan
Epoch [8/15], Batch [100/469], Loss: nan
Epoch [8/15], Batch [200/469], Loss: nan
Epoch [8/15], Batch [300/469], Loss: nan
Epoch [8/15], Batch [400/469], Loss: nan
Epoch [8/15], Loss: nan
Epoch [9/15], Batch [0/469], Loss: nan
Epoch [9/15], Batch [100/469], Loss: nan
Epoch [9/15], Batch [200/469], Loss: nan
Epoch [9/15], Batch [300/469], Loss: nan
Epoch [9/15], Batch [400/469], Loss: nan
Epoch [9/15], Loss: nan
Epoch [10/15], Batch [0/469], Loss: nan
Epoch [10/15], Batch [100/469], Loss: nan
Epoch [10/15], Batch [200/469], Loss: nan
Epoch [10/15], Batch [300/469], Loss: nan
Epoch [10/15], Batch [400/469], Loss: nan
Epoch [10/15], Loss: nan
Epoch [11/15], Batch [0/469], Loss: nan
Epoch [11/15], Batch [100/469], Loss: nan
Epoch [11/15], Batch [200/469], Loss: nan
Epoch [11/15], Batch [300/469], Loss: nan
Epoch [11/15], Batch [400/469], Loss: nan
Epoch [11/15], Loss: nan
Epoch [12/15], Batch [0/469], Loss: nan
Epoch [12/15], Batch [100/469], Loss: nan
Epoch [12/15], Batch [200/469], Loss: nan
Epoch [12/15], Batch [300/469], Loss: nan
Epoch [12/15], Batch [400/469], Loss: nan
Epoch [12/15], Loss: nan
Epoch [13/15], Batch [0/469], Loss: nan
Epoch [13/15], Batch [100/469], Loss: nan
Epoch [13/15], Batch [200/469], Loss: nan
Epoch [13/15], Batch [300/469], Loss: nan
Epoch [13/15], Batch [400/469], Loss: nan
Epoch [13/15], Loss: nan
Epoch [14/15], Batch [0/469], Loss: nan
Epoch [14/15], Batch [100/469], Loss: nan
Epoch [14/15], Batch [200/469], Loss: nan
Epoch [14/15], Batch [300/469], Loss: nan
Epoch [14/15], Batch [400/469], Loss: nan
Epoch [14/15], Loss: nan
Epoch [15/15], Batch [0/469], Loss: nan
Epoch [15/15], Batch [100/469], Loss: nan
Epoch [15/15], Batch [200/469], Loss: nan
Epoch [15/15], Batch [300/469], Loss: nan
Epoch [15/15], Batch [400/469], Loss: nan
Epoch [15/15], Loss: nan
Saved models as f1_hadamard_10.pth and f2_hadamard_10.pth
