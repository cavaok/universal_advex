Using device: cuda

==================================================
Training Hadamard Network with 1 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 0.2781
Epoch [1/15], Step [200/469], Loss: 0.1743
Epoch [1/15], Step [300/469], Loss: 0.1528
Epoch [1/15], Step [400/469], Loss: 0.1373
Epoch [1/15], Test Accuracy: 34.70%
Epoch [2/15], Step [100/469], Loss: 0.1308
Epoch [2/15], Step [200/469], Loss: 0.1271
Epoch [2/15], Step [300/469], Loss: 0.1238
Epoch [2/15], Step [400/469], Loss: 0.1209
Epoch [2/15], Test Accuracy: 47.92%
Epoch [3/15], Step [100/469], Loss: 0.1176
Epoch [3/15], Step [200/469], Loss: 0.1154
Epoch [3/15], Step [300/469], Loss: 0.1134
Epoch [3/15], Step [400/469], Loss: 0.1114
Epoch [3/15], Test Accuracy: 54.26%
Epoch [4/15], Step [100/469], Loss: 0.1090
Epoch [4/15], Step [200/469], Loss: 0.1080
Epoch [4/15], Step [300/469], Loss: 0.1074
Epoch [4/15], Step [400/469], Loss: 0.1065
Epoch [4/15], Test Accuracy: 57.69%
Epoch [5/15], Step [100/469], Loss: 0.1054
Epoch [5/15], Step [200/469], Loss: 0.1054
Epoch [5/15], Step [300/469], Loss: 0.1044
Epoch [5/15], Step [400/469], Loss: 0.1037
Epoch [5/15], Test Accuracy: 63.41%
Epoch [6/15], Step [100/469], Loss: 0.1029
Epoch [6/15], Step [200/469], Loss: 0.1027
Epoch [6/15], Step [300/469], Loss: 0.1032
Epoch [6/15], Step [400/469], Loss: 0.1025
Epoch [6/15], Test Accuracy: 66.97%
Epoch [7/15], Step [100/469], Loss: 0.1014
Epoch [7/15], Step [200/469], Loss: 0.1016
Epoch [7/15], Step [300/469], Loss: 0.1007
Epoch [7/15], Step [400/469], Loss: 0.1000
Epoch [7/15], Test Accuracy: 69.19%
Epoch [8/15], Step [100/469], Loss: 0.0999
Epoch [8/15], Step [200/469], Loss: 0.0996
Epoch [8/15], Step [300/469], Loss: 0.0996
Epoch [8/15], Step [400/469], Loss: 0.0992
Epoch [8/15], Test Accuracy: 69.49%
Epoch [9/15], Step [100/469], Loss: 0.0984
Epoch [9/15], Step [200/469], Loss: 0.0983
Epoch [9/15], Step [300/469], Loss: 0.0989
Epoch [9/15], Step [400/469], Loss: 0.0983
Epoch [9/15], Test Accuracy: 70.17%
Epoch [10/15], Step [100/469], Loss: 0.0977
Epoch [10/15], Step [200/469], Loss: 0.0971
Epoch [10/15], Step [300/469], Loss: 0.0978
Epoch [10/15], Step [400/469], Loss: 0.0973
Epoch [10/15], Test Accuracy: 72.05%
Epoch [11/15], Step [100/469], Loss: 0.0972
Epoch [11/15], Step [200/469], Loss: 0.0970
Epoch [11/15], Step [300/469], Loss: 0.0969
Epoch [11/15], Step [400/469], Loss: 0.0969
Epoch [11/15], Test Accuracy: 73.36%
Epoch [12/15], Step [100/469], Loss: 0.0963
Epoch [12/15], Step [200/469], Loss: 0.0961
Epoch [12/15], Step [300/469], Loss: 0.0963
Epoch [12/15], Step [400/469], Loss: 0.0957
Epoch [12/15], Test Accuracy: 74.90%
Epoch [13/15], Step [100/469], Loss: 0.0951
Epoch [13/15], Step [200/469], Loss: 0.0957
Epoch [13/15], Step [300/469], Loss: 0.0959
Epoch [13/15], Step [400/469], Loss: 0.0953
Epoch [13/15], Test Accuracy: 76.34%
Epoch [14/15], Step [100/469], Loss: 0.0952
Epoch [14/15], Step [200/469], Loss: 0.0952
Epoch [14/15], Step [300/469], Loss: 0.0943
Epoch [14/15], Step [400/469], Loss: 0.0951
Epoch [14/15], Test Accuracy: 77.60%
Epoch [15/15], Step [100/469], Loss: 0.0942
Epoch [15/15], Step [200/469], Loss: 0.0948
Epoch [15/15], Step [300/469], Loss: 0.0943
Epoch [15/15], Step [400/469], Loss: 0.0944
Epoch [15/15], Test Accuracy: 79.81%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter1.pth

==================================================
Training Hadamard Network with 2 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 0.3373
Epoch [1/15], Step [200/469], Loss: 0.2693
Epoch [1/15], Step [300/469], Loss: 0.2565
Epoch [1/15], Step [400/469], Loss: 0.2494
Epoch [1/15], Test Accuracy: 22.87%
Epoch [2/15], Step [100/469], Loss: 0.2376
Epoch [2/15], Step [200/469], Loss: 0.2340
Epoch [2/15], Step [300/469], Loss: 0.2292
Epoch [2/15], Step [400/469], Loss: 0.2267
Epoch [2/15], Test Accuracy: 37.35%
Epoch [3/15], Step [100/469], Loss: 0.2224
Epoch [3/15], Step [200/469], Loss: 0.2191
Epoch [3/15], Step [300/469], Loss: 0.2178
Epoch [3/15], Step [400/469], Loss: 0.2158
Epoch [3/15], Test Accuracy: 38.90%
Epoch [4/15], Step [100/469], Loss: 0.2134
Epoch [4/15], Step [200/469], Loss: 0.2121
Epoch [4/15], Step [300/469], Loss: 0.2122
Epoch [4/15], Step [400/469], Loss: 0.2095
Epoch [4/15], Test Accuracy: 43.81%
Epoch [5/15], Step [100/469], Loss: 0.2080
Epoch [5/15], Step [200/469], Loss: 0.2077
Epoch [5/15], Step [300/469], Loss: 0.2066
Epoch [5/15], Step [400/469], Loss: 0.2058
Epoch [5/15], Test Accuracy: 47.54%
Epoch [6/15], Step [100/469], Loss: 0.2042
Epoch [6/15], Step [200/469], Loss: 0.2032
Epoch [6/15], Step [300/469], Loss: 0.2025
Epoch [6/15], Step [400/469], Loss: 0.2017
Epoch [6/15], Test Accuracy: 52.71%
Epoch [7/15], Step [100/469], Loss: 0.2006
Epoch [7/15], Step [200/469], Loss: 0.1992
Epoch [7/15], Step [300/469], Loss: 0.1987
Epoch [7/15], Step [400/469], Loss: 0.1986
Epoch [7/15], Test Accuracy: 58.47%
Epoch [8/15], Step [100/469], Loss: 0.1977
Epoch [8/15], Step [200/469], Loss: 0.1968
Epoch [8/15], Step [300/469], Loss: 0.1966
Epoch [8/15], Step [400/469], Loss: 0.1958
Epoch [8/15], Test Accuracy: 64.10%
Epoch [9/15], Step [100/469], Loss: 0.1951
Epoch [9/15], Step [200/469], Loss: 0.1948
Epoch [9/15], Step [300/469], Loss: 0.1935
Epoch [9/15], Step [400/469], Loss: 0.1933
Epoch [9/15], Test Accuracy: 69.19%
Epoch [10/15], Step [100/469], Loss: 0.1917
Epoch [10/15], Step [200/469], Loss: 0.1916
Epoch [10/15], Step [300/469], Loss: 0.1913
Epoch [10/15], Step [400/469], Loss: 0.1905
Epoch [10/15], Test Accuracy: 71.52%
Epoch [11/15], Step [100/469], Loss: 0.1897
Epoch [11/15], Step [200/469], Loss: 0.1893
Epoch [11/15], Step [300/469], Loss: 0.1890
Epoch [11/15], Step [400/469], Loss: 0.1886
Epoch [11/15], Test Accuracy: 77.38%
Epoch [12/15], Step [100/469], Loss: 0.1872
Epoch [12/15], Step [200/469], Loss: 0.1872
Epoch [12/15], Step [300/469], Loss: 0.1861
Epoch [12/15], Step [400/469], Loss: 0.1864
Epoch [12/15], Test Accuracy: 77.88%
Epoch [13/15], Step [100/469], Loss: 0.1856
Epoch [13/15], Step [200/469], Loss: 0.1849
Epoch [13/15], Step [300/469], Loss: 0.1855
Epoch [13/15], Step [400/469], Loss: 0.1841
Epoch [13/15], Test Accuracy: 78.61%
Epoch [14/15], Step [100/469], Loss: 0.1848
Epoch [14/15], Step [200/469], Loss: 0.1823
Epoch [14/15], Step [300/469], Loss: 0.1836
Epoch [14/15], Step [400/469], Loss: 0.1833
Epoch [14/15], Test Accuracy: 80.02%
Epoch [15/15], Step [100/469], Loss: 0.1816
Epoch [15/15], Step [200/469], Loss: 0.1818
Epoch [15/15], Step [300/469], Loss: 0.1817
Epoch [15/15], Step [400/469], Loss: 0.1817
Epoch [15/15], Test Accuracy: 80.70%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter2.pth

==================================================
Training Hadamard Network with 3 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 0.4981
Epoch [1/15], Step [200/469], Loss: 0.3963
Epoch [1/15], Step [300/469], Loss: 0.3739
Epoch [1/15], Step [400/469], Loss: 0.3633
Epoch [1/15], Test Accuracy: 19.48%
Epoch [2/15], Step [100/469], Loss: 0.3473
Epoch [2/15], Step [200/469], Loss: 0.3393
Epoch [2/15], Step [300/469], Loss: 0.3346
Epoch [2/15], Step [400/469], Loss: 0.3298
Epoch [2/15], Test Accuracy: 45.96%
Epoch [3/15], Step [100/469], Loss: 0.3229
Epoch [3/15], Step [200/469], Loss: 0.3209
Epoch [3/15], Step [300/469], Loss: 0.3181
Epoch [3/15], Step [400/469], Loss: 0.3156
Epoch [3/15], Test Accuracy: 60.55%
Epoch [4/15], Step [100/469], Loss: 0.3117
Epoch [4/15], Step [200/469], Loss: 0.3105
Epoch [4/15], Step [300/469], Loss: 0.3090
Epoch [4/15], Step [400/469], Loss: 0.3061
Epoch [4/15], Test Accuracy: 53.45%
Epoch [5/15], Step [100/469], Loss: 0.3029
Epoch [5/15], Step [200/469], Loss: 0.3020
Epoch [5/15], Step [300/469], Loss: 0.3018
Epoch [5/15], Step [400/469], Loss: 0.3000
Epoch [5/15], Test Accuracy: 58.45%
Epoch [6/15], Step [100/469], Loss: 0.2974
Epoch [6/15], Step [200/469], Loss: 0.2967
Epoch [6/15], Step [300/469], Loss: 0.2957
Epoch [6/15], Step [400/469], Loss: 0.2939
Epoch [6/15], Test Accuracy: 67.59%
Epoch [7/15], Step [100/469], Loss: 0.2908
Epoch [7/15], Step [200/469], Loss: 0.2916
Epoch [7/15], Step [300/469], Loss: 0.2912
Epoch [7/15], Step [400/469], Loss: 0.2889
Epoch [7/15], Test Accuracy: 71.96%
Epoch [8/15], Step [100/469], Loss: 0.2886
Epoch [8/15], Step [200/469], Loss: 0.2868
Epoch [8/15], Step [300/469], Loss: 0.2859
Epoch [8/15], Step [400/469], Loss: 0.2859
Epoch [8/15], Test Accuracy: 70.10%
Epoch [9/15], Step [100/469], Loss: 0.2837
Epoch [9/15], Step [200/469], Loss: 0.2838
Epoch [9/15], Step [300/469], Loss: 0.2837
Epoch [9/15], Step [400/469], Loss: 0.2829
Epoch [9/15], Test Accuracy: 71.36%
Epoch [10/15], Step [100/469], Loss: 0.2816
Epoch [10/15], Step [200/469], Loss: 0.2813
Epoch [10/15], Step [300/469], Loss: 0.2806
Epoch [10/15], Step [400/469], Loss: 0.2792
Epoch [10/15], Test Accuracy: 75.38%
Epoch [11/15], Step [100/469], Loss: 0.2781
Epoch [11/15], Step [200/469], Loss: 0.2783
Epoch [11/15], Step [300/469], Loss: 0.2764
Epoch [11/15], Step [400/469], Loss: 0.2777
Epoch [11/15], Test Accuracy: 77.58%
Epoch [12/15], Step [100/469], Loss: 0.2741
Epoch [12/15], Step [200/469], Loss: 0.2762
Epoch [12/15], Step [300/469], Loss: 0.2753
Epoch [12/15], Step [400/469], Loss: 0.2736
Epoch [12/15], Test Accuracy: 76.87%
Epoch [13/15], Step [100/469], Loss: 0.2737
Epoch [13/15], Step [200/469], Loss: 0.2722
Epoch [13/15], Step [300/469], Loss: 0.2724
Epoch [13/15], Step [400/469], Loss: 0.2724
Epoch [13/15], Test Accuracy: 76.33%
Epoch [14/15], Step [100/469], Loss: 0.2699
Epoch [14/15], Step [200/469], Loss: 0.2702
Epoch [14/15], Step [300/469], Loss: 0.2704
Epoch [14/15], Step [400/469], Loss: 0.2698
Epoch [14/15], Test Accuracy: 74.30%
Epoch [15/15], Step [100/469], Loss: 0.2688
Epoch [15/15], Step [200/469], Loss: 0.2683
Epoch [15/15], Step [300/469], Loss: 0.2679
Epoch [15/15], Step [400/469], Loss: 0.2685
Epoch [15/15], Test Accuracy: 75.25%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter3.pth

==================================================
Training Hadamard Network with 4 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 0.6416
Epoch [1/15], Step [200/469], Loss: 0.5203
Epoch [1/15], Step [300/469], Loss: 0.5015
Epoch [1/15], Step [400/469], Loss: 0.4884
Epoch [1/15], Test Accuracy: 9.78%
Epoch [2/15], Step [100/469], Loss: 0.4720
Epoch [2/15], Step [200/469], Loss: 0.4647
Epoch [2/15], Step [300/469], Loss: 0.4558
Epoch [2/15], Step [400/469], Loss: 0.4517
Epoch [2/15], Test Accuracy: 30.63%
Epoch [3/15], Step [100/469], Loss: 0.4442
Epoch [3/15], Step [200/469], Loss: 0.4384
Epoch [3/15], Step [300/469], Loss: 0.4327
Epoch [3/15], Step [400/469], Loss: 0.4279
Epoch [3/15], Test Accuracy: 48.34%
Epoch [4/15], Step [100/469], Loss: 0.4232
Epoch [4/15], Step [200/469], Loss: 0.4204
Epoch [4/15], Step [300/469], Loss: 0.4165
Epoch [4/15], Step [400/469], Loss: 0.4126
Epoch [4/15], Test Accuracy: 59.53%
Epoch [5/15], Step [100/469], Loss: 0.4106
Epoch [5/15], Step [200/469], Loss: 0.4097
Epoch [5/15], Step [300/469], Loss: 0.4069
Epoch [5/15], Step [400/469], Loss: 0.4047
Epoch [5/15], Test Accuracy: 52.17%
Epoch [6/15], Step [100/469], Loss: 0.4000
Epoch [6/15], Step [200/469], Loss: 0.4027
Epoch [6/15], Step [300/469], Loss: 0.4000
Epoch [6/15], Step [400/469], Loss: 0.3973
Epoch [6/15], Test Accuracy: 62.28%
Epoch [7/15], Step [100/469], Loss: 0.3950
Epoch [7/15], Step [200/469], Loss: 0.3953
Epoch [7/15], Step [300/469], Loss: 0.3936
Epoch [7/15], Step [400/469], Loss: 0.3929
Epoch [7/15], Test Accuracy: 58.79%
Epoch [8/15], Step [100/469], Loss: 0.3920
Epoch [8/15], Step [200/469], Loss: 0.3895
Epoch [8/15], Step [300/469], Loss: 0.3884
Epoch [8/15], Step [400/469], Loss: 0.3872
Epoch [8/15], Test Accuracy: 61.54%
Epoch [9/15], Step [100/469], Loss: 0.3865
Epoch [9/15], Step [200/469], Loss: 0.3863
Epoch [9/15], Step [300/469], Loss: 0.3858
Epoch [9/15], Step [400/469], Loss: 0.3852
Epoch [9/15], Test Accuracy: 66.47%
Epoch [10/15], Step [100/469], Loss: 0.3839
Epoch [10/15], Step [200/469], Loss: 0.3808
Epoch [10/15], Step [300/469], Loss: 0.3808
Epoch [10/15], Step [400/469], Loss: 0.3810
Epoch [10/15], Test Accuracy: 70.40%
Epoch [11/15], Step [100/469], Loss: 0.3783
Epoch [11/15], Step [200/469], Loss: 0.3786
Epoch [11/15], Step [300/469], Loss: 0.3771
Epoch [11/15], Step [400/469], Loss: 0.3758
Epoch [11/15], Test Accuracy: 67.11%
Epoch [12/15], Step [100/469], Loss: 0.3758
Epoch [12/15], Step [200/469], Loss: 0.3747
Epoch [12/15], Step [300/469], Loss: 0.3735
Epoch [12/15], Step [400/469], Loss: 0.3747
Epoch [12/15], Test Accuracy: 67.38%
Epoch [13/15], Step [100/469], Loss: 0.3700
Epoch [13/15], Step [200/469], Loss: 0.3720
Epoch [13/15], Step [300/469], Loss: 0.3712
Epoch [13/15], Step [400/469], Loss: 0.3698
Epoch [13/15], Test Accuracy: 74.14%
Epoch [14/15], Step [100/469], Loss: 0.3692
Epoch [14/15], Step [200/469], Loss: 0.3677
Epoch [14/15], Step [300/469], Loss: 0.3668
Epoch [14/15], Step [400/469], Loss: 0.3671
Epoch [14/15], Test Accuracy: 73.82%
Epoch [15/15], Step [100/469], Loss: 0.3654
Epoch [15/15], Step [200/469], Loss: 0.3655
Epoch [15/15], Step [300/469], Loss: 0.3641
Epoch [15/15], Step [400/469], Loss: 0.3631
Epoch [15/15], Test Accuracy: 77.09%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter4.pth

==================================================
Training Hadamard Network with 5 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 0.7983
Epoch [1/15], Step [200/469], Loss: 0.6561
Epoch [1/15], Step [300/469], Loss: 0.6309
Epoch [1/15], Step [400/469], Loss: 0.6186
Epoch [1/15], Test Accuracy: 15.80%
Epoch [2/15], Step [100/469], Loss: 0.6007
Epoch [2/15], Step [200/469], Loss: 0.5876
Epoch [2/15], Step [300/469], Loss: 0.5805
Epoch [2/15], Step [400/469], Loss: 0.5717
Epoch [2/15], Test Accuracy: 25.00%
Epoch [3/15], Step [100/469], Loss: 0.5596
Epoch [3/15], Step [200/469], Loss: 0.5552
Epoch [3/15], Step [300/469], Loss: 0.5488
Epoch [3/15], Step [400/469], Loss: 0.5448
Epoch [3/15], Test Accuracy: 55.69%
Epoch [4/15], Step [100/469], Loss: 0.5374
Epoch [4/15], Step [200/469], Loss: 0.5352
Epoch [4/15], Step [300/469], Loss: 0.5291
Epoch [4/15], Step [400/469], Loss: 0.5274
Epoch [4/15], Test Accuracy: 54.65%
Epoch [5/15], Step [100/469], Loss: 0.5222
Epoch [5/15], Step [200/469], Loss: 0.5210
Epoch [5/15], Step [300/469], Loss: 0.5196
Epoch [5/15], Step [400/469], Loss: 0.5172
Epoch [5/15], Test Accuracy: 58.18%
Epoch [6/15], Step [100/469], Loss: 0.5116
Epoch [6/15], Step [200/469], Loss: 0.5094
Epoch [6/15], Step [300/469], Loss: 0.5100
Epoch [6/15], Step [400/469], Loss: 0.5088
Epoch [6/15], Test Accuracy: 63.71%
Epoch [7/15], Step [100/469], Loss: 0.5046
Epoch [7/15], Step [200/469], Loss: 0.5045
Epoch [7/15], Step [300/469], Loss: 0.5036
Epoch [7/15], Step [400/469], Loss: 0.5016
Epoch [7/15], Test Accuracy: 57.56%
Epoch [8/15], Step [100/469], Loss: 0.4987
Epoch [8/15], Step [200/469], Loss: 0.4975
Epoch [8/15], Step [300/469], Loss: 0.4966
Epoch [8/15], Step [400/469], Loss: 0.4937
Epoch [8/15], Test Accuracy: 66.14%
Epoch [9/15], Step [100/469], Loss: 0.4929
Epoch [9/15], Step [200/469], Loss: 0.4896
Epoch [9/15], Step [300/469], Loss: 0.4932
Epoch [9/15], Step [400/469], Loss: 0.4918
Epoch [9/15], Test Accuracy: 70.16%
Epoch [10/15], Step [100/469], Loss: 0.4868
Epoch [10/15], Step [200/469], Loss: 0.4863
Epoch [10/15], Step [300/469], Loss: 0.4846
Epoch [10/15], Step [400/469], Loss: 0.4838
Epoch [10/15], Test Accuracy: 65.39%
Epoch [11/15], Step [100/469], Loss: 0.4812
Epoch [11/15], Step [200/469], Loss: 0.4811
Epoch [11/15], Step [300/469], Loss: 0.4819
Epoch [11/15], Step [400/469], Loss: 0.4802
Epoch [11/15], Test Accuracy: 67.95%
Epoch [12/15], Step [100/469], Loss: 0.4783
Epoch [12/15], Step [200/469], Loss: 0.4764
Epoch [12/15], Step [300/469], Loss: 0.4771
Epoch [12/15], Step [400/469], Loss: 0.4751
Epoch [12/15], Test Accuracy: 67.71%
Epoch [13/15], Step [100/469], Loss: 0.4749
Epoch [13/15], Step [200/469], Loss: 0.4743
Epoch [13/15], Step [300/469], Loss: 0.4720
Epoch [13/15], Step [400/469], Loss: 0.4725
Epoch [13/15], Test Accuracy: 71.67%
Epoch [14/15], Step [100/469], Loss: 0.4707
Epoch [14/15], Step [200/469], Loss: 0.4697
Epoch [14/15], Step [300/469], Loss: 0.4686
Epoch [14/15], Step [400/469], Loss: 0.4674
Epoch [14/15], Test Accuracy: 71.87%
Epoch [15/15], Step [100/469], Loss: 0.4661
Epoch [15/15], Step [200/469], Loss: 0.4651
Epoch [15/15], Step [300/469], Loss: 0.4658
Epoch [15/15], Step [400/469], Loss: 0.4652
Epoch [15/15], Test Accuracy: 73.15%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter5.pth

==================================================
Training Hadamard Network with 6 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 0.9599
Epoch [1/15], Step [200/469], Loss: 0.7814
Epoch [1/15], Step [300/469], Loss: 0.7523
Epoch [1/15], Step [400/469], Loss: 0.7418
Epoch [1/15], Test Accuracy: 15.77%
Epoch [2/15], Step [100/469], Loss: 0.7234
Epoch [2/15], Step [200/469], Loss: 0.7204
Epoch [2/15], Step [300/469], Loss: 0.7075
Epoch [2/15], Step [400/469], Loss: 0.6961
Epoch [2/15], Test Accuracy: 17.36%
Epoch [3/15], Step [100/469], Loss: 0.6867
Epoch [3/15], Step [200/469], Loss: 0.6758
Epoch [3/15], Step [300/469], Loss: 0.6725
Epoch [3/15], Step [400/469], Loss: 0.6667
Epoch [3/15], Test Accuracy: 38.46%
Epoch [4/15], Step [100/469], Loss: 0.6576
Epoch [4/15], Step [200/469], Loss: 0.6527
Epoch [4/15], Step [300/469], Loss: 0.6492
Epoch [4/15], Step [400/469], Loss: 0.6463
Epoch [4/15], Test Accuracy: 44.97%
Epoch [5/15], Step [100/469], Loss: 0.6414
Epoch [5/15], Step [200/469], Loss: 0.6341
Epoch [5/15], Step [300/469], Loss: 0.6316
Epoch [5/15], Step [400/469], Loss: 0.6293
Epoch [5/15], Test Accuracy: 48.48%
Epoch [6/15], Step [100/469], Loss: 0.6232
Epoch [6/15], Step [200/469], Loss: 0.6229
Epoch [6/15], Step [300/469], Loss: 0.6202
Epoch [6/15], Step [400/469], Loss: 0.6215
Epoch [6/15], Test Accuracy: 52.20%
Epoch [7/15], Step [100/469], Loss: 0.6166
Epoch [7/15], Step [200/469], Loss: 0.6132
Epoch [7/15], Step [300/469], Loss: 0.6109
Epoch [7/15], Step [400/469], Loss: 0.6104
Epoch [7/15], Test Accuracy: 56.34%
Epoch [8/15], Step [100/469], Loss: 0.6049
Epoch [8/15], Step [200/469], Loss: 0.6019
Epoch [8/15], Step [300/469], Loss: 0.6032
Epoch [8/15], Step [400/469], Loss: 0.6026
Epoch [8/15], Test Accuracy: 52.32%
Epoch [9/15], Step [100/469], Loss: 0.5944
Epoch [9/15], Step [200/469], Loss: 0.5983
Epoch [9/15], Step [300/469], Loss: 0.5946
Epoch [9/15], Step [400/469], Loss: 0.5956
Epoch [9/15], Test Accuracy: 61.42%
Epoch [10/15], Step [100/469], Loss: 0.5913
Epoch [10/15], Step [200/469], Loss: 0.5914
Epoch [10/15], Step [300/469], Loss: 0.5889
Epoch [10/15], Step [400/469], Loss: 0.5905
Epoch [10/15], Test Accuracy: 62.68%
Epoch [11/15], Step [100/469], Loss: 0.5860
Epoch [11/15], Step [200/469], Loss: 0.5848
Epoch [11/15], Step [300/469], Loss: 0.5820
Epoch [11/15], Step [400/469], Loss: 0.5831
Epoch [11/15], Test Accuracy: 66.84%
Epoch [12/15], Step [100/469], Loss: 0.5781
Epoch [12/15], Step [200/469], Loss: 0.5792
Epoch [12/15], Step [300/469], Loss: 0.5770
Epoch [12/15], Step [400/469], Loss: 0.5750
Epoch [12/15], Test Accuracy: 71.94%
Epoch [13/15], Step [100/469], Loss: 0.5757
Epoch [13/15], Step [200/469], Loss: 0.5739
Epoch [13/15], Step [300/469], Loss: 0.5744
Epoch [13/15], Step [400/469], Loss: 0.5697
Epoch [13/15], Test Accuracy: 67.98%
Epoch [14/15], Step [100/469], Loss: 0.5681
Epoch [14/15], Step [200/469], Loss: 0.5675
Epoch [14/15], Step [300/469], Loss: 0.5683
Epoch [14/15], Step [400/469], Loss: 0.5688
Epoch [14/15], Test Accuracy: 66.80%
Epoch [15/15], Step [100/469], Loss: 0.5630
Epoch [15/15], Step [200/469], Loss: 0.5661
Epoch [15/15], Step [300/469], Loss: 0.5636
Epoch [15/15], Step [400/469], Loss: 0.5618
Epoch [15/15], Test Accuracy: 73.43%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter6.pth

==================================================
Training Hadamard Network with 7 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 1.1265
Epoch [1/15], Step [200/469], Loss: 0.9230
Epoch [1/15], Step [300/469], Loss: 0.8841
Epoch [1/15], Step [400/469], Loss: 0.8711
Epoch [1/15], Test Accuracy: 9.74%
Epoch [2/15], Step [100/469], Loss: 0.8561
Epoch [2/15], Step [200/469], Loss: 0.8475
Epoch [2/15], Step [300/469], Loss: 0.8372
Epoch [2/15], Step [400/469], Loss: 0.8288
Epoch [2/15], Test Accuracy: 17.80%
Epoch [3/15], Step [100/469], Loss: 0.8226
Epoch [3/15], Step [200/469], Loss: 0.8099
Epoch [3/15], Step [300/469], Loss: 0.7986
Epoch [3/15], Step [400/469], Loss: 0.7890
Epoch [3/15], Test Accuracy: 26.52%
Epoch [4/15], Step [100/469], Loss: 0.7837
Epoch [4/15], Step [200/469], Loss: 0.7753
Epoch [4/15], Step [300/469], Loss: 0.7684
Epoch [4/15], Step [400/469], Loss: 0.7602
Epoch [4/15], Test Accuracy: 36.04%
Epoch [5/15], Step [100/469], Loss: 0.7561
Epoch [5/15], Step [200/469], Loss: 0.7543
Epoch [5/15], Step [300/469], Loss: 0.7479
Epoch [5/15], Step [400/469], Loss: 0.7456
Epoch [5/15], Test Accuracy: 44.88%
Epoch [6/15], Step [100/469], Loss: 0.7397
Epoch [6/15], Step [200/469], Loss: 0.7364
Epoch [6/15], Step [300/469], Loss: 0.7369
Epoch [6/15], Step [400/469], Loss: 0.7328
Epoch [6/15], Test Accuracy: 49.95%
Epoch [7/15], Step [100/469], Loss: 0.7280
Epoch [7/15], Step [200/469], Loss: 0.7265
Epoch [7/15], Step [300/469], Loss: 0.7190
Epoch [7/15], Step [400/469], Loss: 0.7225
Epoch [7/15], Test Accuracy: 56.89%
Epoch [8/15], Step [100/469], Loss: 0.7173
Epoch [8/15], Step [200/469], Loss: 0.7138
Epoch [8/15], Step [300/469], Loss: 0.7100
Epoch [8/15], Step [400/469], Loss: 0.7077
Epoch [8/15], Test Accuracy: 59.24%
Epoch [9/15], Step [100/469], Loss: 0.7056
Epoch [9/15], Step [200/469], Loss: 0.7087
Epoch [9/15], Step [300/469], Loss: 0.7017
Epoch [9/15], Step [400/469], Loss: 0.6994
Epoch [9/15], Test Accuracy: 52.35%
Epoch [10/15], Step [100/469], Loss: 0.6981
Epoch [10/15], Step [200/469], Loss: 0.6935
Epoch [10/15], Step [300/469], Loss: 0.6907
Epoch [10/15], Step [400/469], Loss: 0.6943
Epoch [10/15], Test Accuracy: 61.07%
Epoch [11/15], Step [100/469], Loss: 0.6883
Epoch [11/15], Step [200/469], Loss: 0.6857
Epoch [11/15], Step [300/469], Loss: 0.6835
Epoch [11/15], Step [400/469], Loss: 0.6850
Epoch [11/15], Test Accuracy: 68.16%
Epoch [12/15], Step [100/469], Loss: 0.6789
Epoch [12/15], Step [200/469], Loss: 0.6750
Epoch [12/15], Step [300/469], Loss: 0.6812
Epoch [12/15], Step [400/469], Loss: 0.6777
Epoch [12/15], Test Accuracy: 57.78%
Epoch [13/15], Step [100/469], Loss: 0.6724
Epoch [13/15], Step [200/469], Loss: 0.6709
Epoch [13/15], Step [300/469], Loss: 0.6670
Epoch [13/15], Step [400/469], Loss: 0.6680
Epoch [13/15], Test Accuracy: 67.49%
Epoch [14/15], Step [100/469], Loss: 0.6639
Epoch [14/15], Step [200/469], Loss: 0.6652
Epoch [14/15], Step [300/469], Loss: 0.6618
Epoch [14/15], Step [400/469], Loss: 0.6649
Epoch [14/15], Test Accuracy: 66.79%
Epoch [15/15], Step [100/469], Loss: 0.6583
Epoch [15/15], Step [200/469], Loss: 0.6573
Epoch [15/15], Step [300/469], Loss: 0.6560
Epoch [15/15], Step [400/469], Loss: 0.6581
Epoch [15/15], Test Accuracy: 67.18%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter7.pth

==================================================
Training Hadamard Network with 8 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 1.3036
Epoch [1/15], Step [200/469], Loss: 1.0786
Epoch [1/15], Step [300/469], Loss: 1.0222
Epoch [1/15], Step [400/469], Loss: 0.9967
Epoch [1/15], Test Accuracy: 11.35%
Epoch [2/15], Step [100/469], Loss: 0.9839
Epoch [2/15], Step [200/469], Loss: 0.9782
Epoch [2/15], Step [300/469], Loss: 0.9706
Epoch [2/15], Step [400/469], Loss: 0.9584
Epoch [2/15], Test Accuracy: 15.94%
Epoch [3/15], Step [100/469], Loss: 0.9476
Epoch [3/15], Step [200/469], Loss: 0.9367
Epoch [3/15], Step [300/469], Loss: 0.9307
Epoch [3/15], Step [400/469], Loss: 0.9222
Epoch [3/15], Test Accuracy: 11.32%
Epoch [4/15], Step [100/469], Loss: 0.9136
Epoch [4/15], Step [200/469], Loss: 0.9047
Epoch [4/15], Step [300/469], Loss: 0.8939
Epoch [4/15], Step [400/469], Loss: 0.8849
Epoch [4/15], Test Accuracy: 33.51%
Epoch [5/15], Step [100/469], Loss: 0.8740
Epoch [5/15], Step [200/469], Loss: 0.8718
Epoch [5/15], Step [300/469], Loss: 0.8591
Epoch [5/15], Step [400/469], Loss: 0.8564
Epoch [5/15], Test Accuracy: 45.67%
Epoch [6/15], Step [100/469], Loss: 0.8458
Epoch [6/15], Step [200/469], Loss: 0.8422
Epoch [6/15], Step [300/469], Loss: 0.8407
Epoch [6/15], Step [400/469], Loss: 0.8383
Epoch [6/15], Test Accuracy: 52.38%
Epoch [7/15], Step [100/469], Loss: 0.8266
Epoch [7/15], Step [200/469], Loss: 0.8264
Epoch [7/15], Step [300/469], Loss: 0.8173
Epoch [7/15], Step [400/469], Loss: 0.8159
Epoch [7/15], Test Accuracy: 54.87%
Epoch [8/15], Step [100/469], Loss: 0.8047
Epoch [8/15], Step [200/469], Loss: 0.8084
Epoch [8/15], Step [300/469], Loss: 0.8102
Epoch [8/15], Step [400/469], Loss: 0.8026
Epoch [8/15], Test Accuracy: 62.89%
Epoch [9/15], Step [100/469], Loss: 0.7988
Epoch [9/15], Step [200/469], Loss: 0.7996
Epoch [9/15], Step [300/469], Loss: 0.7943
Epoch [9/15], Step [400/469], Loss: 0.7920
Epoch [9/15], Test Accuracy: 62.33%
Epoch [10/15], Step [100/469], Loss: 0.7856
Epoch [10/15], Step [200/469], Loss: 0.7854
Epoch [10/15], Step [300/469], Loss: 0.7795
Epoch [10/15], Step [400/469], Loss: 0.7789
Epoch [10/15], Test Accuracy: 64.38%
Epoch [11/15], Step [100/469], Loss: 0.7742
Epoch [11/15], Step [200/469], Loss: 0.7773
Epoch [11/15], Step [300/469], Loss: 0.7757
Epoch [11/15], Step [400/469], Loss: 0.7674
Epoch [11/15], Test Accuracy: 63.54%
Epoch [12/15], Step [100/469], Loss: 0.7700
Epoch [12/15], Step [200/469], Loss: 0.7646
Epoch [12/15], Step [300/469], Loss: 0.7630
Epoch [12/15], Step [400/469], Loss: 0.7612
Epoch [12/15], Test Accuracy: 70.13%
Epoch [13/15], Step [100/469], Loss: 0.7585
Epoch [13/15], Step [200/469], Loss: 0.7585
Epoch [13/15], Step [300/469], Loss: 0.7591
Epoch [13/15], Step [400/469], Loss: 0.7557
Epoch [13/15], Test Accuracy: 67.90%
Epoch [14/15], Step [100/469], Loss: 0.7522
Epoch [14/15], Step [200/469], Loss: 0.7534
Epoch [14/15], Step [300/469], Loss: 0.7519
Epoch [14/15], Step [400/469], Loss: 0.7528
Epoch [14/15], Test Accuracy: 67.01%
Epoch [15/15], Step [100/469], Loss: 0.7452
Epoch [15/15], Step [200/469], Loss: 0.7435
Epoch [15/15], Step [300/469], Loss: 0.7451
Epoch [15/15], Step [400/469], Loss: 0.7452
Epoch [15/15], Test Accuracy: 67.21%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter8.pth

==================================================
Training Hadamard Network with 9 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 1.4802
Epoch [1/15], Step [200/469], Loss: 1.1775
Epoch [1/15], Step [300/469], Loss: 1.1319
Epoch [1/15], Step [400/469], Loss: 1.1195
Epoch [1/15], Test Accuracy: 10.28%
Epoch [2/15], Step [100/469], Loss: 1.1080
Epoch [2/15], Step [200/469], Loss: 1.0939
Epoch [2/15], Step [300/469], Loss: 1.0848
Epoch [2/15], Step [400/469], Loss: 1.0770
Epoch [2/15], Test Accuracy: 9.74%
Epoch [3/15], Step [100/469], Loss: 1.0655
Epoch [3/15], Step [200/469], Loss: 1.0607
Epoch [3/15], Step [300/469], Loss: 1.0636
Epoch [3/15], Step [400/469], Loss: 1.0483
Epoch [3/15], Test Accuracy: 15.69%
Epoch [4/15], Step [100/469], Loss: 1.0331
Epoch [4/15], Step [200/469], Loss: 1.0316
Epoch [4/15], Step [300/469], Loss: 1.0123
Epoch [4/15], Step [400/469], Loss: 1.0082
Epoch [4/15], Test Accuracy: 28.98%
Epoch [5/15], Step [100/469], Loss: 0.9925
Epoch [5/15], Step [200/469], Loss: 0.9792
Epoch [5/15], Step [300/469], Loss: 0.9651
Epoch [5/15], Step [400/469], Loss: 0.9605
Epoch [5/15], Test Accuracy: 41.34%
Epoch [6/15], Step [100/469], Loss: 0.9516
Epoch [6/15], Step [200/469], Loss: 0.9485
Epoch [6/15], Step [300/469], Loss: 0.9366
Epoch [6/15], Step [400/469], Loss: 0.9331
Epoch [6/15], Test Accuracy: 41.42%
Epoch [7/15], Step [100/469], Loss: 0.9238
Epoch [7/15], Step [200/469], Loss: 0.9219
Epoch [7/15], Step [300/469], Loss: 0.9161
Epoch [7/15], Step [400/469], Loss: 0.9193
Epoch [7/15], Test Accuracy: 50.45%
Epoch [8/15], Step [100/469], Loss: 0.9070
Epoch [8/15], Step [200/469], Loss: 0.9006
Epoch [8/15], Step [300/469], Loss: 0.9028
Epoch [8/15], Step [400/469], Loss: 0.9005
Epoch [8/15], Test Accuracy: 50.81%
Epoch [9/15], Step [100/469], Loss: 0.8965
Epoch [9/15], Step [200/469], Loss: 0.8845
Epoch [9/15], Step [300/469], Loss: 0.8918
Epoch [9/15], Step [400/469], Loss: 0.8847
Epoch [9/15], Test Accuracy: 59.73%
Epoch [10/15], Step [100/469], Loss: 0.8814
Epoch [10/15], Step [200/469], Loss: 0.8842
Epoch [10/15], Step [300/469], Loss: 0.8782
Epoch [10/15], Step [400/469], Loss: 0.8751
Epoch [10/15], Test Accuracy: 58.93%
Epoch [11/15], Step [100/469], Loss: 0.8711
Epoch [11/15], Step [200/469], Loss: 0.8685
Epoch [11/15], Step [300/469], Loss: 0.8689
Epoch [11/15], Step [400/469], Loss: 0.8659
Epoch [11/15], Test Accuracy: 64.61%
Epoch [12/15], Step [100/469], Loss: 0.8653
Epoch [12/15], Step [200/469], Loss: 0.8612
Epoch [12/15], Step [300/469], Loss: 0.8559
Epoch [12/15], Step [400/469], Loss: 0.8579
Epoch [12/15], Test Accuracy: 65.44%
Epoch [13/15], Step [100/469], Loss: 0.8505
Epoch [13/15], Step [200/469], Loss: 0.8453
Epoch [13/15], Step [300/469], Loss: 0.8516
Epoch [13/15], Step [400/469], Loss: 0.8505
Epoch [13/15], Test Accuracy: 65.91%
Epoch [14/15], Step [100/469], Loss: 0.8458
Epoch [14/15], Step [200/469], Loss: 0.8434
Epoch [14/15], Step [300/469], Loss: 0.8443
Epoch [14/15], Step [400/469], Loss: 0.8379
Epoch [14/15], Test Accuracy: 68.36%
Epoch [15/15], Step [100/469], Loss: 0.8404
Epoch [15/15], Step [200/469], Loss: 0.8402
Epoch [15/15], Step [300/469], Loss: 0.8325
Epoch [15/15], Step [400/469], Loss: 0.8321
Epoch [15/15], Test Accuracy: 66.82%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter9.pth

==================================================
Training Hadamard Network with 10 iterations
Learning rate: 0.0014686
Batch size: 128
Lambda weight: 0.022089
==================================================

Epoch [1/15], Step [100/469], Loss: 1.6127
Epoch [1/15], Step [200/469], Loss: 1.3468
Epoch [1/15], Step [300/469], Loss: 1.2713
Epoch [1/15], Step [400/469], Loss: 1.2475
Epoch [1/15], Test Accuracy: 9.80%
Epoch [2/15], Step [100/469], Loss: 1.2395
Epoch [2/15], Step [200/469], Loss: 1.2328
Epoch [2/15], Step [300/469], Loss: 1.2194
Epoch [2/15], Step [400/469], Loss: 1.2139
Epoch [2/15], Test Accuracy: 10.08%
Epoch [3/15], Step [100/469], Loss: 1.2018
Epoch [3/15], Step [200/469], Loss: 1.1903
Epoch [3/15], Step [300/469], Loss: 1.1844
Epoch [3/15], Step [400/469], Loss: 1.1745
Epoch [3/15], Test Accuracy: 10.41%
Epoch [4/15], Step [100/469], Loss: 1.1780
Epoch [4/15], Step [200/469], Loss: 1.1773
Epoch [4/15], Step [300/469], Loss: 1.1699
Epoch [4/15], Step [400/469], Loss: 1.1580
Epoch [4/15], Test Accuracy: 12.99%
Epoch [5/15], Step [100/469], Loss: 1.1505
Epoch [5/15], Step [200/469], Loss: 1.1506
Epoch [5/15], Step [300/469], Loss: 1.1426
Epoch [5/15], Step [400/469], Loss: 1.1309
Epoch [5/15], Test Accuracy: 26.87%
Epoch [6/15], Step [100/469], Loss: 1.1150
Epoch [6/15], Step [200/469], Loss: 1.1004
Epoch [6/15], Step [300/469], Loss: 1.0950
Epoch [6/15], Step [400/469], Loss: 1.0779
Epoch [6/15], Test Accuracy: 44.08%
Epoch [7/15], Step [100/469], Loss: 1.0651
Epoch [7/15], Step [200/469], Loss: 1.0461
Epoch [7/15], Step [300/469], Loss: 1.0436
Epoch [7/15], Step [400/469], Loss: 1.0324
Epoch [7/15], Test Accuracy: 52.20%
Epoch [8/15], Step [100/469], Loss: 1.0210
Epoch [8/15], Step [200/469], Loss: 1.0145
Epoch [8/15], Step [300/469], Loss: 1.0143
Epoch [8/15], Step [400/469], Loss: 1.0107
Epoch [8/15], Test Accuracy: 56.05%
Epoch [9/15], Step [100/469], Loss: 1.0005
Epoch [9/15], Step [200/469], Loss: 0.9957
Epoch [9/15], Step [300/469], Loss: 0.9903
Epoch [9/15], Step [400/469], Loss: 0.9896
Epoch [9/15], Test Accuracy: 58.16%
Epoch [10/15], Step [100/469], Loss: 0.9843
Epoch [10/15], Step [200/469], Loss: 0.9816
Epoch [10/15], Step [300/469], Loss: 0.9822
Epoch [10/15], Step [400/469], Loss: 0.9757
Epoch [10/15], Test Accuracy: 59.69%
Epoch [11/15], Step [100/469], Loss: 0.9668
Epoch [11/15], Step [200/469], Loss: 0.9673
Epoch [11/15], Step [300/469], Loss: 0.9655
Epoch [11/15], Step [400/469], Loss: 0.9623
Epoch [11/15], Test Accuracy: 65.03%
Epoch [12/15], Step [100/469], Loss: 0.9585
Epoch [12/15], Step [200/469], Loss: 0.9598
Epoch [12/15], Step [300/469], Loss: 0.9561
Epoch [12/15], Step [400/469], Loss: 0.9550
Epoch [12/15], Test Accuracy: 63.55%
Epoch [13/15], Step [100/469], Loss: 0.9493
Epoch [13/15], Step [200/469], Loss: 0.9497
Epoch [13/15], Step [300/469], Loss: 0.9454
Epoch [13/15], Step [400/469], Loss: 0.9442
Epoch [13/15], Test Accuracy: 71.34%
Epoch [14/15], Step [100/469], Loss: 0.9453
Epoch [14/15], Step [200/469], Loss: 0.9352
Epoch [14/15], Step [300/469], Loss: 0.9379
Epoch [14/15], Step [400/469], Loss: 0.9329
Epoch [14/15], Test Accuracy: 69.15%
Epoch [15/15], Step [100/469], Loss: 0.9286
Epoch [15/15], Step [200/469], Loss: 0.9288
Epoch [15/15], Step [300/469], Loss: 0.9301
Epoch [15/15], Step [400/469], Loss: 0.9242
Epoch [15/15], Test Accuracy: 71.27%

Model saved to: /home/okcava/projects/universal_advex/trainers/../models/hadamard_sum_iter10.pth

Final Results:
==================================================
Hadamard Sum Loss with 1 iterations: 79.81%
Hadamard Sum Loss with 2 iterations: 80.70%
Hadamard Sum Loss with 3 iterations: 75.25%
Hadamard Sum Loss with 4 iterations: 77.09%
Hadamard Sum Loss with 5 iterations: 73.15%
Hadamard Sum Loss with 6 iterations: 73.43%
Hadamard Sum Loss with 7 iterations: 67.18%
Hadamard Sum Loss with 8 iterations: 67.21%
Hadamard Sum Loss with 9 iterations: 66.82%
Hadamard Sum Loss with 10 iterations: 71.27%
